{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains everything necessary to reproduce the experiments in our paper:  \n",
    "\n",
    "*Multi-View Majority Vote Learning Algorithms: Direct Minimization of PAC-Bayesian Bounds*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from termcolor import colored\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import check_random_state\n",
    "RAND = check_random_state(42)\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from mvpb.dNDF import MultiViewBoundsDeepNeuralDecisionForests\n",
    "\n",
    "\n",
    "# Import data\n",
    "from mvlearn.datasets import load_UCImultifeature\n",
    "from data import (SampleData,\n",
    "                           Nhanes,\n",
    "                           MultipleFeatures,\n",
    "                           MNIST_MV_Datasets,\n",
    "                           Fash_MNIST_MV_Datasets,\n",
    "                           EMNIST_Letters_MV_Datasets,\n",
    "                           Mushrooms,\n",
    "                           PTB_XL_plus,\n",
    "                           Nutrimouse,\n",
    "                           ReutersEN,\n",
    "                           IS,\n",
    "                           CorelImageFeatures,\n",
    "                           NUS_WIDE_OBJECT,\n",
    "                           ALOI,\n",
    "                           train_test_split,\n",
    "                           train_test_merge,\n",
    "                           s1_s2_split,\n",
    "                           multiclass_to_binary,\n",
    "                           balance_dataset,\n",
    "                           other_binary_options,\n",
    "                           poison_dataset)\n",
    "from mvpb.util import uniform_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the multiview datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MultipleFeatures()\n",
    "X_train, y_train, X_test, y_test = dataset.get_data()\n",
    "# real_classes = dataset.get_real_classes(np.unique(y_train))\n",
    "\n",
    "Xs_train = []\n",
    "Xs_test = []\n",
    "for xtr, xts in zip(X_train, X_test):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(xtr)\n",
    "    Xs_train.append(scaler.transform(xtr))\n",
    "    Xs_test.append(scaler.transform(xts))\n",
    "\n",
    "X_train_concat = [np.concatenate(Xs_train, axis=1)]\n",
    "X_test_concat = [np.concatenate(Xs_test, axis=1)]\n",
    "\n",
    "np.unique(y_train), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 216), (1600,), (400, 216), (400,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape, y_train.shape, X_test[1].shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = range(3)\n",
    "\n",
    "OPTIMIZE_LAMBDA_GAMMA = True\n",
    "# ALPHA = [1, 0.5, 1.1, 2]\n",
    "ALPHA = [1]\n",
    "MAX_ITER = 1000\n",
    "\n",
    "stump_config = {\n",
    "    \"name\": \"stump\",\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 1,\n",
    "    \"max_features\": 0.5,\n",
    "}\n",
    "weak_learners_config = {\n",
    "    \"name\": \"weak_learner\",\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 3,\n",
    "    \"max_features\": 0.5,\n",
    "}\n",
    "strong_learners_config = {\n",
    "    \"name\": \"strong_learner\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 6,\n",
    "    \"max_features\": 0.8,\n",
    "}\n",
    "\n",
    "# CFG = [stump_config, weak_learners_config, strong_learners_config]\n",
    "CFG = [strong_learners_config]\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "TO_BINARY  = \"ovo\" # One of [\"ovr\", \"ovo\", \"other\",  None]\n",
    "label_1 = 4 #np.unique(y_test)[np.where(real_classes == \"['NORM']\")[0][0]]\n",
    "label_2 = 9\n",
    "\n",
    "POISON = False\n",
    "\n",
    "USE_UNLABELED = True\n",
    "s_labeled_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5] if not USE_UNLABELED else [0.5]\n",
    "    \n",
    "\n",
    "m = y_train.size #350\n",
    "test_size = 1 - (m  / (y_test.size+y_train.size))\n",
    "experiments = {}\n",
    "for s_labeled_size in s_labeled_sizes:\n",
    "    experiments[s_labeled_size] = {}\n",
    "    for alpha in ALPHA:\n",
    "        experiments[s_labeled_size][alpha] = {}\n",
    "        for cfg in CFG:\n",
    "            experiments[s_labeled_size][alpha][cfg[\"name\"]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: {1: {'strong_learner': []}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if POISON:\n",
    "    Xs_train, y_train = poison_dataset(Xs_train, y_train, poison_label=label_1, target_label=label_2, target_view=0, num_samples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44m\u001b[30m############ Using 50.0% labeled data ############\u001b[0m\n",
      "\u001b[44m\u001b[30m\t############ Using alpha=1 ############\u001b[0m\n",
      "\u001b[44m\u001b[30m\t\t############ Using strong_learner ############\u001b[0m\n",
      "\u001b[34m\n",
      "----------------Run 0---------------\u001b[0m\n",
      "Training multiview classifier-------------------------------\n",
      "Training concatenated views classifier-------------------------------\n",
      "\u001b[32mOptimizing TND_DIS for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND_DIS for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing DIS for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing DIS for concatenated classifier-------------------------------\u001b[0m\n",
      "\t Convergence reached after 796 iterations\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing Lambda for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing Lambda for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[34m\n",
      "----------------Run 1---------------\u001b[0m\n",
      "Training multiview classifier-------------------------------\n",
      "Training concatenated views classifier-------------------------------\n",
      "\u001b[32mOptimizing TND_DIS for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND_DIS for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing DIS for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing DIS for concatenated classifier-------------------------------\u001b[0m\n",
      "\t Convergence reached after 710 iterations\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing Lambda for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing Lambda for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[34m\n",
      "----------------Run 2---------------\u001b[0m\n",
      "Training multiview classifier-------------------------------\n",
      "Training concatenated views classifier-------------------------------\n",
      "\u001b[32mOptimizing TND_DIS for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND_DIS for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing TND for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing DIS for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing DIS for concatenated classifier-------------------------------\u001b[0m\n",
      "\t Convergence reached after 866 iterations\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing Lambda for multiview classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing Lambda for concatenated classifier-------------------------------\u001b[0m\n",
      "\u001b[32mOptimization is done! Computing the bound values ans risks-------------------------------\u001b[0m\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Xs, y = train_test_merge(Xs_train, y_train, Xs_test, y_test)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "# iterate over the labeled data sizes\n",
    "for i, s1_size in enumerate(s_labeled_sizes):\n",
    "    print(colored(f\"############ Using {s1_size*100}% labeled data ############\", 'black', on_color='on_blue'))\n",
    "    s_labeled_dir = 'results'+f\"/s_labeled-{int(s1_size*100)}\"\n",
    "    os.makedirs(s_labeled_dir, exist_ok=True)\n",
    "    # iterate over the alpha values\n",
    "    for j, alpha in enumerate(ALPHA):\n",
    "        if ALPHA == 1.0:\n",
    "            BOUNDS = ['TND_DIS', 'TND', 'DIS', 'Lambda', 'PBkl']\n",
    "        else:\n",
    "            BOUNDS = ['TND_DIS', 'TND', 'DIS', 'Lambda']\n",
    "        print(colored(f\"\\t############ Using {alpha=} ############\", 'black', on_color='on_blue'))\n",
    "        alpha_dir = s_labeled_dir+ f\"/alpha-{alpha}\"\n",
    "        os.makedirs(alpha_dir, exist_ok=True)\n",
    "        # iterate over the configurations\n",
    "        for k, config in enumerate(CFG):\n",
    "            print(colored(f\"\\t\\t############ Using {config['name']} ############\", 'black', on_color='on_blue'))\n",
    "            for run in RUNS:\n",
    "                print(colored(f\"\\n----------------Run {run}---------------\", 'blue'))\n",
    "\n",
    "                # Shuffle and split the dataset into training and testing\n",
    "                Xs_train, y_train, Xs_test, y_test = train_test_split(Xs, y, test_size=test_size, random_state=run*(i+1)*(j+1)*(k+1))\n",
    "                \n",
    "                # Transform to binary OVR (One Vs Rest) or OVO (One Vs One) if needed\n",
    "                if TO_BINARY == \"ovr\":\n",
    "                    Xs_train, y_train, Xs_test, y_test = multiclass_to_binary(Xs_train, y_train, Xs_test, y_test, type=TO_BINARY, label_1=label_1)\n",
    "                elif TO_BINARY == \"ovo\":\n",
    "                    Xs_train, y_train, Xs_test, y_test = multiclass_to_binary(Xs_train, y_train, Xs_test, y_test, type=TO_BINARY, label_1=label_1, label_2=label_2)\n",
    "                elif TO_BINARY == \"other\":\n",
    "                    y_train, y_test = other_binary_options(y_train, y_test)\n",
    "                else:\n",
    "                    print(colored(f\"WARNING: TO_BINARY={TO_BINARY}, continuing\", 'yellow'))\n",
    "\n",
    "                # Split the dataset into labeled and unlabeled\n",
    "                Xs_train, y_train, UlX, _ = s1_s2_split(Xs_train, y_train, s1_size=s1_size, random_state=(run+len(RUNS))*(i+1)*(j+1)*(k+1))\n",
    "                X_train_concat = [np.concatenate(Xs_train, axis=1)]\n",
    "                X_test_concat = [np.concatenate(Xs_test, axis=1)]    \n",
    "                    \n",
    "                # instantiate multiview dNDF classifier\n",
    "                dNDF_mv = MultiViewBoundsDeepNeuralDecisionForests(nb_estimators=config[\"n_estimators\"],\n",
    "                                                        nb_views=len(Xs_train),\n",
    "                                                        depth =config[\"max_depth\"],\n",
    "                                                        used_feature_rate=config[\"max_features\"],\n",
    "                                                        epochs=EPOCHS,\n",
    "                                                        use_dndf=False)\n",
    "                \n",
    "                # instantiate dNDF classifier for concatenated views\n",
    "                dNDF_concat = MultiViewBoundsDeepNeuralDecisionForests(nb_estimators=config[\"n_estimators\"],\n",
    "                                                        nb_views=len(X_train_concat),\n",
    "                                                        depth =config[\"max_depth\"],\n",
    "                                                        used_feature_rate=config[\"max_features\"],\n",
    "                                                        epochs=EPOCHS,\n",
    "                                                        use_dndf=False)\n",
    "                \n",
    "                print(\"Training multiview classifier-------------------------------\")\n",
    "                dNDF_mv.fit(Xs_train, y_train)\n",
    "                \n",
    "                print(\"Training concatenated views classifier-------------------------------\")\n",
    "                dNDF_concat.fit(X_train_concat, y_train)\n",
    "                \n",
    "                \n",
    "                # Optimize the posterior distributions for the each bound\n",
    "                for bound in BOUNDS:\n",
    "                    \n",
    "                    # use the unlabeled data for DIS\n",
    "                    unlabeled_data, c_unlabeled_data = None, None\n",
    "                    if USE_UNLABELED and bound in [\"DIS\", \"TND_DIS\"]:\n",
    "                        unlabeled_data = UlX\n",
    "                        c_unlabeled_data = [np.concatenate(UlX, axis=1)]\n",
    "                        \n",
    "                    if bound != \"PBkl\":\n",
    "                        print(colored(f\"Optimizing {bound} for multiview classifier-------------------------------\", 'green'))\n",
    "                        posterior_Qv , posterior_rho = dNDF_mv.optimize_rho(bound,\n",
    "                                                                            labeled_data=(Xs_train, y_train),\n",
    "                                                                            unlabeled_data=unlabeled_data,\n",
    "                                                                            incl_oob=False,\n",
    "                                                                            max_iter=MAX_ITER,\n",
    "                                                                            optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                            alpha=alpha)\n",
    "                        \n",
    "                        print(colored(f\"Optimizing {bound} for concatenated classifier-------------------------------\", 'green'))\n",
    "                        posterior_Qv_concat , _ = dNDF_concat.optimize_rho(bound,\n",
    "                                                                            labeled_data=(X_train_concat, y_train),\n",
    "                                                                            unlabeled_data=c_unlabeled_data,\n",
    "                                                                            incl_oob=False,\n",
    "                                                                            max_iter=MAX_ITER,\n",
    "                                                                            optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                            alpha=alpha)\n",
    "                    \n",
    "                    # Compute the bound for the multiview classifier\n",
    "                    print(colored(f\"Optimization is done! Computing the bound values ans risks-------------------------------\", 'green'))\n",
    "                    mv_bound, v_bounds = dNDF_mv.bound(\n",
    "                                        bound=bound,\n",
    "                                        labeled_data=(Xs_test, y_test),\n",
    "                                        unlabeled_data=unlabeled_data,\n",
    "                                        incl_oob=False,\n",
    "                                        alpha=alpha)\n",
    "                    _, concat_bound = dNDF_concat.bound(\n",
    "                                        bound=bound,\n",
    "                                        labeled_data=(X_test_concat, y_test),\n",
    "                                        unlabeled_data=c_unlabeled_data,\n",
    "                                        incl_oob=False,\n",
    "                                        alpha=alpha)\n",
    "                    # print(f\"{dNDF_mv.posterior_Qv=} {dNDF_mv.posterior_rho=}\")\n",
    "                    \n",
    "                    # Compute the risk of the multiview classifier\n",
    "                    _, mv_risk, v_risks = dNDF_mv.predict_MV(Xs_test, y_test)\n",
    "                    _, _, concat_risk = dNDF_concat.predict_MV(X_test_concat, y_test)\n",
    "                    \n",
    "                    # Save the results\n",
    "                    print(colored(f\"Entering save and stats zone-------------------------------\", 'green'))\n",
    "                    views_risks = {f\"View{i+1}\": v_risks[i] for i in range(len(v_risks))}\n",
    "                    views_risks.update({\"Concatenated\": concat_risk[0]})\n",
    "                    views_risks.update({\"Multiview\": mv_risk})\n",
    "                    views_bounds = {f\"View{i+1}\": v_bounds[i] for i in range(len(v_bounds))}\n",
    "                    views_bounds.update({\"Concatenated\": concat_bound[0]})\n",
    "                    views_bounds.update({\"Multiview\": mv_bound})\n",
    "                    for (kr, r), (kb, b) in zip(views_risks.items(), views_bounds.items()):\n",
    "                        assert kr == kb # check if the keys are the same\n",
    "                        exp = {\"Run\": run+1, \n",
    "                            \"Bound_name\": bound, \n",
    "                            \"View\": kr, \n",
    "                            \"Risk\": \"{:.3f}\".format(r),\n",
    "                            \"Bound\": \"{:.3f}\".format(b)}\n",
    "                        experiments[s1_size][alpha][config[\"name\"]].append(exp)\n",
    "                    cfg_dir = alpha_dir + \"/\" + config[\"name\"]\n",
    "                    os.makedirs(cfg_dir, exist_ok=True)\n",
    "                    experiment_df = pd.DataFrame(experiments[s1_size][alpha][config[\"name\"]])\n",
    "                    # example: results/s_labeled-5/alpha-1/stump/MNIST_4vs9_20runs.csv\n",
    "                    file_name = f\"{cfg_dir}/{dataset._name}_{label_1}vs{label_2}_{len(RUNS)}runs.csv\"\n",
    "                    experiment_df.to_csv(file_name, sep=\" \", index=False)\n",
    "                    # TODO: add the posterior_Qv and posterior_rho to the experiment\n",
    "                del dNDF_mv, dNDF_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results-dup\", exist_ok=True)\n",
    "\n",
    "for s_labeled_size, size_exp in experiments.items():\n",
    "    s_labeled_dir = 'results-dup'+f\"/s_labeled-{int(s_labeled_size*100)}\"\n",
    "    os.makedirs(s_labeled_dir, exist_ok=True)\n",
    "    for alpha, alpha_exp in size_exp.items():\n",
    "        alpha_dir = s_labeled_dir+ f\"/alpha-{alpha}\"\n",
    "        os.makedirs(alpha_dir, exist_ok=True)\n",
    "        for cfg, cfg_exp in alpha_exp.items():\n",
    "            if cfg_exp == []:\n",
    "                continue\n",
    "            cfg_dir = alpha_dir + \"/\" + cfg\n",
    "            os.makedirs(cfg_dir, exist_ok=True)\n",
    "            experiment_df = pd.DataFrame(cfg_exp)\n",
    "            # example: results-dup/s_labeled-5/alpha-1/stump/MNIST_4vs9_20runs.csv\n",
    "            file_name = f\"{cfg_dir}/{dataset._name}_{label_1}vs{label_2}_{len(RUNS)}runs.csv\"\n",
    "            experiment_df.to_csv(file_name, sep=\" \", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(experiments[0.1][1.1][\"weak_learner\"])\n",
    "# df['Bound'] = df['Bound'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)\n",
    "# df['Risk'] = df['Risk'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Risk\"] > df[\"Bound\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Risk\"] = df[\"Risk\"].astype(float)\n",
    "df[\"Bound\"] = df[\"Bound\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_df = df.groupby([\"Bound_name\", \"View\"]).mean()\n",
    "# agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_grid(exps, m, runs=len(RUNS), bounds=BOUNDS):\n",
    "\n",
    "    sns.set_style(style=\"ticks\")\n",
    "    bounds_palette = sns.color_palette(\"viridis\", n_colors=len(bounds))\n",
    "    risk_palette = sns.color_palette(\"flare\", n_colors=len(bounds))\n",
    "    risk_palette.reverse()\n",
    "\n",
    "    num_views = len(exps['View'].unique())\n",
    "    num_cols = num_views // 2 + num_views % 2  # Calculate number of columns for subplots\n",
    "\n",
    "    fig, ax = plt.subplots(2, num_cols, figsize=(12, 8), sharey=True)\n",
    "\n",
    "    for i, view in enumerate(exps['View'].unique()):\n",
    "        view_data = exps[exps['View'] == view]\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        \n",
    "        # Plot Bound\n",
    "        sns.barplot(x='Bound_name', y='Bound', data=view_data, ax=ax[row, col], hue='Bound_name', hatch='.', palette=bounds_palette)\n",
    "        sns.barplot(x='Bound_name', y='Risk', data=view_data, ax=ax[row, col], hue='Bound_name', hatch='\\\\', palette=risk_palette)\n",
    "        ax[row, col].set_title(f'{view}')\n",
    "        ax[row, col].set_xlabel('Bounds')\n",
    "        ax[row, col].set_ylabel('Means')\n",
    "\n",
    "        \n",
    "\n",
    "    # handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "    # labels = [labels[i]+\" bound\" if i < len(labels)/2 else labels[i]+\" Gibbs risk\" for i in range(len(labels))]\n",
    "    # ax[0, 0].legend(handles, labels, title='Bounds', loc='upper right', fontsize='medium')\n",
    "    fig.suptitle(f'Test error rates and multiview PAC-Bayesian bound values, {m=},\\naveraged over {runs} runs')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_grid(df, len(Xs_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(experiments, m, runs=len(RUNS), bounds=BOUNDS):\n",
    "    sns.set_style(style=\"ticks\")\n",
    "    bounds_palette = sns.mpl_palette(\"viridis\", n_colors=len(bounds))\n",
    "    risk_palette = sns.color_palette(\"flare\", n_colors=len(bounds))\n",
    "    risk_palette.reverse()\n",
    "    \n",
    "    ax = sns.barplot(experiments, x='View', hue='Bound_name', y='Bound', errorbar=\"sd\", width=0.8, hatch='.', palette=bounds_palette)\n",
    "    ax = sns.barplot(experiments, x='View', hue='Bound_name', y='Risk', errorbar=\"sd\", width=0.8, hatch='\\\\', palette=risk_palette)\n",
    "\n",
    "    plt.title(f'Test error rates and multiview PAC-Bayesian bound values, {m=},\\naveraged over {runs} runs')\n",
    "    plt.xlabel('Views')\n",
    "    plt.ylabel('Means')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels = [labels[i]+\" bound\" if i < len(labels)/2 else labels[i]+\" Gibbs risk\" for i in range(len(labels))]\n",
    "\n",
    "    # Creating a unified legend for both plots\n",
    "    plt.legend(handles, labels, title=\"Bounds and risks\", loc='upper right', fontsize='medium')\n",
    "    plt.tight_layout() \n",
    "    plt.gcf().set_size_inches(12, 8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df, len(Xs_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
