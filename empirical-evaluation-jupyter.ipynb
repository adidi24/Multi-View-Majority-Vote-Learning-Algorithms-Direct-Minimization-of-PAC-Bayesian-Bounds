{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains everything necessary to reproduce the experiments in our paper:  \n",
    "\n",
    "*Multi-View Majority Vote Learning Algorithms: Direct Minimization of PAC-Bayesian Bounds*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.core.defchararray import find, lower\n",
    "from termcolor import colored\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from mvpb import MultiViewMajorityVoteLearner, MajorityVoteLearner\n",
    "from mvpb.util import uniform_distribution\n",
    "\n",
    "\n",
    "# Import data\n",
    "from data import (SampleData,\n",
    "                           Nhanes,\n",
    "                           MultipleFeatures,\n",
    "                           MNIST_MV_Datasets,\n",
    "                           Fash_MNIST_MV_Datasets,\n",
    "                           EMNIST_Letters_MV_Datasets,\n",
    "                           Mushrooms,\n",
    "                           PTB_XL_plus,\n",
    "                           Nutrimouse,\n",
    "                           ReutersEN,\n",
    "                           IS,\n",
    "                           CorelImageFeatures,\n",
    "                           NUS_WIDE_OBJECT,\n",
    "                           ALOI,\n",
    "                           train_test_split,\n",
    "                           train_test_merge,\n",
    "                           s1_s2_split,\n",
    "                           multiclass_to_binary,\n",
    "                           balance_dataset,\n",
    "                           other_binary_options,\n",
    "                           poison_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare the multiview datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MultipleFeatures(size=\"large\")\n",
    "X_train, y_train, X_test, y_test = dataset.get_data()\n",
    "if isinstance(dataset, PTB_XL_plus):\n",
    "    real_classes = dataset.get_real_classes(np.unique(y_train))\n",
    "\n",
    "Xs_train = []\n",
    "Xs_test = []\n",
    "for xtr, xts in zip(X_train, X_test):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(xtr)\n",
    "    Xs_train.append(scaler.transform(xtr))\n",
    "    Xs_test.append(scaler.transform(xts))\n",
    "\n",
    "X_train_concat = [np.concatenate(Xs_train, axis=1)]\n",
    "X_test_concat = [np.concatenate(Xs_test, axis=1)]\n",
    "\n",
    "np.unique(y_train), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 200), (60000,), (10000, 200), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape, y_train.shape, X_test[1].shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='0'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = range(3)\n",
    "\n",
    "OPTIMIZE_LAMBDA_GAMMA = True\n",
    "# ALPHA = [1, 0.5, 1.1, 2]\n",
    "ALPHA = [0.5]\n",
    "MAX_ITER = 1000\n",
    "\n",
    "\n",
    "USE_DNDF = False\n",
    "stump_config = {\n",
    "    \"name\": \"stump\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 1,\n",
    "    \"max_features\": 0.5,\n",
    "}\n",
    "weak_learners_config = {\n",
    "    \"name\": \"weak_learner\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 3,\n",
    "    \"max_features\": 0.5,\n",
    "}\n",
    "strong_learners_config = {\n",
    "    \"name\": \"strong_learner\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 6,\n",
    "    \"max_features\": 0.8,\n",
    "}\n",
    "\n",
    "# CFG = [stump_config, weak_learners_config, strong_learners_config]\n",
    "CFG = [weak_learners_config]\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "TO_BINARY  = \"ovo\" # One of [\"ovr\", \"ovo\", \"other\",  None]\n",
    "label_1 = 4\n",
    "label_2 = 9\n",
    "\n",
    "POISON = False\n",
    "\n",
    "USE_UNLABELED = True\n",
    "# s_labeled_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5] if USE_UNLABELED else [1]\n",
    "s_labeled_sizes = [0.2] if USE_UNLABELED else [0.1]\n",
    "\n",
    "# BOUNDS = ['Uniform', 'PBkl', 'PBkl_inv', 'TND_DIS', 'TND_DIS_inv', 'TND', 'TND_inv', 'DIS', 'DIS_inv', 'Cbound', 'C_TND']\n",
    "BOUNDS = ['Uniform', 'PBkl']\n",
    "\n",
    "m = y_train.size #350\n",
    "test_size = 1 - (m  / (y_test.size+y_train.size))\n",
    "experiments = {}\n",
    "for s_labeled_size in s_labeled_sizes:\n",
    "    experiments[s_labeled_size] = {}\n",
    "    for alpha in ALPHA:\n",
    "        experiments[s_labeled_size][alpha] = {}\n",
    "        for cfg in CFG:\n",
    "            experiments[s_labeled_size][alpha][cfg[\"name\"]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0.2: {0.5: {'weak_learner': []}}}, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments, label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if POISON:\n",
    "#     Xs_train, y_train = poison_dataset(Xs_train, y_train, poison_label=label_1, target_label=label_2, target_view=3, num_samples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to binary OVR (One Vs Rest) or OVO (One Vs One) if needed\n",
    "if TO_BINARY == \"ovr\":\n",
    "    Xs_train, y_train, Xs_test, y_test = multiclass_to_binary(Xs_train, y_train, Xs_test, y_test, type=TO_BINARY, label_1=label_1)\n",
    "elif TO_BINARY == \"ovo\":\n",
    "    Xs_train, y_train, Xs_test, y_test = multiclass_to_binary(Xs_train, y_train, Xs_test, y_test, type=TO_BINARY, label_1=label_1, label_2=label_2)\n",
    "elif TO_BINARY == \"other\":\n",
    "    y_train, y_test = other_binary_options(dataset, y_train, y_test)\n",
    "else:\n",
    "    print(colored(f\"WARNING: TO_BINARY={TO_BINARY}, continuing\", 'yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([0, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11791, 200), (11791,), (1991, 200), (1991,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs_train[1].shape, y_train.shape, Xs_test[1].shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44m\u001b[30m############ Using 20.0% labeled data ############\u001b[0m\n",
      "\u001b[44m\u001b[30m\t############ Using alpha=0.5 ############\u001b[0m\n",
      "\u001b[44m\u001b[30m\t\t############ Using weak_learner ############\u001b[0m\n",
      "\u001b[34m\n",
      "----------------Run 1---------------\u001b[0m\n",
      "Training multiview classifier-------------------------------\n",
      "Training separate views classifiers-------------------------------\n",
      "Training concatenated view classifier-------------------------------\n",
      "\u001b[32mComputing the bound values ans risks -------------------------------\u001b[0m\n",
      "right_hand_side=0.00320597436600025, empirical_gibbs_risk=0.2974400225797347\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "### Multiview classifier gibbs risk before Optim: 0.2974400225797347\n",
      "\u001b[32mOptimizing PBkl for multiview classifier-------------------------------\u001b[0m\n",
      "\t Convergence reached after 0 iterations with alpha=1.8445345163345337\n",
      "\t Convergence reached after 1 iterations with alpha=1.7608983516693115\n",
      "\t Convergence reached after 2 iterations with alpha=1.6697301864624023\n",
      "\t Convergence reached after 3 iterations with alpha=1.574222207069397\n",
      "\t Convergence reached after 4 iterations with alpha=1.4755940437316895\n",
      "\t Convergence reached after 5 iterations with alpha=1.3745194673538208\n",
      "\t Convergence reached after 6 iterations with alpha=1.2714836597442627\n",
      "\t Convergence reached after 7 iterations with alpha=1.166927695274353\n",
      "\t Convergence reached after 8 iterations with alpha=1.061336636543274\n",
      "\t Convergence reached after 9 iterations with alpha=0.9553182721138\n",
      "\t Convergence reached after 10 iterations with alpha=0.849694013595581\n",
      "\t Convergence reached after 11 iterations with alpha=0.7456105947494507\n",
      "\t Convergence reached after 12 iterations with alpha=0.6446694135665894\n",
      "\t Convergence reached after 13 iterations with alpha=0.5490406155586243\n",
      "\t Convergence reached after 14 iterations with alpha=0.46148043870925903\n",
      "\t Convergence reached after 15 iterations with alpha=0.38512900471687317\n",
      "\t Convergence reached after 16 iterations with alpha=0.3230133056640625\n",
      "\t Convergence reached after 17 iterations with alpha=0.2773822247982025\n",
      "\t Convergence reached after 18 iterations with alpha=0.24920828640460968\n",
      "\t Convergence reached after 19 iterations with alpha=0.23812171816825867\n",
      "\t Convergence reached after 20 iterations with alpha=0.24272224307060242\n",
      "\t Convergence reached after 21 iterations with alpha=0.2610073983669281\n",
      "\t Convergence reached after 22 iterations with alpha=0.2907087504863739\n",
      "\t Convergence reached after 23 iterations with alpha=0.3294704258441925\n",
      "\t Convergence reached after 24 iterations with alpha=0.3748929500579834\n",
      "\t Convergence reached after 25 iterations with alpha=0.41618430614471436\n",
      "\t Convergence reached after 26 iterations with alpha=0.45361328125\n",
      "\t Convergence reached after 27 iterations with alpha=0.4874649941921234\n",
      "\t Convergence reached after 28 iterations with alpha=0.5180070996284485\n",
      "\t Convergence reached after 29 iterations with alpha=0.5454864501953125\n",
      "\t Convergence reached after 30 iterations with alpha=0.5459793210029602\n",
      "\t Convergence reached after 31 iterations with alpha=0.5464228987693787\n",
      "\t Convergence reached after 32 iterations with alpha=0.5468215346336365\n",
      "\t Convergence reached after 33 iterations with alpha=0.5471791625022888\n",
      "\t Convergence reached after 34 iterations with alpha=0.547499418258667\n",
      "\t Convergence reached after 35 iterations with alpha=0.5477855205535889\n",
      "\t Convergence reached after 36 iterations with alpha=0.548040509223938\n",
      "\t Convergence reached after 37 iterations with alpha=0.5482670664787292\n",
      "\t Convergence reached after 38 iterations with alpha=0.548467755317688\n",
      "\t Convergence reached after 39 iterations with alpha=0.5486448407173157\n",
      "\t Convergence reached after 40 iterations with alpha=0.5488003492355347\n",
      "\t Convergence reached after 41 iterations with alpha=0.5489361882209778\n",
      "\t Convergence reached after 42 iterations with alpha=0.5490541458129883\n",
      "\t Convergence reached after 43 iterations with alpha=0.5491557717323303\n",
      "\t Convergence reached after 44 iterations with alpha=0.5492424964904785\n",
      "\t Convergence reached after 45 iterations with alpha=0.5493156313896179\n",
      "\t Convergence reached after 46 iterations with alpha=0.549376368522644\n",
      "\t Convergence reached after 47 iterations with alpha=0.5494258403778076\n",
      "\t Convergence reached after 48 iterations with alpha=0.5494650602340698\n",
      "\t Convergence reached after 49 iterations with alpha=0.5494949221611023\n",
      "\t Convergence reached after 50 iterations with alpha=0.5495162606239319\n",
      "\t Convergence reached after 51 iterations with alpha=0.5495298504829407\n",
      "\t Convergence reached after 52 iterations with alpha=0.549536406993866\n",
      "\t Convergence reached after 53 iterations with alpha=0.5495365262031555\n",
      "\t Convergence reached after 54 iterations with alpha=0.5495307445526123\n",
      "\t Convergence reached after 55 iterations with alpha=0.5495196580886841\n",
      "\t Convergence reached after 56 iterations with alpha=0.5495036840438843\n",
      "\t Convergence reached after 57 iterations with alpha=0.5494832992553711\n",
      "\t Convergence reached after 58 iterations with alpha=0.549458920955658\n",
      "\t Convergence reached after 59 iterations with alpha=0.5494308471679688\n",
      "\t Convergence reached after 60 iterations with alpha=0.5493994355201721\n",
      "\t Convergence reached after 61 iterations with alpha=0.5493649840354919\n",
      "\t Convergence reached after 62 iterations with alpha=0.5493277907371521\n",
      "\t Convergence reached after 63 iterations with alpha=0.5492880940437317\n",
      "\t Convergence reached after 64 iterations with alpha=0.5492461323738098\n",
      "\t Convergence reached after 65 iterations with alpha=0.5492020845413208\n",
      "\t Convergence reached after 66 iterations with alpha=0.549156129360199\n",
      "\t Convergence reached after 67 iterations with alpha=0.5491085052490234\n",
      "\t Convergence reached after 68 iterations with alpha=0.5490593314170837\n",
      "\t Convergence reached after 69 iterations with alpha=0.5490087270736694\n",
      "\t Convergence reached after 70 iterations with alpha=0.5489568114280701\n",
      "\t Convergence reached after 71 iterations with alpha=0.54890376329422\n",
      "\t Convergence reached after 72 iterations with alpha=0.5488496422767639\n",
      "\t Convergence reached after 73 iterations with alpha=0.5487945079803467\n",
      "\t Convergence reached after 74 iterations with alpha=0.5487384796142578\n",
      "\t Convergence reached after 75 iterations with alpha=0.5486816763877869\n",
      "\t Convergence reached after 76 iterations with alpha=0.5486240983009338\n",
      "\t Convergence reached after 77 iterations with alpha=0.5485658645629883\n",
      "\t Convergence reached after 78 iterations with alpha=0.5485070943832397\n",
      "\t Convergence reached after 79 iterations with alpha=0.5484477877616882\n",
      "\t Convergence reached after 80 iterations with alpha=0.548446536064148\n",
      "\t Convergence reached after 81 iterations with alpha=0.5484452843666077\n",
      "\t Convergence reached after 82 iterations with alpha=0.5484440326690674\n",
      "\t Convergence reached after 83 iterations with alpha=0.5484427809715271\n",
      "\t Convergence reached after 84 iterations with alpha=0.5484415292739868\n",
      "\t Convergence reached after 85 iterations with alpha=0.5484402775764465\n",
      "\t Convergence reached after 86 iterations with alpha=0.5484390258789062\n",
      "\t Convergence reached after 87 iterations with alpha=0.5484377145767212\n",
      "\t Convergence reached after 88 iterations with alpha=0.5484364032745361\n",
      "\t Convergence reached after 89 iterations with alpha=0.5484350919723511\n",
      "\t Convergence reached after 90 iterations with alpha=0.548433780670166\n",
      "\t Convergence reached after 91 iterations with alpha=0.548432469367981\n",
      "\t Convergence reached after 92 iterations with alpha=0.5484311580657959\n",
      "\t Convergence reached after 93 iterations with alpha=0.5484298467636108\n",
      "\t Convergence reached after 94 iterations with alpha=0.5484285354614258\n",
      "\t Convergence reached after 95 iterations with alpha=0.5484272241592407\n",
      "\t Convergence reached after 96 iterations with alpha=0.5484259128570557\n",
      "\t Convergence reached after 97 iterations with alpha=0.5484246015548706\n",
      "\t Convergence reached after 98 iterations with alpha=0.5484232902526855\n",
      "\t Convergence reached after 99 iterations with alpha=0.5484219789505005\n",
      "\t Convergence reached after 100 iterations with alpha=0.5484219193458557\n",
      "\t Convergence reached after 101 iterations with alpha=0.5484218597412109\n",
      "\t Convergence reached after 102 iterations with alpha=0.5484218001365662\n",
      "\t Convergence reached after 103 iterations with alpha=0.5484217405319214\n",
      "\t Convergence reached after 104 iterations with alpha=0.5484216809272766\n",
      "\t Convergence reached after 105 iterations with alpha=0.5484216213226318\n",
      "\t Convergence reached after 106 iterations with alpha=0.5484215617179871\n",
      "\t Convergence reached after 107 iterations, alpha=0.5484215021133423\n",
      "\u001b[33mOptimization took 0:00:05.350517 -------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing PBkl for separate views classifiers-------------------------------\u001b[0m\n",
      "Iteration: 0,\t Loss: 1.0201712846755981, alpha=0.13138467073440552\n",
      "Iteration: 1,\t Loss: 0.8877633213996887, alpha=0.2051025629043579\n",
      "Iteration: 2,\t Loss: 0.8956623673439026, alpha=0.28613564372062683\n",
      "Iteration: 3,\t Loss: 0.9067912697792053, alpha=0.37235116958618164\n",
      "Iteration: 4,\t Loss: 0.9157626032829285, alpha=0.46247613430023193\n",
      "Iteration: 5,\t Loss: 0.9218578338623047, alpha=0.5499840378761292\n",
      "Iteration: 6,\t Loss: 0.9251316785812378, alpha=0.602388858795166\n",
      "Iteration: 7,\t Loss: 0.925818145275116, alpha=0.5997481942176819\n",
      "Iteration: 8,\t Loss: 0.9241870045661926, alpha=0.56591796875\n",
      "Iteration: 9,\t Loss: 0.9205306768417358, alpha=0.517708420753479\n",
      "Iteration: 10,\t Loss: 0.9151422381401062, alpha=0.47258460521698\n",
      "Iteration: 11,\t Loss: 0.908303439617157, alpha=0.45546332001686096\n",
      "Iteration: 12,\t Loss: 0.9002684950828552, alpha=0.46983733773231506\n",
      "Iteration: 13,\t Loss: 0.8912732005119324, alpha=0.5034829378128052\n",
      "Iteration: 14,\t Loss: 0.8815470933914185, alpha=0.5424806475639343\n",
      "Iteration: 15,\t Loss: 0.8713114857673645, alpha=0.5655050873756409\n",
      "Iteration: 16,\t Loss: 0.8607683777809143, alpha=0.5592727065086365\n",
      "Iteration: 17,\t Loss: 0.8501023054122925, alpha=0.5334293246269226\n",
      "Iteration: 18,\t Loss: 0.8394990563392639, alpha=0.5063242316246033\n",
      "Iteration: 19,\t Loss: 0.8291419744491577, alpha=0.5011334419250488\n",
      "Iteration: 20,\t Loss: 0.8191945552825928, alpha=0.5202293992042542\n",
      "Iteration: 21,\t Loss: 0.8098209500312805, alpha=0.5479681491851807\n",
      "Iteration: 22,\t Loss: 0.801207423210144, alpha=0.5570842027664185\n",
      "Iteration: 23,\t Loss: 0.7935448884963989, alpha=0.5398868322372437\n",
      "Iteration: 24,\t Loss: 0.787042498588562, alpha=0.5195386409759521\n",
      "Iteration: 25,\t Loss: 0.7819573283195496, alpha=0.5269487500190735\n",
      "Iteration: 26,\t Loss: 0.7785240411758423, alpha=0.551025390625\n",
      "Iteration: 27,\t Loss: 0.7769201993942261, alpha=0.5513518452644348\n",
      "Iteration: 28,\t Loss: 0.7770487666130066, alpha=0.5306479334831238\n",
      "Iteration: 29,\t Loss: 0.778235912322998, alpha=0.5362244248390198\n",
      "Iteration: 30,\t Loss: 0.779046356678009, alpha=0.5366101861000061\n",
      "Iteration: 31,\t Loss: 0.7790215015411377, alpha=0.5372189283370972\n",
      "Iteration: 32,\t Loss: 0.7789512872695923, alpha=0.5379995703697205\n",
      "Iteration: 33,\t Loss: 0.7788389325141907, alpha=0.5389026403427124\n",
      "Iteration: 34,\t Loss: 0.7786893248558044, alpha=0.5398804545402527\n",
      "Iteration: 35,\t Loss: 0.778506338596344, alpha=0.5408867597579956\n",
      "Iteration: 36,\t Loss: 0.7782944440841675, alpha=0.541877269744873\n",
      "Iteration: 37,\t Loss: 0.7780575752258301, alpha=0.5428104400634766\n",
      "Iteration: 38,\t Loss: 0.777799129486084, alpha=0.5436487793922424\n",
      "Iteration: 39,\t Loss: 0.7775233387947083, alpha=0.5443611741065979\n",
      "Iteration: 40,\t Loss: 0.7772331833839417, alpha=0.544924259185791\n",
      "Iteration: 41,\t Loss: 0.7769321203231812, alpha=0.5453238487243652\n",
      "Iteration: 42,\t Loss: 0.7766222357749939, alpha=0.5455553531646729\n",
      "Iteration: 43,\t Loss: 0.7763067483901978, alpha=0.5456229448318481\n",
      "Iteration: 44,\t Loss: 0.7759878635406494, alpha=0.5455384850502014\n",
      "Iteration: 45,\t Loss: 0.7756673693656921, alpha=0.5453197956085205\n",
      "Iteration: 46,\t Loss: 0.7753471732139587, alpha=0.5449889898300171\n",
      "Iteration: 47,\t Loss: 0.7750284671783447, alpha=0.544571042060852\n",
      "Iteration: 48,\t Loss: 0.7747131586074829, alpha=0.5440921783447266\n",
      "Iteration: 49,\t Loss: 0.7744020223617554, alpha=0.5435791015625\n",
      "Iteration: 50,\t Loss: 0.774095892906189, alpha=0.5430575609207153\n",
      "Iteration: 51,\t Loss: 0.7737959027290344, alpha=0.542551577091217\n",
      "Iteration: 52,\t Loss: 0.7735021710395813, alpha=0.5420823693275452\n",
      "Iteration: 53,\t Loss: 0.7732155323028564, alpha=0.541667640209198\n",
      "Iteration: 54,\t Loss: 0.7729360461235046, alpha=0.5413211584091187\n",
      "Iteration: 55,\t Loss: 0.7726643085479736, alpha=0.5410521626472473\n",
      "Iteration: 56,\t Loss: 0.7724003195762634, alpha=0.5408652424812317\n",
      "Iteration: 57,\t Loss: 0.7721439003944397, alpha=0.5407606959342957\n",
      "Iteration: 58,\t Loss: 0.7718956470489502, alpha=0.5407348871231079\n",
      "Iteration: 59,\t Loss: 0.7716551423072815, alpha=0.5407806038856506\n",
      "Iteration: 60,\t Loss: 0.7714223861694336, alpha=0.5408880114555359\n",
      "Iteration: 61,\t Loss: 0.7711971402168274, alpha=0.5410451292991638\n",
      "Iteration: 62,\t Loss: 0.7709793448448181, alpha=0.5412387847900391\n",
      "Iteration: 63,\t Loss: 0.7707690000534058, alpha=0.5414553284645081\n",
      "Iteration: 64,\t Loss: 0.7705656290054321, alpha=0.5416810512542725\n",
      "Iteration: 65,\t Loss: 0.7703691720962524, alpha=0.5419031977653503\n",
      "Iteration: 66,\t Loss: 0.7701795697212219, alpha=0.5421101450920105\n",
      "Iteration: 67,\t Loss: 0.7699962258338928, alpha=0.5422919988632202\n",
      "Iteration: 68,\t Loss: 0.7698191404342651, alpha=0.5424410700798035\n",
      "Iteration: 69,\t Loss: 0.7696480751037598, alpha=0.5425518751144409\n",
      "Iteration: 70,\t Loss: 0.7694827318191528, alpha=0.5426211953163147\n",
      "Iteration: 71,\t Loss: 0.7693227529525757, alpha=0.5426481366157532\n",
      "Iteration: 72,\t Loss: 0.7691682577133179, alpha=0.5426340103149414\n",
      "Iteration: 73,\t Loss: 0.7690187096595764, alpha=0.5425820350646973\n",
      "Iteration: 74,\t Loss: 0.7688741087913513, alpha=0.5424970388412476\n",
      "Iteration: 75,\t Loss: 0.7687339186668396, alpha=0.542384922504425\n",
      "Iteration: 76,\t Loss: 0.7685983180999756, alpha=0.5422524809837341\n",
      "Iteration: 77,\t Loss: 0.7684667110443115, alpha=0.5421068668365479\n",
      "Iteration: 78,\t Loss: 0.7683390974998474, alpha=0.5419552326202393\n",
      "Iteration: 79,\t Loss: 0.7682152390480042, alpha=0.5418045520782471\n",
      "Iteration: 80,\t Loss: 0.7680951952934265, alpha=0.5418016910552979\n",
      "Iteration: 81,\t Loss: 0.7680928707122803, alpha=0.5417989492416382\n",
      "Iteration: 82,\t Loss: 0.7680904865264893, alpha=0.5417963266372681\n",
      "Iteration: 83,\t Loss: 0.7680882215499878, alpha=0.5417938232421875\n",
      "Iteration: 84,\t Loss: 0.7680858969688416, alpha=0.5417914390563965\n",
      "Iteration: 85,\t Loss: 0.7680837512016296, alpha=0.5417891144752502\n",
      "Iteration: 86,\t Loss: 0.7680814266204834, alpha=0.5417869091033936\n",
      "Iteration: 87,\t Loss: 0.7680791616439819, alpha=0.5417847633361816\n",
      "Iteration: 88,\t Loss: 0.7680768966674805, alpha=0.5417826771736145\n",
      "Iteration: 89,\t Loss: 0.7680748105049133, alpha=0.5417807102203369\n",
      "Iteration: 90,\t Loss: 0.7680726647377014, alpha=0.5417788028717041\n",
      "Iteration: 91,\t Loss: 0.7680703401565552, alpha=0.5417769551277161\n",
      "Iteration: 92,\t Loss: 0.768068253993988, alpha=0.5417751669883728\n",
      "Iteration: 93,\t Loss: 0.7680659890174866, alpha=0.5417733788490295\n",
      "Iteration: 94,\t Loss: 0.7680638432502747, alpha=0.541771650314331\n",
      "Iteration: 95,\t Loss: 0.768061637878418, alpha=0.5417699813842773\n",
      "Iteration: 96,\t Loss: 0.768059492111206, alpha=0.5417683720588684\n",
      "Iteration: 97,\t Loss: 0.7680572867393494, alpha=0.5417667627334595\n",
      "Iteration: 98,\t Loss: 0.7680552005767822, alpha=0.5417652130126953\n",
      "Iteration: 99,\t Loss: 0.7680529952049255, alpha=0.5417637228965759\n",
      "Iteration: 100,\t Loss: 0.7680508494377136, alpha=0.5417636632919312\n",
      "Iteration: 101,\t Loss: 0.7680507898330688, alpha=0.5417636036872864\n",
      "Iteration: 102,\t Loss: 0.7680506706237793, alpha=0.5417635440826416\n",
      "Iteration: 103,\t Loss: 0.7680506110191345, alpha=0.5417634844779968\n",
      "Iteration: 104,\t Loss: 0.7680507302284241, alpha=0.541763424873352\n",
      "Iteration: 105,\t Loss: 0.7680505514144897, alpha=0.5417633652687073\n",
      "\t Convergence reached after 106 iterations with alpha=0.5417633056640625\n",
      "Iteration: 0,\t Loss: 248.26348876953125, alpha=1.712900996208191\n",
      "Iteration: 1,\t Loss: 121.86604309082031, alpha=1.6299233436584473\n",
      "Iteration: 2,\t Loss: 75.91581726074219, alpha=1.541886806488037\n",
      "Iteration: 3,\t Loss: 50.64387512207031, alpha=1.4500417709350586\n",
      "Iteration: 4,\t Loss: 34.56706619262695, alpha=1.3551580905914307\n",
      "Iteration: 5,\t Loss: 23.407947540283203, alpha=1.2576820850372314\n",
      "Iteration: 6,\t Loss: 15.202277183532715, alpha=1.1579134464263916\n",
      "Iteration: 7,\t Loss: 8.917390823364258, alpha=1.05610990524292\n",
      "Iteration: 8,\t Loss: 3.9559326171875, alpha=0.9525765776634216\n",
      "Iteration: 9,\t Loss: 1.022153615951538, alpha=0.8594732880592346\n",
      "Iteration: 10,\t Loss: 0.9467520117759705, alpha=0.7755495309829712\n",
      "Iteration: 11,\t Loss: 0.8918663859367371, alpha=0.6997069120407104\n",
      "Iteration: 12,\t Loss: 0.8479273915290833, alpha=0.6310515999794006\n",
      "Iteration: 13,\t Loss: 0.8117515444755554, alpha=0.5688487887382507\n",
      "Iteration: 14,\t Loss: 0.7814545631408691, alpha=0.5124855041503906\n",
      "Iteration: 15,\t Loss: 0.755757212638855, alpha=0.4614427089691162\n",
      "Iteration: 16,\t Loss: 0.7337368130683899, alpha=0.4152742326259613\n",
      "Iteration: 17,\t Loss: 0.714705228805542, alpha=0.3735910654067993\n",
      "Iteration: 18,\t Loss: 0.6981355547904968, alpha=0.3360496461391449\n",
      "Iteration: 19,\t Loss: 0.6836162805557251, alpha=0.3023430109024048\n",
      "Iteration: 20,\t Loss: 0.670821487903595, alpha=0.27219417691230774\n",
      "Iteration: 21,\t Loss: 0.6594892144203186, alpha=0.24535107612609863\n",
      "Iteration: 22,\t Loss: 0.6494066119194031, alpha=0.22158272564411163\n",
      "Iteration: 23,\t Loss: 0.6403985619544983, alpha=0.20067617297172546\n",
      "Iteration: 24,\t Loss: 0.6323203444480896, alpha=0.18243426084518433\n",
      "Iteration: 25,\t Loss: 0.6250503659248352, alpha=0.1666737049818039\n",
      "Iteration: 26,\t Loss: 0.6184868216514587, alpha=0.15322360396385193\n",
      "Iteration: 27,\t Loss: 0.6125430464744568, alpha=0.141924187541008\n",
      "Iteration: 28,\t Loss: 0.6071451902389526, alpha=0.13262580335140228\n",
      "Iteration: 29,\t Loss: 0.6022297739982605, alpha=0.12518799304962158\n",
      "Iteration: 30,\t Loss: 0.5977422595024109, alpha=0.12507380545139313\n",
      "Iteration: 31,\t Loss: 0.5976595878601074, alpha=0.12499072402715683\n",
      "Iteration: 32,\t Loss: 0.597582221031189, alpha=0.12493623793125153\n",
      "Iteration: 33,\t Loss: 0.597509503364563, alpha=0.12490808218717575\n",
      "Iteration: 34,\t Loss: 0.5974414944648743, alpha=0.12490416318178177\n",
      "Iteration: 35,\t Loss: 0.597377359867096, alpha=0.12492258101701736\n",
      "Iteration: 36,\t Loss: 0.5973168015480042, alpha=0.12496160715818405\n",
      "Iteration: 37,\t Loss: 0.5972597599029541, alpha=0.12501965463161469\n",
      "Iteration: 38,\t Loss: 0.597205638885498, alpha=0.12509526312351227\n",
      "Iteration: 39,\t Loss: 0.5971542596817017, alpha=0.12518709897994995\n",
      "Iteration: 40,\t Loss: 0.5971055030822754, alpha=0.12529395520687103\n",
      "Iteration: 41,\t Loss: 0.5970590710639954, alpha=0.12541471421718597\n",
      "Iteration: 42,\t Loss: 0.5970146656036377, alpha=0.1255483627319336\n",
      "Iteration: 43,\t Loss: 0.5969721674919128, alpha=0.12569397687911987\n",
      "Iteration: 44,\t Loss: 0.5969313383102417, alpha=0.12585070729255676\n",
      "Iteration: 45,\t Loss: 0.5968921184539795, alpha=0.126017764210701\n",
      "Iteration: 46,\t Loss: 0.5968543291091919, alpha=0.12619444727897644\n",
      "Iteration: 47,\t Loss: 0.5968179106712341, alpha=0.1263800859451294\n",
      "Iteration: 48,\t Loss: 0.5967826247215271, alpha=0.1265740990638733\n",
      "Iteration: 49,\t Loss: 0.5967482924461365, alpha=0.12677592039108276\n",
      "Iteration: 50,\t Loss: 0.596714973449707, alpha=0.1269850730895996\n",
      "Iteration: 51,\t Loss: 0.5966827869415283, alpha=0.12720108032226562\n",
      "Iteration: 52,\t Loss: 0.5966511964797974, alpha=0.12742352485656738\n",
      "Iteration: 53,\t Loss: 0.5966203212738037, alpha=0.12765203416347504\n",
      "Iteration: 54,\t Loss: 0.5965901613235474, alpha=0.12788623571395874\n",
      "Iteration: 55,\t Loss: 0.5965606570243835, alpha=0.12812581658363342\n",
      "Iteration: 56,\t Loss: 0.5965315699577332, alpha=0.1283704787492752\n",
      "Iteration: 57,\t Loss: 0.596502959728241, alpha=0.1286199390888214\n",
      "Iteration: 58,\t Loss: 0.596474826335907, alpha=0.12887397408485413\n",
      "Iteration: 59,\t Loss: 0.596447229385376, alpha=0.12913233041763306\n",
      "Iteration: 60,\t Loss: 0.5964199304580688, alpha=0.12939481437206268\n",
      "Iteration: 61,\t Loss: 0.596392810344696, alpha=0.12966123223304749\n",
      "Iteration: 62,\t Loss: 0.5963661074638367, alpha=0.12993139028549194\n",
      "Iteration: 63,\t Loss: 0.5963395833969116, alpha=0.13020513951778412\n",
      "Iteration: 64,\t Loss: 0.5963135361671448, alpha=0.13048233091831207\n",
      "Iteration: 65,\t Loss: 0.5962875485420227, alpha=0.13076281547546387\n",
      "Iteration: 66,\t Loss: 0.5962616205215454, alpha=0.13104647397994995\n",
      "Iteration: 67,\t Loss: 0.5962360501289368, alpha=0.13133318722248077\n",
      "Iteration: 68,\t Loss: 0.5962105393409729, alpha=0.13162283599376678\n",
      "Iteration: 69,\t Loss: 0.5961852669715881, alpha=0.13191534578800201\n",
      "Iteration: 70,\t Loss: 0.5961599946022034, alpha=0.1322106122970581\n",
      "Iteration: 71,\t Loss: 0.5961350202560425, alpha=0.1325085461139679\n",
      "Iteration: 72,\t Loss: 0.5961099863052368, alpha=0.13280907273292542\n",
      "Iteration: 73,\t Loss: 0.5960850119590759, alpha=0.1331121176481247\n",
      "Iteration: 74,\t Loss: 0.5960601568222046, alpha=0.13341762125492096\n",
      "Iteration: 75,\t Loss: 0.5960354208946228, alpha=0.13372550904750824\n",
      "Iteration: 76,\t Loss: 0.5960108041763306, alpha=0.13403570652008057\n",
      "Iteration: 77,\t Loss: 0.5959860682487488, alpha=0.13434818387031555\n",
      "Iteration: 78,\t Loss: 0.5959615707397461, alpha=0.13466289639472961\n",
      "Iteration: 79,\t Loss: 0.5959369540214539, alpha=0.13497978448867798\n",
      "Iteration: 80,\t Loss: 0.595912516117096, alpha=0.13498616218566895\n",
      "Iteration: 81,\t Loss: 0.5959119200706482, alpha=0.1349925845861435\n",
      "Iteration: 82,\t Loss: 0.59591144323349, alpha=0.13499905169010162\n",
      "Iteration: 83,\t Loss: 0.595910906791687, alpha=0.13500556349754333\n",
      "Iteration: 84,\t Loss: 0.5959104895591736, alpha=0.13501212000846863\n",
      "Iteration: 85,\t Loss: 0.5959100127220154, alpha=0.1350187063217163\n",
      "Iteration: 86,\t Loss: 0.5959094762802124, alpha=0.13502533733844757\n",
      "Iteration: 87,\t Loss: 0.5959089994430542, alpha=0.13503201305866241\n",
      "Iteration: 88,\t Loss: 0.595908522605896, alpha=0.13503873348236084\n",
      "Iteration: 89,\t Loss: 0.595907986164093, alpha=0.13504548370838165\n",
      "Iteration: 90,\t Loss: 0.59590744972229, alpha=0.13505227863788605\n",
      "Iteration: 91,\t Loss: 0.5959069728851318, alpha=0.13505911827087402\n",
      "Iteration: 92,\t Loss: 0.5959065556526184, alpha=0.1350659877061844\n",
      "Iteration: 93,\t Loss: 0.5959060192108154, alpha=0.13507290184497833\n",
      "Iteration: 94,\t Loss: 0.5959054827690125, alpha=0.13507986068725586\n",
      "Iteration: 95,\t Loss: 0.595905065536499, alpha=0.13508684933185577\n",
      "Iteration: 96,\t Loss: 0.595904529094696, alpha=0.13509388267993927\n",
      "Iteration: 97,\t Loss: 0.5959039926528931, alpha=0.13510094583034515\n",
      "Iteration: 98,\t Loss: 0.5959035158157349, alpha=0.13510805368423462\n",
      "Iteration: 99,\t Loss: 0.5959030389785767, alpha=0.13511519134044647\n",
      "Iteration: 100,\t Loss: 0.5959025025367737, alpha=0.13511532545089722\n",
      "\t Convergence reached after 101 iterations with alpha=0.13511545956134796\n",
      "Iteration: 0,\t Loss: 2.3229808807373047, alpha=0.7648258805274963\n",
      "Iteration: 1,\t Loss: 0.9567313194274902, alpha=0.6867859363555908\n",
      "Iteration: 2,\t Loss: 0.8939947485923767, alpha=0.6003625988960266\n",
      "Iteration: 3,\t Loss: 0.8507229089736938, alpha=0.5079799890518188\n",
      "Iteration: 4,\t Loss: 0.8178776502609253, alpha=0.42795220017433167\n",
      "Iteration: 5,\t Loss: 0.7916930317878723, alpha=0.41543400287628174\n",
      "Iteration: 6,\t Loss: 0.7701680064201355, alpha=0.4438280165195465\n",
      "Iteration: 7,\t Loss: 0.7520979642868042, alpha=0.4905466139316559\n",
      "Iteration: 8,\t Loss: 0.736694872379303, alpha=0.5392288565635681\n",
      "Iteration: 9,\t Loss: 0.7234143018722534, alpha=0.5635318160057068\n",
      "Iteration: 10,\t Loss: 0.7118622660636902, alpha=0.5537369251251221\n",
      "Iteration: 11,\t Loss: 0.7017425894737244, alpha=0.5234376788139343\n",
      "Iteration: 12,\t Loss: 0.6928306221961975, alpha=0.4891970753669739\n",
      "Iteration: 13,\t Loss: 0.6849510073661804, alpha=0.4724964201450348\n",
      "Iteration: 14,\t Loss: 0.677962064743042, alpha=0.4817490577697754\n",
      "Iteration: 15,\t Loss: 0.6717492341995239, alpha=0.5074692368507385\n",
      "Iteration: 16,\t Loss: 0.6662194132804871, alpha=0.533997654914856\n",
      "Iteration: 17,\t Loss: 0.6612959504127502, alpha=0.5422326922416687\n",
      "Iteration: 18,\t Loss: 0.6569141149520874, alpha=0.5286933779716492\n",
      "Iteration: 19,\t Loss: 0.6530185341835022, alpha=0.5056567788124084\n",
      "Iteration: 20,\t Loss: 0.6495636701583862, alpha=0.49201875925064087\n",
      "Iteration: 21,\t Loss: 0.6465098857879639, alpha=0.4988449215888977\n",
      "Iteration: 22,\t Loss: 0.6438217163085938, alpha=0.5185848474502563\n",
      "Iteration: 23,\t Loss: 0.6414700150489807, alpha=0.5336723327636719\n",
      "Iteration: 24,\t Loss: 0.6394281387329102, alpha=0.5297291278839111\n",
      "Iteration: 25,\t Loss: 0.6376715898513794, alpha=0.51285320520401\n",
      "Iteration: 26,\t Loss: 0.6361784338951111, alpha=0.5015614628791809\n",
      "Iteration: 27,\t Loss: 0.6349290609359741, alpha=0.5082904100418091\n",
      "Iteration: 28,\t Loss: 0.6339031457901001, alpha=0.5244253873825073\n",
      "Iteration: 29,\t Loss: 0.6330816745758057, alpha=0.5305300354957581\n",
      "Iteration: 30,\t Loss: 0.6324453353881836, alpha=0.5303180813789368\n",
      "Iteration: 31,\t Loss: 0.6324347257614136, alpha=0.5298252105712891\n",
      "Iteration: 32,\t Loss: 0.6324242949485779, alpha=0.5291002988815308\n",
      "Iteration: 33,\t Loss: 0.6324144601821899, alpha=0.5281880497932434\n",
      "Iteration: 34,\t Loss: 0.6324048042297363, alpha=0.5271292924880981\n",
      "Iteration: 35,\t Loss: 0.6323957443237305, alpha=0.5259614586830139\n",
      "Iteration: 36,\t Loss: 0.6323866248130798, alpha=0.5247192978858948\n",
      "Iteration: 37,\t Loss: 0.632378101348877, alpha=0.5234352946281433\n",
      "Iteration: 38,\t Loss: 0.6323696970939636, alpha=0.522139847278595\n",
      "Iteration: 39,\t Loss: 0.6323614120483398, alpha=0.5208616256713867\n",
      "Iteration: 40,\t Loss: 0.6323534250259399, alpha=0.5196273922920227\n",
      "Iteration: 41,\t Loss: 0.63234543800354, alpha=0.5184617042541504\n",
      "Iteration: 42,\t Loss: 0.6323374509811401, alpha=0.5173863768577576\n",
      "Iteration: 43,\t Loss: 0.6323298811912537, alpha=0.5164203643798828\n",
      "Iteration: 44,\t Loss: 0.6323220729827881, alpha=0.5155787467956543\n",
      "Iteration: 45,\t Loss: 0.6323146224021912, alpha=0.5148727297782898\n",
      "Iteration: 46,\t Loss: 0.6323069930076599, alpha=0.5143091082572937\n",
      "Iteration: 47,\t Loss: 0.6322992444038391, alpha=0.5138903856277466\n",
      "Iteration: 48,\t Loss: 0.6322919130325317, alpha=0.513615071773529\n",
      "Iteration: 49,\t Loss: 0.6322843432426453, alpha=0.5134777426719666\n",
      "Iteration: 50,\t Loss: 0.6322768330574036, alpha=0.513469934463501\n",
      "Iteration: 51,\t Loss: 0.6322693824768066, alpha=0.5135804414749146\n",
      "Iteration: 52,\t Loss: 0.6322617530822754, alpha=0.5137959718704224\n",
      "Iteration: 53,\t Loss: 0.6322541236877441, alpha=0.5141016840934753\n",
      "Iteration: 54,\t Loss: 0.6322466731071472, alpha=0.5144817233085632\n",
      "Iteration: 55,\t Loss: 0.6322391033172607, alpha=0.5149197578430176\n",
      "Iteration: 56,\t Loss: 0.6322316527366638, alpha=0.5153992772102356\n",
      "Iteration: 57,\t Loss: 0.6322239637374878, alpha=0.5159040689468384\n",
      "Iteration: 58,\t Loss: 0.632216215133667, alpha=0.5164185762405396\n",
      "Iteration: 59,\t Loss: 0.6322085857391357, alpha=0.5169280767440796\n",
      "Iteration: 60,\t Loss: 0.6322007179260254, alpha=0.517419159412384\n",
      "Iteration: 61,\t Loss: 0.632192850112915, alpha=0.5178797841072083\n",
      "Iteration: 62,\t Loss: 0.6321849226951599, alpha=0.5182996988296509\n",
      "Iteration: 63,\t Loss: 0.63217693567276, alpha=0.5186705589294434\n",
      "Iteration: 64,\t Loss: 0.6321689486503601, alpha=0.5189860463142395\n",
      "Iteration: 65,\t Loss: 0.6321609020233154, alpha=0.5192417502403259\n",
      "Iteration: 66,\t Loss: 0.6321527361869812, alpha=0.5194355249404907\n",
      "Iteration: 67,\t Loss: 0.6321445107460022, alpha=0.51956707239151\n",
      "Iteration: 68,\t Loss: 0.6321362257003784, alpha=0.5196380615234375\n",
      "Iteration: 69,\t Loss: 0.6321278214454651, alpha=0.5196517109870911\n",
      "Iteration: 70,\t Loss: 0.6321195363998413, alpha=0.5196127891540527\n",
      "Iteration: 71,\t Loss: 0.6321110725402832, alpha=0.5195271372795105\n",
      "Iteration: 72,\t Loss: 0.6321024298667908, alpha=0.5194016098976135\n",
      "Iteration: 73,\t Loss: 0.6320938467979431, alpha=0.5192437767982483\n",
      "Iteration: 74,\t Loss: 0.6320852637290955, alpha=0.5190615057945251\n",
      "Iteration: 75,\t Loss: 0.6320765018463135, alpha=0.518862783908844\n",
      "Iteration: 76,\t Loss: 0.632067859172821, alpha=0.51865553855896\n",
      "Iteration: 77,\t Loss: 0.6320587396621704, alpha=0.5184472799301147\n",
      "Iteration: 78,\t Loss: 0.6320500373840332, alpha=0.5182448625564575\n",
      "Iteration: 79,\t Loss: 0.6320408582687378, alpha=0.5180544853210449\n",
      "Iteration: 80,\t Loss: 0.6320319771766663, alpha=0.5180509686470032\n",
      "Iteration: 81,\t Loss: 0.6320316791534424, alpha=0.5180478096008301\n",
      "\t Convergence reached after 82 iterations with alpha=0.5180449485778809\n",
      "Iteration: 0,\t Loss: 0.7890364527702332, alpha=0.696144163608551\n",
      "Iteration: 1,\t Loss: 0.7098286151885986, alpha=0.6185301542282104\n",
      "Iteration: 2,\t Loss: 0.6465346217155457, alpha=0.534793496131897\n",
      "Iteration: 3,\t Loss: 0.5942878723144531, alpha=0.4456595182418823\n",
      "Iteration: 4,\t Loss: 0.5515105128288269, alpha=0.45941415429115295\n",
      "Iteration: 5,\t Loss: 0.5175626873970032, alpha=0.5072705745697021\n",
      "Iteration: 6,\t Loss: 0.4935086667537689, alpha=0.5545443892478943\n",
      "Iteration: 7,\t Loss: 0.4855101406574249, alpha=0.5218328237533569\n",
      "Iteration: 8,\t Loss: 0.5157636404037476, alpha=0.48125484585762024\n",
      "Iteration: 9,\t Loss: 0.5052725672721863, alpha=0.5105716586112976\n",
      "Iteration: 10,\t Loss: 0.47556430101394653, alpha=0.5471034646034241\n",
      "Iteration: 11,\t Loss: 0.4595339298248291, alpha=0.5613948702812195\n",
      "Iteration: 12,\t Loss: 0.4517994225025177, alpha=0.5517085194587708\n",
      "Iteration: 13,\t Loss: 0.4477410316467285, alpha=0.5306631922721863\n",
      "Iteration: 14,\t Loss: 0.4451223313808441, alpha=0.5127546191215515\n",
      "Iteration: 15,\t Loss: 0.44285881519317627, alpha=0.509472668170929\n",
      "Iteration: 16,\t Loss: 0.44041094183921814, alpha=0.5217267274856567\n",
      "Iteration: 17,\t Loss: 0.4375249743461609, alpha=0.5421340465545654\n",
      "Iteration: 18,\t Loss: 0.4341120421886444, alpha=0.5588416457176208\n",
      "Iteration: 19,\t Loss: 0.430178701877594, alpha=0.5611489415168762\n",
      "Iteration: 20,\t Loss: 0.4257871210575104, alpha=0.5493446588516235\n",
      "Iteration: 21,\t Loss: 0.42103710770606995, alpha=0.5339372158050537\n",
      "Iteration: 22,\t Loss: 0.41605344414711, alpha=0.5290529131889343\n",
      "Iteration: 23,\t Loss: 0.4109691083431244, alpha=0.5395493507385254\n",
      "Iteration: 24,\t Loss: 0.4059221148490906, alpha=0.5559355020523071\n",
      "Iteration: 25,\t Loss: 0.4010573625564575, alpha=0.5608234405517578\n",
      "Iteration: 26,\t Loss: 0.39651355147361755, alpha=0.5493540167808533\n",
      "Iteration: 27,\t Loss: 0.392416775226593, alpha=0.5383325219154358\n",
      "Iteration: 28,\t Loss: 0.388870507478714, alpha=0.5455630421638489\n",
      "Iteration: 29,\t Loss: 0.3859138488769531, alpha=0.5596203804016113\n",
      "Iteration: 30,\t Loss: 0.38349205255508423, alpha=0.5595126152038574\n",
      "Iteration: 31,\t Loss: 0.3834460377693176, alpha=0.5590812563896179\n",
      "Iteration: 32,\t Loss: 0.383396714925766, alpha=0.5583876967430115\n",
      "Iteration: 33,\t Loss: 0.3833441138267517, alpha=0.5574883818626404\n",
      "Iteration: 34,\t Loss: 0.38328856229782104, alpha=0.5564352869987488\n",
      "Iteration: 35,\t Loss: 0.3832302391529083, alpha=0.5552763342857361\n",
      "Iteration: 36,\t Loss: 0.3831694424152374, alpha=0.5540566444396973\n",
      "Iteration: 37,\t Loss: 0.3831063210964203, alpha=0.552819013595581\n",
      "Iteration: 38,\t Loss: 0.3830411732196808, alpha=0.5516040325164795\n",
      "Iteration: 39,\t Loss: 0.38297390937805176, alpha=0.5504496693611145\n",
      "Iteration: 40,\t Loss: 0.3829050362110138, alpha=0.5493903160095215\n",
      "Iteration: 41,\t Loss: 0.3828345239162445, alpha=0.5484554767608643\n",
      "Iteration: 42,\t Loss: 0.3827626407146454, alpha=0.54766845703125\n",
      "Iteration: 43,\t Loss: 0.3826894462108612, alpha=0.5470451712608337\n",
      "Iteration: 44,\t Loss: 0.3826150596141815, alpha=0.5465937852859497\n",
      "Iteration: 45,\t Loss: 0.3825396001338959, alpha=0.5463148355484009\n",
      "Iteration: 46,\t Loss: 0.3824633061885834, alpha=0.546201765537262\n",
      "Iteration: 47,\t Loss: 0.38238614797592163, alpha=0.5462421774864197\n",
      "Iteration: 48,\t Loss: 0.38230833411216736, alpha=0.5464190244674683\n",
      "Iteration: 49,\t Loss: 0.38222989439964294, alpha=0.546711802482605\n",
      "Iteration: 50,\t Loss: 0.3821510374546051, alpha=0.5470978021621704\n",
      "Iteration: 51,\t Loss: 0.38207173347473145, alpha=0.5475529432296753\n",
      "Iteration: 52,\t Loss: 0.3819921314716339, alpha=0.5480527877807617\n",
      "Iteration: 53,\t Loss: 0.3819122314453125, alpha=0.5485731363296509\n",
      "Iteration: 54,\t Loss: 0.38183215260505676, alpha=0.5490909814834595\n",
      "Iteration: 55,\t Loss: 0.3817518949508667, alpha=0.5495851039886475\n",
      "Iteration: 56,\t Loss: 0.38167160749435425, alpha=0.5500364899635315\n",
      "Iteration: 57,\t Loss: 0.38159120082855225, alpha=0.5504293441772461\n",
      "Iteration: 58,\t Loss: 0.38151082396507263, alpha=0.5507512092590332\n",
      "Iteration: 59,\t Loss: 0.3814304769039154, alpha=0.5509935617446899\n",
      "Iteration: 60,\t Loss: 0.38135015964508057, alpha=0.5511515736579895\n",
      "Iteration: 61,\t Loss: 0.38127002120018005, alpha=0.5512244701385498\n",
      "Iteration: 62,\t Loss: 0.3811900317668915, alpha=0.5512149333953857\n",
      "Iteration: 63,\t Loss: 0.38110995292663574, alpha=0.5511288642883301\n",
      "Iteration: 64,\t Loss: 0.38103023171424866, alpha=0.5509747862815857\n",
      "Iteration: 65,\t Loss: 0.3809504806995392, alpha=0.5507633686065674\n",
      "Iteration: 66,\t Loss: 0.3808709383010864, alpha=0.5505067110061646\n",
      "Iteration: 67,\t Loss: 0.38079166412353516, alpha=0.5502177476882935\n",
      "Iteration: 68,\t Loss: 0.3807125687599182, alpha=0.5499097108840942\n",
      "Iteration: 69,\t Loss: 0.3806335926055908, alpha=0.5495955348014832\n",
      "Iteration: 70,\t Loss: 0.38055485486984253, alpha=0.5492873787879944\n",
      "Iteration: 71,\t Loss: 0.38047629594802856, alpha=0.5489960312843323\n",
      "Iteration: 72,\t Loss: 0.3803979158401489, alpha=0.5487308502197266\n",
      "Iteration: 73,\t Loss: 0.3803198039531708, alpha=0.5484990477561951\n",
      "Iteration: 74,\t Loss: 0.38024187088012695, alpha=0.5483060479164124\n",
      "Iteration: 75,\t Loss: 0.3801639974117279, alpha=0.5481548309326172\n",
      "Iteration: 76,\t Loss: 0.38008642196655273, alpha=0.5480464100837708\n",
      "Iteration: 77,\t Loss: 0.3800090253353119, alpha=0.5479797720909119\n",
      "Iteration: 78,\t Loss: 0.37993180751800537, alpha=0.5479521155357361\n",
      "Iteration: 79,\t Loss: 0.3798547685146332, alpha=0.5479591488838196\n",
      "Iteration: 80,\t Loss: 0.37977784872055054, alpha=0.5479598045349121\n",
      "Iteration: 81,\t Loss: 0.37977638840675354, alpha=0.5479609966278076\n",
      "Iteration: 82,\t Loss: 0.379774808883667, alpha=0.5479626655578613\n",
      "Iteration: 83,\t Loss: 0.37977325916290283, alpha=0.5479648113250732\n",
      "Iteration: 84,\t Loss: 0.37977173924446106, alpha=0.5479673147201538\n",
      "Iteration: 85,\t Loss: 0.3797701597213745, alpha=0.5479702353477478\n",
      "Iteration: 86,\t Loss: 0.3797686696052551, alpha=0.5479734539985657\n",
      "Iteration: 87,\t Loss: 0.37976714968681335, alpha=0.5479769706726074\n",
      "Iteration: 88,\t Loss: 0.3797655701637268, alpha=0.5479807257652283\n",
      "Iteration: 89,\t Loss: 0.3797640800476074, alpha=0.5479847192764282\n",
      "Iteration: 90,\t Loss: 0.37976256012916565, alpha=0.5479889512062073\n",
      "Iteration: 91,\t Loss: 0.3797610104084015, alpha=0.5479933619499207\n",
      "Iteration: 92,\t Loss: 0.37975946068763733, alpha=0.5479979515075684\n",
      "Iteration: 93,\t Loss: 0.37975794076919556, alpha=0.5480027198791504\n",
      "Iteration: 94,\t Loss: 0.3797564208507538, alpha=0.548007607460022\n",
      "Iteration: 95,\t Loss: 0.37975484132766724, alpha=0.5480126142501831\n",
      "Iteration: 96,\t Loss: 0.37975332140922546, alpha=0.5480177402496338\n",
      "Iteration: 97,\t Loss: 0.37975171208381653, alpha=0.548022985458374\n",
      "Iteration: 98,\t Loss: 0.37975022196769714, alpha=0.548028290271759\n",
      "Iteration: 99,\t Loss: 0.37974873185157776, alpha=0.5480336546897888\n",
      "Iteration: 100,\t Loss: 0.3797471523284912, alpha=0.5480337142944336\n",
      "Iteration: 101,\t Loss: 0.37974709272384644, alpha=0.5480337738990784\n",
      "Iteration: 102,\t Loss: 0.3797471225261688, alpha=0.5480338335037231\n",
      "Iteration: 103,\t Loss: 0.37974703311920166, alpha=0.5480338931083679\n",
      "Iteration: 104,\t Loss: 0.3797470033168793, alpha=0.5480339527130127\n",
      "Iteration: 105,\t Loss: 0.3797469735145569, alpha=0.5480340123176575\n",
      "Iteration: 106,\t Loss: 0.37974703311920166, alpha=0.5480340719223022\n",
      "Iteration: 107,\t Loss: 0.3797470033168793, alpha=0.548034131526947\n",
      "Iteration: 108,\t Loss: 0.37974685430526733, alpha=0.5480341911315918\n",
      "Iteration: 109,\t Loss: 0.3797468841075897, alpha=0.5480342507362366\n",
      "\t Convergence reached after 110 iterations with alpha=0.5480343103408813\n",
      "Iteration: 0,\t Loss: 1441.1168212890625, alpha=1.9514068365097046\n",
      "Iteration: 1,\t Loss: 778.8413696289062, alpha=1.8672386407852173\n",
      "Iteration: 2,\t Loss: 225.50698852539062, alpha=1.7720450162887573\n",
      "Iteration: 3,\t Loss: 127.2360610961914, alpha=1.672255039215088\n",
      "Iteration: 4,\t Loss: 85.58773040771484, alpha=1.569836139678955\n",
      "Iteration: 5,\t Loss: 62.3338737487793, alpha=1.4656099081039429\n",
      "Iteration: 6,\t Loss: 47.39990234375, alpha=1.360058069229126\n",
      "Iteration: 7,\t Loss: 36.95796203613281, alpha=1.2535771131515503\n",
      "Iteration: 8,\t Loss: 29.227405548095703, alpha=1.1465890407562256\n",
      "Iteration: 9,\t Loss: 23.264442443847656, alpha=1.0396130084991455\n",
      "Iteration: 10,\t Loss: 18.520448684692383, alpha=0.933329701423645\n",
      "Iteration: 11,\t Loss: 14.65407943725586, alpha=0.8286470770835876\n",
      "Iteration: 12,\t Loss: 11.441353797912598, alpha=0.7267621755599976\n",
      "Iteration: 13,\t Loss: 8.729000091552734, alpha=0.6292012333869934\n",
      "Iteration: 14,\t Loss: 6.408351421356201, alpha=0.5378064513206482\n",
      "Iteration: 15,\t Loss: 4.400168418884277, alpha=0.45463547110557556\n",
      "Iteration: 16,\t Loss: 2.6451549530029297, alpha=0.3817562758922577\n",
      "Iteration: 17,\t Loss: 1.0980695486068726, alpha=0.3209705352783203\n",
      "Iteration: 18,\t Loss: 1.0160127878189087, alpha=0.2660501003265381\n",
      "Iteration: 19,\t Loss: 0.9857915043830872, alpha=0.216454416513443\n",
      "Iteration: 20,\t Loss: 0.9609886407852173, alpha=0.17171135544776917\n",
      "Iteration: 21,\t Loss: 0.9395279288291931, alpha=0.1313994824886322\n",
      "Iteration: 22,\t Loss: 0.9205656051635742, alpha=0.09513965994119644\n",
      "Iteration: 23,\t Loss: 0.9036070704460144, alpha=0.06258900463581085\n",
      "Iteration: 24,\t Loss: 0.8883122801780701, alpha=0.033436354249715805\n",
      "Iteration: 25,\t Loss: 0.8744271993637085, alpha=0.007398480549454689\n",
      "Iteration: 26,\t Loss: 0.8617521524429321, alpha=9.999999747378752e-05\n",
      "Iteration: 27,\t Loss: 0.8501164317131042, alpha=9.999999747378752e-05\n",
      "Iteration: 28,\t Loss: 0.8393933176994324, alpha=9.999999747378752e-05\n",
      "Iteration: 29,\t Loss: 0.8294766545295715, alpha=9.999999747378752e-05\n",
      "Iteration: 30,\t Loss: 0.8202722072601318, alpha=9.999999747378752e-05\n",
      "Iteration: 31,\t Loss: 0.8200989961624146, alpha=9.999999747378752e-05\n",
      "Iteration: 32,\t Loss: 0.8199326395988464, alpha=9.999999747378752e-05\n",
      "Iteration: 33,\t Loss: 0.8197727799415588, alpha=9.999999747378752e-05\n",
      "Iteration: 34,\t Loss: 0.8196188807487488, alpha=9.999999747378752e-05\n",
      "Iteration: 35,\t Loss: 0.8194698691368103, alpha=9.999999747378752e-05\n",
      "Iteration: 36,\t Loss: 0.8193264007568359, alpha=9.999999747378752e-05\n",
      "Iteration: 37,\t Loss: 0.8191871643066406, alpha=9.999999747378752e-05\n",
      "Iteration: 38,\t Loss: 0.81905198097229, alpha=9.999999747378752e-05\n",
      "Iteration: 39,\t Loss: 0.8189201951026917, alpha=9.999999747378752e-05\n",
      "Iteration: 40,\t Loss: 0.8187920451164246, alpha=9.999999747378752e-05\n",
      "Iteration: 41,\t Loss: 0.8186670541763306, alpha=9.999999747378752e-05\n",
      "Iteration: 42,\t Loss: 0.8185446262359619, alpha=9.999999747378752e-05\n",
      "Iteration: 43,\t Loss: 0.8184248805046082, alpha=9.999999747378752e-05\n",
      "Iteration: 44,\t Loss: 0.8183073401451111, alpha=0.00010564595140749589\n",
      "Iteration: 45,\t Loss: 0.8181921243667603, alpha=0.00012162090570200235\n",
      "Iteration: 46,\t Loss: 0.8180788159370422, alpha=0.00014709462993778288\n",
      "Iteration: 47,\t Loss: 0.8179672956466675, alpha=0.00018130891839973629\n",
      "Iteration: 48,\t Loss: 0.8178573846817017, alpha=0.0002235713618574664\n",
      "Iteration: 49,\t Loss: 0.8177486062049866, alpha=0.0002732496941462159\n",
      "Iteration: 50,\t Loss: 0.8176414370536804, alpha=0.00032976659713312984\n",
      "Iteration: 51,\t Loss: 0.8175353407859802, alpha=0.00039259501500055194\n",
      "Iteration: 52,\t Loss: 0.8174304366111755, alpha=0.0004612536868080497\n",
      "Iteration: 53,\t Loss: 0.8173264265060425, alpha=0.0005353032611310482\n",
      "Iteration: 54,\t Loss: 0.8172236680984497, alpha=0.0006143428036011755\n",
      "Iteration: 55,\t Loss: 0.8171214461326599, alpha=0.000698006188031286\n",
      "Iteration: 56,\t Loss: 0.8170202970504761, alpha=0.0007859593606553972\n",
      "Iteration: 57,\t Loss: 0.8169196248054504, alpha=0.0008778973133303225\n",
      "Iteration: 58,\t Loss: 0.8168197274208069, alpha=0.0009735416388139129\n",
      "Iteration: 59,\t Loss: 0.8167203068733215, alpha=0.00107263820245862\n",
      "Iteration: 60,\t Loss: 0.8166215419769287, alpha=0.0011749551631510258\n",
      "Iteration: 61,\t Loss: 0.8165228366851807, alpha=0.0012802808778360486\n",
      "Iteration: 62,\t Loss: 0.8164248466491699, alpha=0.001388421980664134\n",
      "Iteration: 63,\t Loss: 0.8163272738456726, alpha=0.0014992021024227142\n",
      "Iteration: 64,\t Loss: 0.8162301778793335, alpha=0.0016124603571370244\n",
      "Iteration: 65,\t Loss: 0.816133439540863, alpha=0.0017280494794249535\n",
      "Iteration: 66,\t Loss: 0.8160368800163269, alpha=0.001845835242420435\n",
      "Iteration: 67,\t Loss: 0.8159406185150146, alpha=0.001965695060789585\n",
      "Iteration: 68,\t Loss: 0.8158444166183472, alpha=0.0020875167101621628\n",
      "Iteration: 69,\t Loss: 0.8157488703727722, alpha=0.0022111977450549603\n",
      "Iteration: 70,\t Loss: 0.8156532049179077, alpha=0.0023366445675492287\n",
      "Iteration: 71,\t Loss: 0.8155580163002014, alpha=0.002463771728798747\n",
      "Iteration: 72,\t Loss: 0.8154628276824951, alpha=0.0025925009977072477\n",
      "Iteration: 73,\t Loss: 0.8153677582740784, alpha=0.0027227604296058416\n",
      "Iteration: 74,\t Loss: 0.815272867679596, alpha=0.0028544848319143057\n",
      "Iteration: 75,\t Loss: 0.8151783347129822, alpha=0.002987614134326577\n",
      "Iteration: 76,\t Loss: 0.8150838613510132, alpha=0.00312209315598011\n",
      "Iteration: 77,\t Loss: 0.8149895071983337, alpha=0.0032578716054558754\n",
      "Iteration: 78,\t Loss: 0.8148952126502991, alpha=0.0033949031494557858\n",
      "Iteration: 79,\t Loss: 0.8148009777069092, alpha=0.0035331454128026962\n",
      "Iteration: 80,\t Loss: 0.8147068023681641, alpha=0.0035359335597604513\n",
      "Iteration: 81,\t Loss: 0.8147050142288208, alpha=0.003538744756951928\n",
      "Iteration: 82,\t Loss: 0.8147030472755432, alpha=0.0035415785387158394\n",
      "Iteration: 83,\t Loss: 0.8147011995315552, alpha=0.003544434206560254\n",
      "Iteration: 84,\t Loss: 0.8146991729736328, alpha=0.0035473115276545286\n",
      "Iteration: 85,\t Loss: 0.814697265625, alpha=0.0035502100363373756\n",
      "Iteration: 86,\t Loss: 0.8146952986717224, alpha=0.003553129034116864\n",
      "Iteration: 87,\t Loss: 0.8146932721138, alpha=0.0035560685209929943\n",
      "Iteration: 88,\t Loss: 0.8146913647651672, alpha=0.003559027798473835\n",
      "Iteration: 89,\t Loss: 0.8146893382072449, alpha=0.0035620068665593863\n",
      "Iteration: 90,\t Loss: 0.8146874904632568, alpha=0.003565005259588361\n",
      "Iteration: 91,\t Loss: 0.8146857023239136, alpha=0.003568022744730115\n",
      "Iteration: 92,\t Loss: 0.8146837949752808, alpha=0.003571059089154005\n",
      "Iteration: 93,\t Loss: 0.8146818280220032, alpha=0.003574113827198744\n",
      "Iteration: 94,\t Loss: 0.8146798610687256, alpha=0.0035771869588643312\n",
      "Iteration: 95,\t Loss: 0.814677894115448, alpha=0.0035802782513201237\n",
      "Iteration: 96,\t Loss: 0.8146761059761047, alpha=0.0035833874717354774\n",
      "Iteration: 97,\t Loss: 0.8146741390228271, alpha=0.0035865146201103926\n",
      "Iteration: 98,\t Loss: 0.8146722316741943, alpha=0.0035896592307835817\n",
      "Iteration: 99,\t Loss: 0.8146702647209167, alpha=0.003592821303755045\n",
      "Iteration: 100,\t Loss: 0.8146682977676392, alpha=0.0035928848665207624\n",
      "Iteration: 101,\t Loss: 0.8146681785583496, alpha=0.0035929486621171236\n",
      "Iteration: 102,\t Loss: 0.8146679997444153, alpha=0.003593012923374772\n",
      "Iteration: 103,\t Loss: 0.8146678805351257, alpha=0.003593077417463064\n",
      "\t Convergence reached after 104 iterations with alpha=0.0035931423772126436\n",
      "Iteration: 0,\t Loss: 68.95345306396484, alpha=1.0494201183319092\n",
      "Iteration: 1,\t Loss: 45.42253112792969, alpha=0.9697571396827698\n",
      "Iteration: 2,\t Loss: 36.02393341064453, alpha=0.8868412375450134\n",
      "Iteration: 3,\t Loss: 28.520957946777344, alpha=0.7998783588409424\n",
      "Iteration: 4,\t Loss: 22.41098403930664, alpha=0.7089540958404541\n",
      "Iteration: 5,\t Loss: 17.361703872680664, alpha=0.6142707467079163\n",
      "Iteration: 6,\t Loss: 13.14260482788086, alpha=0.5178785920143127\n",
      "Iteration: 7,\t Loss: 9.590786933898926, alpha=0.43459218740463257\n",
      "Iteration: 8,\t Loss: 6.593222141265869, alpha=0.40493059158325195\n",
      "Iteration: 9,\t Loss: 4.07916784286499, alpha=0.42336586117744446\n",
      "Iteration: 10,\t Loss: 2.0281167030334473, alpha=0.4680147171020508\n",
      "Iteration: 11,\t Loss: 1.0401114225387573, alpha=0.5089315176010132\n",
      "Iteration: 12,\t Loss: 1.0216102600097656, alpha=0.5461306571960449\n",
      "Iteration: 13,\t Loss: 2.015732526779175, alpha=0.5740208625793457\n",
      "Iteration: 14,\t Loss: 9.571294784545898, alpha=0.5346915125846863\n",
      "Iteration: 15,\t Loss: 1.2098655700683594, alpha=0.5089808702468872\n",
      "Iteration: 16,\t Loss: 1.0201953649520874, alpha=0.4860493838787079\n",
      "Iteration: 17,\t Loss: 1.017922282218933, alpha=0.46574413776397705\n",
      "Iteration: 18,\t Loss: 1.0256398916244507, alpha=0.44795680046081543\n",
      "Iteration: 19,\t Loss: 1.038463830947876, alpha=0.43270576000213623\n",
      "Iteration: 20,\t Loss: 1.0595499277114868, alpha=0.42104583978652954\n",
      "Iteration: 21,\t Loss: 1.3708980083465576, alpha=0.4394857883453369\n",
      "Iteration: 22,\t Loss: 1.7702534198760986, alpha=0.4762306213378906\n",
      "Iteration: 23,\t Loss: 2.1025805473327637, alpha=0.5230175852775574\n",
      "Iteration: 24,\t Loss: 2.3620731830596924, alpha=0.5720034837722778\n",
      "Iteration: 25,\t Loss: 2.5473086833953857, alpha=0.6141838431358337\n",
      "Iteration: 26,\t Loss: 2.659515142440796, alpha=0.6406046748161316\n",
      "Iteration: 27,\t Loss: 2.7015438079833984, alpha=0.6465519070625305\n",
      "Iteration: 28,\t Loss: 2.677553653717041, alpha=0.6333754062652588\n",
      "Iteration: 29,\t Loss: 2.592991352081299, alpha=0.6061455011367798\n",
      "Iteration: 30,\t Loss: 2.4543323516845703, alpha=0.6054533123970032\n",
      "Iteration: 31,\t Loss: 2.450587272644043, alpha=0.6046291589736938\n",
      "Iteration: 32,\t Loss: 2.4459240436553955, alpha=0.6036896705627441\n",
      "Iteration: 33,\t Loss: 2.4404280185699463, alpha=0.602649986743927\n",
      "Iteration: 34,\t Loss: 2.434129476547241, alpha=0.6015238165855408\n",
      "Iteration: 35,\t Loss: 2.4271013736724854, alpha=0.6003236770629883\n",
      "Iteration: 36,\t Loss: 2.419416666030884, alpha=0.5990609526634216\n",
      "Iteration: 37,\t Loss: 2.411142349243164, alpha=0.5977460741996765\n",
      "Iteration: 38,\t Loss: 2.4022598266601562, alpha=0.5963885188102722\n",
      "Iteration: 39,\t Loss: 2.392866611480713, alpha=0.5949969291687012\n",
      "Iteration: 40,\t Loss: 2.382981300354004, alpha=0.5935793519020081\n",
      "Iteration: 41,\t Loss: 2.3726518154144287, alpha=0.5921429395675659\n",
      "Iteration: 42,\t Loss: 2.3618907928466797, alpha=0.5906944274902344\n",
      "Iteration: 43,\t Loss: 2.350740671157837, alpha=0.5892399549484253\n",
      "Iteration: 44,\t Loss: 2.3392319679260254, alpha=0.5877851247787476\n",
      "Iteration: 45,\t Loss: 2.327394723892212, alpha=0.5863351225852966\n",
      "Iteration: 46,\t Loss: 2.3152353763580322, alpha=0.584894597530365\n",
      "Iteration: 47,\t Loss: 2.3027658462524414, alpha=0.583467960357666\n",
      "Iteration: 48,\t Loss: 2.2900595664978027, alpha=0.5820590853691101\n",
      "Iteration: 49,\t Loss: 2.2770793437957764, alpha=0.5806715488433838\n",
      "Iteration: 50,\t Loss: 2.2638742923736572, alpha=0.5793087482452393\n",
      "Iteration: 51,\t Loss: 2.2504444122314453, alpha=0.5779735445976257\n",
      "Iteration: 52,\t Loss: 2.2368075847625732, alpha=0.5766686797142029\n",
      "Iteration: 53,\t Loss: 2.222964286804199, alpha=0.5753964781761169\n",
      "Iteration: 54,\t Loss: 2.208944320678711, alpha=0.5741591453552246\n",
      "Iteration: 55,\t Loss: 2.1947546005249023, alpha=0.5729585886001587\n",
      "Iteration: 56,\t Loss: 2.180412530899048, alpha=0.5717964172363281\n",
      "Iteration: 57,\t Loss: 2.165912628173828, alpha=0.5706740021705627\n",
      "Iteration: 58,\t Loss: 2.151272773742676, alpha=0.5695925354957581\n",
      "Iteration: 59,\t Loss: 2.1364989280700684, alpha=0.5685530304908752\n",
      "Iteration: 60,\t Loss: 2.1216039657592773, alpha=0.5675563216209412\n",
      "Iteration: 61,\t Loss: 2.1065869331359863, alpha=0.5666029453277588\n",
      "Iteration: 62,\t Loss: 2.0914487838745117, alpha=0.565693199634552\n",
      "Iteration: 63,\t Loss: 2.0762009620666504, alpha=0.5648274421691895\n",
      "Iteration: 64,\t Loss: 2.0608677864074707, alpha=0.5640056133270264\n",
      "Iteration: 65,\t Loss: 2.045431613922119, alpha=0.563227653503418\n",
      "Iteration: 66,\t Loss: 2.029916286468506, alpha=0.5624932646751404\n",
      "Iteration: 67,\t Loss: 2.014309883117676, alpha=0.5618020296096802\n",
      "Iteration: 68,\t Loss: 1.9986181259155273, alpha=0.5611534118652344\n",
      "Iteration: 69,\t Loss: 1.9828414916992188, alpha=0.5605466961860657\n",
      "Iteration: 70,\t Loss: 1.9669978618621826, alpha=0.559981107711792\n",
      "Iteration: 71,\t Loss: 1.9510993957519531, alpha=0.5594557523727417\n",
      "Iteration: 72,\t Loss: 1.9351038932800293, alpha=0.5589696168899536\n",
      "Iteration: 73,\t Loss: 1.919083833694458, alpha=0.5585216283798218\n",
      "Iteration: 74,\t Loss: 1.902984857559204, alpha=0.5581105351448059\n",
      "Iteration: 75,\t Loss: 1.886831283569336, alpha=0.5577350854873657\n",
      "Iteration: 76,\t Loss: 1.8706228733062744, alpha=0.5573940277099609\n",
      "Iteration: 77,\t Loss: 1.8543658256530762, alpha=0.5570859313011169\n",
      "Iteration: 78,\t Loss: 1.8380539417266846, alpha=0.5568093657493591\n",
      "Iteration: 79,\t Loss: 1.8217179775238037, alpha=0.5565629005432129\n",
      "Iteration: 80,\t Loss: 1.8053028583526611, alpha=0.5565585494041443\n",
      "Iteration: 81,\t Loss: 1.8049867153167725, alpha=0.5565546751022339\n",
      "Iteration: 82,\t Loss: 1.804652214050293, alpha=0.5565512776374817\n",
      "Iteration: 83,\t Loss: 1.8043239116668701, alpha=0.5565482974052429\n",
      "Iteration: 84,\t Loss: 1.803983449935913, alpha=0.5565457344055176\n",
      "Iteration: 85,\t Loss: 1.8036551475524902, alpha=0.5565435290336609\n",
      "Iteration: 86,\t Loss: 1.8033268451690674, alpha=0.5565416216850281\n",
      "Iteration: 87,\t Loss: 1.8029863834381104, alpha=0.5565400123596191\n",
      "Iteration: 88,\t Loss: 1.8026337623596191, alpha=0.5565387010574341\n",
      "Iteration: 89,\t Loss: 1.8023054599761963, alpha=0.5565376281738281\n",
      "Iteration: 90,\t Loss: 1.801952838897705, alpha=0.5565367937088013\n",
      "Iteration: 91,\t Loss: 1.801612377166748, alpha=0.5565361380577087\n",
      "Iteration: 92,\t Loss: 1.801271915435791, alpha=0.5565356612205505\n",
      "Iteration: 93,\t Loss: 1.8009192943572998, alpha=0.5565353631973267\n",
      "Iteration: 94,\t Loss: 1.8005788326263428, alpha=0.5565351843833923\n",
      "Iteration: 95,\t Loss: 1.800220251083374, alpha=0.5565351843833923\n",
      "Iteration: 96,\t Loss: 1.7998735904693604, alpha=0.5565353035926819\n",
      "Iteration: 97,\t Loss: 1.7995209693908691, alpha=0.556535542011261\n",
      "Iteration: 98,\t Loss: 1.7991561889648438, alpha=0.5565358996391296\n",
      "Iteration: 99,\t Loss: 1.7988097667694092, alpha=0.5565363168716431\n",
      "Iteration: 100,\t Loss: 1.7984509468078613, alpha=0.5565363168716431\n",
      "Iteration: 101,\t Loss: 1.7984449863433838, alpha=0.5565363168716431\n",
      "Iteration: 102,\t Loss: 1.7984387874603271, alpha=0.5565363168716431\n",
      "Iteration: 103,\t Loss: 1.798426628112793, alpha=0.5565363168716431\n",
      "Iteration: 104,\t Loss: 1.7984206676483154, alpha=0.5565363168716431\n",
      "Iteration: 105,\t Loss: 1.7984144687652588, alpha=0.5565363168716431\n",
      "Iteration: 106,\t Loss: 1.7984085083007812, alpha=0.5565363168716431\n",
      "Iteration: 107,\t Loss: 1.798396348953247, alpha=0.5565363168716431\n",
      "Iteration: 108,\t Loss: 1.798384189605713, alpha=0.5565363168716431\n",
      "Iteration: 109,\t Loss: 1.7983779907226562, alpha=0.5565363168716431\n",
      "Iteration: 110,\t Loss: 1.7983720302581787, alpha=0.5565363168716431\n",
      "Iteration: 111,\t Loss: 1.7983598709106445, alpha=0.5565363168716431\n",
      "Iteration: 112,\t Loss: 1.798353672027588, alpha=0.5565363168716431\n",
      "\t Convergence reached after 113 iterations with alpha=0.5565363168716431\n",
      "\u001b[32mOptimizing PBkl for concatenated classifier-------------------------------\u001b[0m\n",
      "Iteration: 0,\t Loss: 0.8702404499053955, alpha=0.9574426412582397\n",
      "Iteration: 1,\t Loss: 0.7632023096084595, alpha=0.8783732652664185\n",
      "Iteration: 2,\t Loss: 0.6827121376991272, alpha=0.7958279252052307\n",
      "Iteration: 3,\t Loss: 0.6181285977363586, alpha=0.709214448928833\n",
      "Iteration: 4,\t Loss: 0.5657018423080444, alpha=0.6183056235313416\n",
      "Iteration: 5,\t Loss: 0.5228738784790039, alpha=0.5236355066299438\n",
      "Iteration: 6,\t Loss: 0.48788172006607056, alpha=0.43705302476882935\n",
      "Iteration: 7,\t Loss: 0.45960867404937744, alpha=0.4125025272369385\n",
      "Iteration: 8,\t Loss: 0.43761536478996277, alpha=0.43778014183044434\n",
      "Iteration: 9,\t Loss: 0.4225322902202606, alpha=0.4870499074459076\n",
      "Iteration: 10,\t Loss: 0.4175022840499878, alpha=0.548241138458252\n",
      "Iteration: 11,\t Loss: 0.4327138662338257, alpha=0.5778036713600159\n",
      "Iteration: 12,\t Loss: 0.4664582908153534, alpha=0.5474096536636353\n",
      "Iteration: 13,\t Loss: 0.4383448362350464, alpha=0.5108555555343628\n",
      "Iteration: 14,\t Loss: 0.41390663385391235, alpha=0.4940532147884369\n",
      "Iteration: 15,\t Loss: 0.40233635902404785, alpha=0.49914515018463135\n",
      "Iteration: 16,\t Loss: 0.39739376306533813, alpha=0.5186242461204529\n",
      "Iteration: 17,\t Loss: 0.39550790190696716, alpha=0.5442363619804382\n",
      "Iteration: 18,\t Loss: 0.3949502408504486, alpha=0.5676449537277222\n",
      "Iteration: 19,\t Loss: 0.39484068751335144, alpha=0.5815776586532593\n",
      "Iteration: 20,\t Loss: 0.39470434188842773, alpha=0.58260178565979\n",
      "Iteration: 21,\t Loss: 0.39428216218948364, alpha=0.572160542011261\n",
      "Iteration: 22,\t Loss: 0.3934422433376312, alpha=0.555103600025177\n",
      "Iteration: 23,\t Loss: 0.3921331465244293, alpha=0.5380316972732544\n",
      "Iteration: 24,\t Loss: 0.39035409688949585, alpha=0.5275000929832458\n",
      "Iteration: 25,\t Loss: 0.38813653588294983, alpha=0.5274216532707214\n",
      "Iteration: 26,\t Loss: 0.3855346739292145, alpha=0.5373640060424805\n",
      "Iteration: 27,\t Loss: 0.38262104988098145, alpha=0.5528981685638428\n",
      "Iteration: 28,\t Loss: 0.3794814646244049, alpha=0.5668388605117798\n",
      "Iteration: 29,\t Loss: 0.3762074112892151, alpha=0.5719079375267029\n",
      "Iteration: 30,\t Loss: 0.37289097905158997, alpha=0.5717788338661194\n",
      "Iteration: 31,\t Loss: 0.37282344698905945, alpha=0.5714460015296936\n",
      "Iteration: 32,\t Loss: 0.3727528750896454, alpha=0.570934534072876\n",
      "Iteration: 33,\t Loss: 0.3726795017719269, alpha=0.5702683925628662\n",
      "Iteration: 34,\t Loss: 0.3726034462451935, alpha=0.5694701075553894\n",
      "Iteration: 35,\t Loss: 0.37252509593963623, alpha=0.5685608386993408\n",
      "Iteration: 36,\t Loss: 0.37244459986686707, alpha=0.5675603747367859\n",
      "Iteration: 37,\t Loss: 0.3723621666431427, alpha=0.5664872527122498\n",
      "Iteration: 38,\t Loss: 0.3722778856754303, alpha=0.5653586387634277\n",
      "Iteration: 39,\t Loss: 0.37219199538230896, alpha=0.5641907453536987\n",
      "Iteration: 40,\t Loss: 0.3721045255661011, alpha=0.5629985928535461\n",
      "Iteration: 41,\t Loss: 0.3720157742500305, alpha=0.5617963075637817\n",
      "Iteration: 42,\t Loss: 0.37192580103874207, alpha=0.5605968832969666\n",
      "Iteration: 43,\t Loss: 0.37183472514152527, alpha=0.5594124794006348\n",
      "Iteration: 44,\t Loss: 0.3717425465583801, alpha=0.5582542419433594\n",
      "Iteration: 45,\t Loss: 0.37164950370788574, alpha=0.5571324229240417\n",
      "Iteration: 46,\t Loss: 0.3715556263923645, alpha=0.5560561418533325\n",
      "Iteration: 47,\t Loss: 0.37146106362342834, alpha=0.5550336837768555\n",
      "Iteration: 48,\t Loss: 0.37136590480804443, alpha=0.5540721416473389\n",
      "Iteration: 49,\t Loss: 0.371270090341568, alpha=0.5531774759292603\n",
      "Iteration: 50,\t Loss: 0.3711737096309662, alpha=0.5523547530174255\n",
      "Iteration: 51,\t Loss: 0.37107691168785095, alpha=0.5516077280044556\n",
      "Iteration: 52,\t Loss: 0.3709796965122223, alpha=0.55093914270401\n",
      "Iteration: 53,\t Loss: 0.3708820939064026, alpha=0.5503506064414978\n",
      "Iteration: 54,\t Loss: 0.37078434228897095, alpha=0.5498426556587219\n",
      "Iteration: 55,\t Loss: 0.37068623304367065, alpha=0.5494147539138794\n",
      "Iteration: 56,\t Loss: 0.37058794498443604, alpha=0.5490654706954956\n",
      "Iteration: 57,\t Loss: 0.3704895079135895, alpha=0.5487924218177795\n",
      "Iteration: 58,\t Loss: 0.370390921831131, alpha=0.5485924482345581\n",
      "Iteration: 59,\t Loss: 0.37029221653938293, alpha=0.5484616756439209\n",
      "Iteration: 60,\t Loss: 0.3701934814453125, alpha=0.5483956336975098\n",
      "Iteration: 61,\t Loss: 0.37009477615356445, alpha=0.5483893156051636\n",
      "Iteration: 62,\t Loss: 0.3699960708618164, alpha=0.548437237739563\n",
      "Iteration: 63,\t Loss: 0.36989742517471313, alpha=0.5485336780548096\n",
      "Iteration: 64,\t Loss: 0.36979880928993225, alpha=0.5486726760864258\n",
      "Iteration: 65,\t Loss: 0.36970028281211853, alpha=0.548848032951355\n",
      "Iteration: 66,\t Loss: 0.36960187554359436, alpha=0.5490537285804749\n",
      "Iteration: 67,\t Loss: 0.36950361728668213, alpha=0.5492836236953735\n",
      "Iteration: 68,\t Loss: 0.3694056570529938, alpha=0.5495316982269287\n",
      "Iteration: 69,\t Loss: 0.3693076968193054, alpha=0.5497922897338867\n",
      "Iteration: 70,\t Loss: 0.36921006441116333, alpha=0.5500597357749939\n",
      "Iteration: 71,\t Loss: 0.3691127300262451, alpha=0.5503289103507996\n",
      "Iteration: 72,\t Loss: 0.36901551485061646, alpha=0.5505949258804321\n",
      "Iteration: 73,\t Loss: 0.36891862750053406, alpha=0.5508533120155334\n",
      "Iteration: 74,\t Loss: 0.36882197856903076, alpha=0.5511000752449036\n",
      "Iteration: 75,\t Loss: 0.3687257468700409, alpha=0.551331639289856\n",
      "Iteration: 76,\t Loss: 0.3686297535896301, alpha=0.5515450239181519\n",
      "Iteration: 77,\t Loss: 0.36853405833244324, alpha=0.5517376661300659\n",
      "Iteration: 78,\t Loss: 0.36843883991241455, alpha=0.5519075393676758\n",
      "Iteration: 79,\t Loss: 0.3683439791202545, alpha=0.5520531535148621\n",
      "Iteration: 80,\t Loss: 0.3682493567466736, alpha=0.5520555377006531\n",
      "Iteration: 81,\t Loss: 0.36824750900268555, alpha=0.5520574450492859\n",
      "Iteration: 82,\t Loss: 0.36824557185173035, alpha=0.5520589351654053\n",
      "Iteration: 83,\t Loss: 0.3682437837123871, alpha=0.552060067653656\n",
      "Iteration: 84,\t Loss: 0.3682417869567871, alpha=0.5520608425140381\n",
      "Iteration: 85,\t Loss: 0.36823999881744385, alpha=0.5520613193511963\n",
      "Iteration: 86,\t Loss: 0.36823806166648865, alpha=0.5520614981651306\n",
      "Iteration: 87,\t Loss: 0.36823612451553345, alpha=0.5520614385604858\n",
      "Iteration: 88,\t Loss: 0.3682342767715454, alpha=0.552061140537262\n",
      "Iteration: 89,\t Loss: 0.3682323098182678, alpha=0.552060604095459\n",
      "Iteration: 90,\t Loss: 0.3682304322719574, alpha=0.5520598888397217\n",
      "Iteration: 91,\t Loss: 0.3682285249233246, alpha=0.55205899477005\n",
      "Iteration: 92,\t Loss: 0.3682265877723694, alpha=0.5520579218864441\n",
      "Iteration: 93,\t Loss: 0.3682246506214142, alpha=0.5520566701889038\n",
      "Iteration: 94,\t Loss: 0.36822274327278137, alpha=0.552055299282074\n",
      "Iteration: 95,\t Loss: 0.36822086572647095, alpha=0.5520538091659546\n",
      "Iteration: 96,\t Loss: 0.36821889877319336, alpha=0.5520521998405457\n",
      "Iteration: 97,\t Loss: 0.3682169020175934, alpha=0.5520505309104919\n",
      "Iteration: 98,\t Loss: 0.36821499466896057, alpha=0.5520487427711487\n",
      "Iteration: 99,\t Loss: 0.36821305751800537, alpha=0.5520468950271606\n",
      "Iteration: 100,\t Loss: 0.36821115016937256, alpha=0.5520468354225159\n",
      "Iteration: 101,\t Loss: 0.3682110905647278, alpha=0.5520467758178711\n",
      "Iteration: 102,\t Loss: 0.3682110607624054, alpha=0.5520467162132263\n",
      "Iteration: 103,\t Loss: 0.368211030960083, alpha=0.5520466566085815\n",
      "Iteration: 104,\t Loss: 0.36821097135543823, alpha=0.5520465970039368\n",
      "Iteration: 105,\t Loss: 0.3682110011577606, alpha=0.552046537399292\n",
      "Iteration: 106,\t Loss: 0.3682108521461487, alpha=0.5520464777946472\n",
      "Iteration: 107,\t Loss: 0.36821091175079346, alpha=0.5520464181900024\n",
      "Iteration: 108,\t Loss: 0.3682107627391815, alpha=0.5520463585853577\n",
      "Iteration: 109,\t Loss: 0.3682107925415039, alpha=0.5520462989807129\n",
      "Iteration: 110,\t Loss: 0.36821073293685913, alpha=0.5520462393760681\n",
      "Iteration: 111,\t Loss: 0.36821067333221436, alpha=0.5520461797714233\n",
      "Iteration: 112,\t Loss: 0.36821073293685913, alpha=0.5520461201667786\n",
      "Iteration: 113,\t Loss: 0.36821067333221436, alpha=0.5520460605621338\n",
      "Iteration: 114,\t Loss: 0.3682105839252472, alpha=0.552046000957489\n",
      "Iteration: 115,\t Loss: 0.3682106137275696, alpha=0.5520459413528442\n",
      "Iteration: 116,\t Loss: 0.3682105839252472, alpha=0.5520458817481995\n",
      "Iteration: 117,\t Loss: 0.36821049451828003, alpha=0.5520458221435547\n",
      "Iteration: 118,\t Loss: 0.36821046471595764, alpha=0.5520457625389099\n",
      "Iteration: 119,\t Loss: 0.36821043491363525, alpha=0.5520456433296204\n",
      "Iteration: 120,\t Loss: 0.36821040511131287, alpha=0.5520455241203308\n",
      "Iteration: 121,\t Loss: 0.3682103455066681, alpha=0.5520454049110413\n",
      "Iteration: 122,\t Loss: 0.3682102560997009, alpha=0.5520452857017517\n",
      "Iteration: 123,\t Loss: 0.3682102859020233, alpha=0.5520451664924622\n",
      "Iteration: 124,\t Loss: 0.3682102560997009, alpha=0.5520450472831726\n",
      "Iteration: 125,\t Loss: 0.36821016669273376, alpha=0.5520449280738831\n",
      "Iteration: 126,\t Loss: 0.3682101368904114, alpha=0.5520448088645935\n",
      "\t Convergence reached after 127 iterations with alpha=0.552044689655304\n",
      "### Multiview classifier gibbs risk after Optim: 0.19980749009597812\n",
      "\u001b[32mOptimization is done! -------------------------------\u001b[0m\n",
      "\u001b[32mComputing the bound values ans risks -------------------------------\u001b[0m\n",
      "right_hand_side=0.0031081334961647186, empirical_gibbs_risk=0.19980749009597812\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[34m\n",
      "----------------Run 2---------------\u001b[0m\n",
      "Training multiview classifier-------------------------------\n",
      "Training separate views classifiers-------------------------------\n",
      "Training concatenated view classifier-------------------------------\n",
      "\u001b[32mComputing the bound values ans risks -------------------------------\u001b[0m\n",
      "right_hand_side=0.00320597436600025, empirical_gibbs_risk=0.2920935647756138\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "### Multiview classifier gibbs risk before Optim: 0.2920935647756138\n",
      "\u001b[32mOptimizing PBkl for multiview classifier-------------------------------\u001b[0m\n",
      "\t Convergence reached after 0 iterations with alpha=0.335252583026886\n",
      "\t Convergence reached after 1 iterations with alpha=0.40787702798843384\n",
      "\t Convergence reached after 2 iterations with alpha=0.48486778140068054\n",
      "\t Convergence reached after 3 iterations with alpha=0.5689424872398376\n",
      "\t Convergence reached after 4 iterations with alpha=0.52451491355896\n",
      "\t Convergence reached after 5 iterations with alpha=0.4745364487171173\n",
      "\t Convergence reached after 6 iterations with alpha=0.4663112163543701\n",
      "\t Convergence reached after 7 iterations with alpha=0.48922982811927795\n",
      "\t Convergence reached after 8 iterations with alpha=0.5250664353370667\n",
      "\t Convergence reached after 9 iterations with alpha=0.5541761517524719\n",
      "\t Convergence reached after 10 iterations with alpha=0.5574566125869751\n",
      "\t Convergence reached after 11 iterations with alpha=0.5391498804092407\n",
      "\t Convergence reached after 12 iterations with alpha=0.5161189436912537\n",
      "\t Convergence reached after 13 iterations with alpha=0.510741651058197\n",
      "\t Convergence reached after 14 iterations with alpha=0.5281551480293274\n",
      "\t Convergence reached after 15 iterations with alpha=0.554593563079834\n",
      "\t Convergence reached after 16 iterations with alpha=0.564889132976532\n",
      "\t Convergence reached after 17 iterations with alpha=0.5510409474372864\n",
      "\t Convergence reached after 18 iterations with alpha=0.5353888273239136\n",
      "\t Convergence reached after 19 iterations with alpha=0.5447647571563721\n",
      "\t Convergence reached after 20 iterations with alpha=0.5671891570091248\n",
      "\t Convergence reached after 21 iterations with alpha=0.5670194029808044\n",
      "\t Convergence reached after 22 iterations with alpha=0.5506542325019836\n",
      "\t Convergence reached after 23 iterations with alpha=0.5553359985351562\n",
      "\t Convergence reached after 24 iterations with alpha=0.5729404091835022\n",
      "\t Convergence reached after 25 iterations with alpha=0.5699436068534851\n",
      "\t Convergence reached after 26 iterations with alpha=0.5577812194824219\n",
      "\t Convergence reached after 27 iterations with alpha=0.5624421834945679\n",
      "\t Convergence reached after 28 iterations with alpha=0.5747115612030029\n",
      "\t Convergence reached after 29 iterations with alpha=0.5724457502365112\n",
      "\t Convergence reached after 30 iterations with alpha=0.572234570980072\n",
      "\t Convergence reached after 31 iterations with alpha=0.5718806982040405\n",
      "\t Convergence reached after 32 iterations with alpha=0.5714094042778015\n",
      "\t Convergence reached after 33 iterations with alpha=0.5708471536636353\n",
      "\t Convergence reached after 34 iterations with alpha=0.5702207684516907\n",
      "\t Convergence reached after 35 iterations with alpha=0.5695567727088928\n",
      "\t Convergence reached after 36 iterations with alpha=0.5688808560371399\n",
      "\t Convergence reached after 37 iterations with alpha=0.568217396736145\n",
      "\t Convergence reached after 38 iterations with alpha=0.5675886869430542\n",
      "\t Convergence reached after 39 iterations with alpha=0.5670143365859985\n",
      "\t Convergence reached after 40 iterations with alpha=0.5665106773376465\n",
      "\t Convergence reached after 41 iterations with alpha=0.5660901665687561\n",
      "\t Convergence reached after 42 iterations with alpha=0.5657612085342407\n",
      "\t Convergence reached after 43 iterations with alpha=0.5655278563499451\n",
      "\t Convergence reached after 44 iterations with alpha=0.5653899908065796\n",
      "\t Convergence reached after 45 iterations with alpha=0.5653435587882996\n",
      "\t Convergence reached after 46 iterations with alpha=0.5653811693191528\n",
      "\t Convergence reached after 47 iterations with alpha=0.565492570400238\n",
      "\t Convergence reached after 48 iterations with alpha=0.5656653046607971\n",
      "\t Convergence reached after 49 iterations with alpha=0.5658853650093079\n",
      "\t Convergence reached after 50 iterations with alpha=0.5661377906799316\n",
      "\t Convergence reached after 51 iterations with alpha=0.5664075016975403\n",
      "\t Convergence reached after 52 iterations with alpha=0.5666796565055847\n",
      "\t Convergence reached after 53 iterations with alpha=0.5669403076171875\n",
      "\t Convergence reached after 54 iterations with alpha=0.5671770572662354\n",
      "\t Convergence reached after 55 iterations with alpha=0.5673792362213135\n",
      "\t Convergence reached after 56 iterations with alpha=0.5675386190414429\n",
      "\t Convergence reached after 57 iterations with alpha=0.5676493048667908\n",
      "\t Convergence reached after 58 iterations with alpha=0.5677080154418945\n",
      "\t Convergence reached after 59 iterations with alpha=0.5677142143249512\n",
      "\t Convergence reached after 60 iterations with alpha=0.567669689655304\n",
      "\t Convergence reached after 61 iterations with alpha=0.5675784945487976\n",
      "\t Convergence reached after 62 iterations with alpha=0.5674465894699097\n",
      "\t Convergence reached after 63 iterations with alpha=0.5672812461853027\n",
      "\t Convergence reached after 64 iterations with alpha=0.5670908093452454\n",
      "\t Convergence reached after 65 iterations with alpha=0.5668841600418091\n",
      "\t Convergence reached after 66 iterations with alpha=0.5666703581809998\n",
      "\t Convergence reached after 67 iterations with alpha=0.5664579272270203\n",
      "\t Convergence reached after 68 iterations with alpha=0.5662547945976257\n",
      "\t Convergence reached after 69 iterations with alpha=0.5660678148269653\n",
      "\t Convergence reached after 70 iterations with alpha=0.5659024119377136\n",
      "\t Convergence reached after 71 iterations with alpha=0.5657625198364258\n",
      "\t Convergence reached after 72 iterations with alpha=0.565650463104248\n",
      "\t Convergence reached after 73 iterations with alpha=0.565566897392273\n",
      "\t Convergence reached after 74 iterations with alpha=0.5655109286308289\n",
      "\t Convergence reached after 75 iterations with alpha=0.5654802918434143\n",
      "\t Convergence reached after 76 iterations with alpha=0.5654715895652771\n",
      "\t Convergence reached after 77 iterations with alpha=0.5654804706573486\n",
      "\t Convergence reached after 78 iterations with alpha=0.5655019879341125\n",
      "\t Convergence reached after 79 iterations with alpha=0.5655308961868286\n",
      "\t Convergence reached after 80 iterations with alpha=0.5655314922332764\n",
      "\t Convergence reached after 81 iterations with alpha=0.5655321478843689\n",
      "\t Convergence reached after 82 iterations with alpha=0.5655328631401062\n",
      "\t Convergence reached after 83 iterations with alpha=0.5655335783958435\n",
      "\t Convergence reached after 84 iterations with alpha=0.5655343532562256\n",
      "\t Convergence reached after 85 iterations with alpha=0.5655351281166077\n",
      "\t Convergence reached after 86 iterations with alpha=0.5655359029769897\n",
      "\t Convergence reached after 87 iterations with alpha=0.5655367374420166\n",
      "\t Convergence reached after 88 iterations with alpha=0.5655375719070435\n",
      "\t Convergence reached after 89 iterations with alpha=0.5655384063720703\n",
      "\t Convergence reached after 90 iterations with alpha=0.5655392408370972\n",
      "\t Convergence reached after 91 iterations with alpha=0.565540075302124\n",
      "\t Convergence reached after 92 iterations with alpha=0.5655409097671509\n",
      "\t Convergence reached after 93 iterations with alpha=0.5655417442321777\n",
      "\t Convergence reached after 94 iterations with alpha=0.5655425786972046\n",
      "\t Convergence reached after 95 iterations with alpha=0.5655434131622314\n",
      "\t Convergence reached after 96 iterations with alpha=0.5655442476272583\n",
      "\t Convergence reached after 97 iterations with alpha=0.5655450820922852\n",
      "\t Convergence reached after 98 iterations with alpha=0.565545916557312\n",
      "\t Convergence reached after 99 iterations with alpha=0.5655467510223389\n",
      "\t Convergence reached after 100 iterations with alpha=0.5655467510223389\n",
      "\t Convergence reached after 101 iterations with alpha=0.5655467510223389\n",
      "\t Convergence reached after 102 iterations, alpha=0.5655467510223389\n",
      "\u001b[33mOptimization took 0:00:04.420596 -------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing PBkl for separate views classifiers-------------------------------\u001b[0m\n",
      "Iteration: 0,\t Loss: 25.4482364654541, alpha=0.803819477558136\n",
      "Iteration: 1,\t Loss: 13.529691696166992, alpha=0.7253832817077637\n",
      "Iteration: 2,\t Loss: 8.24738597869873, alpha=0.6426817774772644\n",
      "Iteration: 3,\t Loss: 3.8526878356933594, alpha=0.5544901490211487\n",
      "Iteration: 4,\t Loss: 1.028556227684021, alpha=0.47893548011779785\n",
      "Iteration: 5,\t Loss: 0.9614905118942261, alpha=0.41430139541625977\n",
      "Iteration: 6,\t Loss: 0.9163505434989929, alpha=0.3605484962463379\n",
      "Iteration: 7,\t Loss: 0.8826320171356201, alpha=0.3192613422870636\n",
      "Iteration: 8,\t Loss: 0.8573646545410156, alpha=0.293368935585022\n",
      "Iteration: 9,\t Loss: 0.8393984436988831, alpha=0.2863003611564636\n",
      "Iteration: 10,\t Loss: 0.8289448618888855, alpha=0.30029168725013733\n",
      "Iteration: 11,\t Loss: 0.8283102512359619, alpha=0.33488187193870544\n",
      "Iteration: 12,\t Loss: 0.8453447222709656, alpha=0.3871949315071106\n",
      "Iteration: 13,\t Loss: 0.9120303988456726, alpha=0.45349472761154175\n",
      "Iteration: 14,\t Loss: 12.696830749511719, alpha=0.5011764764785767\n",
      "Iteration: 15,\t Loss: 0.8521417379379272, alpha=0.5439849495887756\n",
      "Iteration: 16,\t Loss: 0.8206771016120911, alpha=0.5823038220405579\n",
      "Iteration: 17,\t Loss: 0.8173806667327881, alpha=0.6165271401405334\n",
      "Iteration: 18,\t Loss: 0.821666419506073, alpha=0.647018313407898\n",
      "Iteration: 19,\t Loss: 0.828708827495575, alpha=0.6741063594818115\n",
      "Iteration: 20,\t Loss: 0.836790919303894, alpha=0.6980880498886108\n",
      "Iteration: 21,\t Loss: 0.8451553583145142, alpha=0.7192310690879822\n",
      "Iteration: 22,\t Loss: 0.853425145149231, alpha=0.7377777099609375\n",
      "Iteration: 23,\t Loss: 0.861397922039032, alpha=0.7539475560188293\n",
      "Iteration: 24,\t Loss: 0.8689625263214111, alpha=0.7679401636123657\n",
      "Iteration: 25,\t Loss: 0.8760585188865662, alpha=0.7799375653266907\n",
      "Iteration: 26,\t Loss: 0.8826566934585571, alpha=0.7901058793067932\n",
      "Iteration: 27,\t Loss: 0.8887458443641663, alpha=0.7985972762107849\n",
      "Iteration: 28,\t Loss: 0.8943288922309875, alpha=0.8055513501167297\n",
      "Iteration: 29,\t Loss: 0.8994162082672119, alpha=0.8110964298248291\n",
      "Iteration: 30,\t Loss: 0.904023289680481, alpha=0.8111814856529236\n",
      "Iteration: 31,\t Loss: 0.9041053056716919, alpha=0.8112434148788452\n",
      "Iteration: 32,\t Loss: 0.9041782021522522, alpha=0.8112841844558716\n",
      "Iteration: 33,\t Loss: 0.9042421579360962, alpha=0.8113055229187012\n",
      "Iteration: 34,\t Loss: 0.9042984843254089, alpha=0.8113090991973877\n",
      "Iteration: 35,\t Loss: 0.9043474793434143, alpha=0.8112963438034058\n",
      "Iteration: 36,\t Loss: 0.9043899774551392, alpha=0.8112686276435852\n",
      "Iteration: 37,\t Loss: 0.9044262170791626, alpha=0.8112271428108215\n",
      "Iteration: 38,\t Loss: 0.9044572710990906, alpha=0.8111730813980103\n",
      "Iteration: 39,\t Loss: 0.9044829607009888, alpha=0.8111074566841125\n",
      "Iteration: 40,\t Loss: 0.9045042991638184, alpha=0.8110311627388\n",
      "Iteration: 41,\t Loss: 0.9045211672782898, alpha=0.8109450936317444\n",
      "Iteration: 42,\t Loss: 0.9045345187187195, alpha=0.8108500242233276\n",
      "Iteration: 43,\t Loss: 0.9045441746711731, alpha=0.8107466697692871\n",
      "Iteration: 44,\t Loss: 0.9045506715774536, alpha=0.8106357455253601\n",
      "Iteration: 45,\t Loss: 0.9045544266700745, alpha=0.8105177879333496\n",
      "Iteration: 46,\t Loss: 0.9045554399490356, alpha=0.8103933334350586\n",
      "Iteration: 47,\t Loss: 0.9045538902282715, alpha=0.81026291847229\n",
      "Iteration: 48,\t Loss: 0.904550313949585, alpha=0.8101269602775574\n",
      "Iteration: 49,\t Loss: 0.9045447707176208, alpha=0.8099859952926636\n",
      "Iteration: 50,\t Loss: 0.9045371413230896, alpha=0.8098402619361877\n",
      "Iteration: 51,\t Loss: 0.9045277237892151, alpha=0.8096901774406433\n",
      "Iteration: 52,\t Loss: 0.9045171737670898, alpha=0.8095360398292542\n",
      "Iteration: 53,\t Loss: 0.9045050144195557, alpha=0.8093781471252441\n",
      "Iteration: 54,\t Loss: 0.9044914245605469, alpha=0.8092167973518372\n",
      "Iteration: 55,\t Loss: 0.9044766426086426, alpha=0.8090521693229675\n",
      "Iteration: 56,\t Loss: 0.9044608473777771, alpha=0.8088845014572144\n",
      "Iteration: 57,\t Loss: 0.90444415807724, alpha=0.8087140321731567\n",
      "Iteration: 58,\t Loss: 0.9044263958930969, alpha=0.808540940284729\n",
      "Iteration: 59,\t Loss: 0.9044078588485718, alpha=0.8083653450012207\n",
      "Iteration: 60,\t Loss: 0.9043887853622437, alpha=0.8081874847412109\n",
      "Iteration: 61,\t Loss: 0.9043687582015991, alpha=0.8080074191093445\n",
      "Iteration: 62,\t Loss: 0.9043482542037964, alpha=0.8078253269195557\n",
      "Iteration: 63,\t Loss: 0.9043269157409668, alpha=0.8076412677764893\n",
      "Iteration: 64,\t Loss: 0.9043052792549133, alpha=0.8074554204940796\n",
      "Iteration: 65,\t Loss: 0.9042830467224121, alpha=0.8072678446769714\n",
      "Iteration: 66,\t Loss: 0.9042603969573975, alpha=0.8070786595344543\n",
      "Iteration: 67,\t Loss: 0.9042372107505798, alpha=0.8068879246711731\n",
      "Iteration: 68,\t Loss: 0.9042136669158936, alpha=0.8066957592964172\n",
      "Iteration: 69,\t Loss: 0.9041900038719177, alpha=0.8065021634101868\n",
      "Iteration: 70,\t Loss: 0.9041659832000732, alpha=0.8063072562217712\n",
      "Iteration: 71,\t Loss: 0.9041413068771362, alpha=0.8061110377311707\n",
      "Iteration: 72,\t Loss: 0.9041166305541992, alpha=0.8059136271476746\n",
      "Iteration: 73,\t Loss: 0.904091477394104, alpha=0.8057150840759277\n",
      "Iteration: 74,\t Loss: 0.9040662050247192, alpha=0.8055153489112854\n",
      "Iteration: 75,\t Loss: 0.9040406942367554, alpha=0.8053145408630371\n",
      "Iteration: 76,\t Loss: 0.9040149450302124, alpha=0.8051127195358276\n",
      "Iteration: 77,\t Loss: 0.9039891362190247, alpha=0.804909884929657\n",
      "Iteration: 78,\t Loss: 0.903963029384613, alpha=0.8047060370445251\n",
      "Iteration: 79,\t Loss: 0.9039366245269775, alpha=0.8045012950897217\n",
      "Iteration: 80,\t Loss: 0.9039102792739868, alpha=0.8044971823692322\n",
      "Iteration: 81,\t Loss: 0.9039095640182495, alpha=0.8044930696487427\n",
      "Iteration: 82,\t Loss: 0.9039093255996704, alpha=0.8044888973236084\n",
      "Iteration: 83,\t Loss: 0.9039086699485779, alpha=0.8044847249984741\n",
      "Iteration: 84,\t Loss: 0.9039081335067749, alpha=0.8044805526733398\n",
      "Iteration: 85,\t Loss: 0.9039075970649719, alpha=0.8044763207435608\n",
      "Iteration: 86,\t Loss: 0.9039069414138794, alpha=0.8044720888137817\n",
      "Iteration: 87,\t Loss: 0.903906524181366, alpha=0.8044678568840027\n",
      "Iteration: 88,\t Loss: 0.903905987739563, alpha=0.8044636249542236\n",
      "Iteration: 89,\t Loss: 0.9039053320884705, alpha=0.8044593334197998\n",
      "Iteration: 90,\t Loss: 0.9039048552513123, alpha=0.804455041885376\n",
      "Iteration: 91,\t Loss: 0.9039043188095093, alpha=0.8044507503509521\n",
      "Iteration: 92,\t Loss: 0.9039037823677063, alpha=0.8044464588165283\n",
      "Iteration: 93,\t Loss: 0.903903067111969, alpha=0.8044421076774597\n",
      "Iteration: 94,\t Loss: 0.9039027094841003, alpha=0.8044377565383911\n",
      "Iteration: 95,\t Loss: 0.9039020538330078, alpha=0.8044334053993225\n",
      "Iteration: 96,\t Loss: 0.9039013981819153, alpha=0.8044289946556091\n",
      "Iteration: 97,\t Loss: 0.9039007425308228, alpha=0.8044245839118958\n",
      "Iteration: 98,\t Loss: 0.9039003849029541, alpha=0.8044201731681824\n",
      "Iteration: 99,\t Loss: 0.903899610042572, alpha=0.804415762424469\n",
      "Iteration: 100,\t Loss: 0.9038992524147034, alpha=0.8044156432151794\n",
      "Iteration: 101,\t Loss: 0.9038990139961243, alpha=0.8044155240058899\n",
      "Iteration: 102,\t Loss: 0.9038991928100586, alpha=0.8044154047966003\n",
      "Iteration: 103,\t Loss: 0.903899073600769, alpha=0.8044152855873108\n",
      "Iteration: 104,\t Loss: 0.9038991928100586, alpha=0.8044151663780212\n",
      "Iteration: 105,\t Loss: 0.9038991332054138, alpha=0.8044150471687317\n",
      "Iteration: 106,\t Loss: 0.903899073600769, alpha=0.8044149279594421\n",
      "\t Convergence reached after 107 iterations with alpha=0.8044148087501526\n",
      "Iteration: 0,\t Loss: 20.58009147644043, alpha=1.1236534118652344\n",
      "Iteration: 1,\t Loss: 10.189026832580566, alpha=1.043623447418213\n",
      "Iteration: 2,\t Loss: 2.6011016368865967, alpha=0.9604676961898804\n",
      "Iteration: 3,\t Loss: 0.9435545802116394, alpha=0.8896299600601196\n",
      "Iteration: 4,\t Loss: 0.856210470199585, alpha=0.8266832828521729\n",
      "Iteration: 5,\t Loss: 0.7935867309570312, alpha=0.7691338062286377\n",
      "Iteration: 6,\t Loss: 0.7458819150924683, alpha=0.7155346274375916\n",
      "Iteration: 7,\t Loss: 0.7082531452178955, alpha=0.6650928258895874\n",
      "Iteration: 8,\t Loss: 0.6778470277786255, alpha=0.6174717545509338\n",
      "Iteration: 9,\t Loss: 0.6528338193893433, alpha=0.572679340839386\n",
      "Iteration: 10,\t Loss: 0.631973147392273, alpha=0.5309956073760986\n",
      "Iteration: 11,\t Loss: 0.614388108253479, alpha=0.49291548132896423\n",
      "Iteration: 12,\t Loss: 0.5994372367858887, alpha=0.4590918719768524\n",
      "Iteration: 13,\t Loss: 0.5866401791572571, alpha=0.43026918172836304\n",
      "Iteration: 14,\t Loss: 0.5756285786628723, alpha=0.40720227360725403\n",
      "Iteration: 15,\t Loss: 0.5661135911941528, alpha=0.3905622363090515\n",
      "Iteration: 16,\t Loss: 0.5578665733337402, alpha=0.3808399736881256\n",
      "Iteration: 17,\t Loss: 0.5507019758224487, alpha=0.3782622516155243\n",
      "Iteration: 18,\t Loss: 0.5444689989089966, alpha=0.38273537158966064\n",
      "Iteration: 19,\t Loss: 0.5390430092811584, alpha=0.3938221335411072\n",
      "Iteration: 20,\t Loss: 0.5343201756477356, alpha=0.41074731945991516\n",
      "Iteration: 21,\t Loss: 0.5302135944366455, alpha=0.4324191212654114\n",
      "Iteration: 22,\t Loss: 0.5266492962837219, alpha=0.4574558734893799\n",
      "Iteration: 23,\t Loss: 0.523564875125885, alpha=0.48421603441238403\n",
      "Iteration: 24,\t Loss: 0.5209056735038757, alpha=0.5108439922332764\n",
      "Iteration: 25,\t Loss: 0.5186246633529663, alpha=0.5353586673736572\n",
      "Iteration: 26,\t Loss: 0.5166796445846558, alpha=0.5558110475540161\n",
      "Iteration: 27,\t Loss: 0.5150327086448669, alpha=0.5705150365829468\n",
      "Iteration: 28,\t Loss: 0.5136498212814331, alpha=0.5783085823059082\n",
      "Iteration: 29,\t Loss: 0.5124998688697815, alpha=0.5787597298622131\n",
      "Iteration: 30,\t Loss: 0.5115553736686707, alpha=0.5786295533180237\n",
      "Iteration: 31,\t Loss: 0.5115390419960022, alpha=0.5783728361129761\n",
      "Iteration: 32,\t Loss: 0.5115239024162292, alpha=0.5780013799667358\n",
      "Iteration: 33,\t Loss: 0.5115099549293518, alpha=0.5775260925292969\n",
      "Iteration: 34,\t Loss: 0.5114969611167908, alpha=0.5769570469856262\n",
      "Iteration: 35,\t Loss: 0.5114847421646118, alpha=0.5763035416603088\n",
      "Iteration: 36,\t Loss: 0.5114733576774597, alpha=0.5755742788314819\n",
      "Iteration: 37,\t Loss: 0.5114626884460449, alpha=0.5747771859169006\n",
      "Iteration: 38,\t Loss: 0.5114524960517883, alpha=0.5739197134971619\n",
      "Iteration: 39,\t Loss: 0.511443018913269, alpha=0.57300865650177\n",
      "Iteration: 40,\t Loss: 0.511434018611908, alpha=0.5720503330230713\n",
      "Iteration: 41,\t Loss: 0.5114254951477051, alpha=0.5710506439208984\n",
      "Iteration: 42,\t Loss: 0.5114173889160156, alpha=0.570015013217926\n",
      "Iteration: 43,\t Loss: 0.5114094614982605, alpha=0.5689484477043152\n",
      "Iteration: 44,\t Loss: 0.5114021301269531, alpha=0.5678554773330688\n",
      "Iteration: 45,\t Loss: 0.5113949179649353, alpha=0.5667404532432556\n",
      "Iteration: 46,\t Loss: 0.5113879442214966, alpha=0.5656073689460754\n",
      "Iteration: 47,\t Loss: 0.5113813281059265, alpha=0.5644598007202148\n",
      "Iteration: 48,\t Loss: 0.5113746523857117, alpha=0.5633012056350708\n",
      "Iteration: 49,\t Loss: 0.5113683938980103, alpha=0.5621346831321716\n",
      "Iteration: 50,\t Loss: 0.5113621354103088, alpha=0.5609630942344666\n",
      "Iteration: 51,\t Loss: 0.511355996131897, alpha=0.5597890615463257\n",
      "Iteration: 52,\t Loss: 0.5113500356674194, alpha=0.5586150884628296\n",
      "Iteration: 53,\t Loss: 0.5113440752029419, alpha=0.557443380355835\n",
      "Iteration: 54,\t Loss: 0.5113382935523987, alpha=0.5562760233879089\n",
      "Iteration: 55,\t Loss: 0.5113324522972107, alpha=0.5551149845123291\n",
      "Iteration: 56,\t Loss: 0.5113266110420227, alpha=0.5539619326591492\n",
      "Iteration: 57,\t Loss: 0.511320948600769, alpha=0.5528184771537781\n",
      "Iteration: 58,\t Loss: 0.5113152861595154, alpha=0.5516861081123352\n",
      "Iteration: 59,\t Loss: 0.5113096833229065, alpha=0.5505661368370056\n",
      "Iteration: 60,\t Loss: 0.5113038420677185, alpha=0.5494598746299744\n",
      "Iteration: 61,\t Loss: 0.5112981796264648, alpha=0.5483683347702026\n",
      "Iteration: 62,\t Loss: 0.5112925171852112, alpha=0.5472925901412964\n",
      "Iteration: 63,\t Loss: 0.5112867951393127, alpha=0.5462335348129272\n",
      "Iteration: 64,\t Loss: 0.5112810134887695, alpha=0.5451918840408325\n",
      "Iteration: 65,\t Loss: 0.5112752318382263, alpha=0.5441684126853943\n",
      "Iteration: 66,\t Loss: 0.5112693905830383, alpha=0.5431638360023499\n",
      "Iteration: 67,\t Loss: 0.5112634897232056, alpha=0.5421786904335022\n",
      "Iteration: 68,\t Loss: 0.5112577080726624, alpha=0.5412133932113647\n",
      "Iteration: 69,\t Loss: 0.51125168800354, alpha=0.5402684211730957\n",
      "Iteration: 70,\t Loss: 0.5112457275390625, alpha=0.5393441319465637\n",
      "Iteration: 71,\t Loss: 0.5112396478652954, alpha=0.5384408831596375\n",
      "Iteration: 72,\t Loss: 0.5112335085868835, alpha=0.5375588536262512\n",
      "Iteration: 73,\t Loss: 0.5112273693084717, alpha=0.5366982817649841\n",
      "Iteration: 74,\t Loss: 0.5112211108207703, alpha=0.5358592867851257\n",
      "Iteration: 75,\t Loss: 0.5112148523330688, alpha=0.5350419878959656\n",
      "Iteration: 76,\t Loss: 0.5112085342407227, alpha=0.5342463850975037\n",
      "Iteration: 77,\t Loss: 0.5112022161483765, alpha=0.5334725975990295\n",
      "Iteration: 78,\t Loss: 0.5111954808235168, alpha=0.5327205061912537\n",
      "Iteration: 79,\t Loss: 0.5111891031265259, alpha=0.5319900512695312\n",
      "Iteration: 80,\t Loss: 0.5111823678016663, alpha=0.5319758653640747\n",
      "\t Convergence reached after 81 iterations with alpha=0.5319620370864868\n",
      "Iteration: 0,\t Loss: 13.83090877532959, alpha=1.0708845853805542\n",
      "Iteration: 1,\t Loss: 11.632712364196777, alpha=0.9911311268806458\n",
      "Iteration: 2,\t Loss: 4.748734474182129, alpha=0.9082958102226257\n",
      "Iteration: 3,\t Loss: 1.0043587684631348, alpha=0.8374791145324707\n",
      "Iteration: 4,\t Loss: 0.9187871813774109, alpha=0.7746202349662781\n",
      "Iteration: 5,\t Loss: 0.8597730994224548, alpha=0.7173656225204468\n",
      "Iteration: 6,\t Loss: 0.8150765895843506, alpha=0.6644113063812256\n",
      "Iteration: 7,\t Loss: 0.7798663973808289, alpha=0.615119218826294\n",
      "Iteration: 8,\t Loss: 0.7514252066612244, alpha=0.5693175196647644\n",
      "Iteration: 9,\t Loss: 0.7280408143997192, alpha=0.5271828174591064\n",
      "Iteration: 10,\t Loss: 0.7085637450218201, alpha=0.4891591966152191\n",
      "Iteration: 11,\t Loss: 0.6921863555908203, alpha=0.45588618516921997\n",
      "Iteration: 12,\t Loss: 0.678321123123169, alpha=0.42812132835388184\n",
      "Iteration: 13,\t Loss: 0.666529655456543, alpha=0.4066476821899414\n",
      "Iteration: 14,\t Loss: 0.6564767956733704, alpha=0.39216628670692444\n",
      "Iteration: 15,\t Loss: 0.6479014158248901, alpha=0.38518333435058594\n",
      "Iteration: 16,\t Loss: 0.6405979990959167, alpha=0.3859114348888397\n",
      "Iteration: 17,\t Loss: 0.6344007253646851, alpha=0.3942003548145294\n",
      "Iteration: 18,\t Loss: 0.6291766166687012, alpha=0.4095050096511841\n",
      "Iteration: 19,\t Loss: 0.6248162388801575, alpha=0.43088144063949585\n",
      "Iteration: 20,\t Loss: 0.6212303638458252, alpha=0.4569924771785736\n",
      "Iteration: 21,\t Loss: 0.6183440685272217, alpha=0.4861067831516266\n",
      "Iteration: 22,\t Loss: 0.6160942316055298, alpha=0.5160953998565674\n",
      "Iteration: 23,\t Loss: 0.6144269704818726, alpha=0.5444629788398743\n",
      "Iteration: 24,\t Loss: 0.6132936477661133, alpha=0.5684952139854431\n",
      "Iteration: 25,\t Loss: 0.6126492023468018, alpha=0.585608184337616\n",
      "Iteration: 26,\t Loss: 0.612449586391449, alpha=0.5938807725906372\n",
      "Iteration: 27,\t Loss: 0.6126505136489868, alpha=0.5925641059875488\n",
      "Iteration: 28,\t Loss: 0.6132079362869263, alpha=0.5823127627372742\n",
      "Iteration: 29,\t Loss: 0.6140749454498291, alpha=0.5650842785835266\n",
      "Iteration: 30,\t Loss: 0.6152026653289795, alpha=0.564659833908081\n",
      "Iteration: 31,\t Loss: 0.6152262091636658, alpha=0.5641618371009827\n",
      "Iteration: 32,\t Loss: 0.6152470111846924, alpha=0.5635977387428284\n",
      "Iteration: 33,\t Loss: 0.6152658462524414, alpha=0.5629746317863464\n",
      "Iteration: 34,\t Loss: 0.615282416343689, alpha=0.5622991323471069\n",
      "Iteration: 35,\t Loss: 0.615297257900238, alpha=0.5615774393081665\n",
      "Iteration: 36,\t Loss: 0.6153101325035095, alpha=0.5608154535293579\n",
      "Iteration: 37,\t Loss: 0.6153213381767273, alpha=0.5600185394287109\n",
      "Iteration: 38,\t Loss: 0.6153310537338257, alpha=0.5591918230056763\n",
      "Iteration: 39,\t Loss: 0.6153395175933838, alpha=0.5583400726318359\n",
      "Iteration: 40,\t Loss: 0.6153465509414673, alpha=0.5574677586555481\n",
      "Iteration: 41,\t Loss: 0.6153524518013, alpha=0.5565791130065918\n",
      "Iteration: 42,\t Loss: 0.615356981754303, alpha=0.5556779503822327\n",
      "Iteration: 43,\t Loss: 0.6153607964515686, alpha=0.5547679662704468\n",
      "Iteration: 44,\t Loss: 0.6153637170791626, alpha=0.5538524985313416\n",
      "Iteration: 45,\t Loss: 0.6153652667999268, alpha=0.5529347062110901\n",
      "Iteration: 46,\t Loss: 0.615366518497467, alpha=0.5520175099372864\n",
      "Iteration: 47,\t Loss: 0.6153666973114014, alpha=0.5511036515235901\n",
      "Iteration: 48,\t Loss: 0.6153662800788879, alpha=0.550195574760437\n",
      "Iteration: 49,\t Loss: 0.615365207195282, alpha=0.5492956638336182\n",
      "Iteration: 50,\t Loss: 0.6153635382652283, alpha=0.5484059453010559\n",
      "Iteration: 51,\t Loss: 0.6153615117073059, alpha=0.5475284457206726\n",
      "Iteration: 52,\t Loss: 0.6153589487075806, alpha=0.5466648936271667\n",
      "Iteration: 53,\t Loss: 0.6153557896614075, alpha=0.545816957950592\n",
      "Iteration: 54,\t Loss: 0.6153521537780762, alpha=0.5449860095977783\n",
      "Iteration: 55,\t Loss: 0.6153480410575867, alpha=0.5441734790802002\n",
      "Iteration: 56,\t Loss: 0.6153437495231628, alpha=0.5433803796768188\n",
      "Iteration: 57,\t Loss: 0.615338921546936, alpha=0.5426077842712402\n",
      "Iteration: 58,\t Loss: 0.6153337955474854, alpha=0.5418566465377808\n",
      "Iteration: 59,\t Loss: 0.6153284311294556, alpha=0.5411276817321777\n",
      "Iteration: 60,\t Loss: 0.6153227090835571, alpha=0.5404215455055237\n",
      "Iteration: 61,\t Loss: 0.6153168082237244, alpha=0.5397388339042664\n",
      "Iteration: 62,\t Loss: 0.615310788154602, alpha=0.5390799045562744\n",
      "Iteration: 63,\t Loss: 0.6153044104576111, alpha=0.5384451746940613\n",
      "Iteration: 64,\t Loss: 0.6152975559234619, alpha=0.5378348231315613\n",
      "Iteration: 65,\t Loss: 0.6152907609939575, alpha=0.5372490882873535\n",
      "Iteration: 66,\t Loss: 0.6152836084365845, alpha=0.5366879105567932\n",
      "Iteration: 67,\t Loss: 0.6152764558792114, alpha=0.5361512899398804\n",
      "Iteration: 68,\t Loss: 0.6152691841125488, alpha=0.5356391668319702\n",
      "Iteration: 69,\t Loss: 0.615261435508728, alpha=0.5351514220237732\n",
      "Iteration: 70,\t Loss: 0.6152536273002625, alpha=0.5346878170967102\n",
      "Iteration: 71,\t Loss: 0.6152455806732178, alpha=0.5342480540275574\n",
      "Iteration: 72,\t Loss: 0.6152377128601074, alpha=0.533831775188446\n",
      "Iteration: 73,\t Loss: 0.6152294278144836, alpha=0.533438503742218\n",
      "Iteration: 74,\t Loss: 0.6152210235595703, alpha=0.5330678820610046\n",
      "Iteration: 75,\t Loss: 0.6152125597000122, alpha=0.5327194333076477\n",
      "Iteration: 76,\t Loss: 0.6152040362358093, alpha=0.5323925614356995\n",
      "Iteration: 77,\t Loss: 0.6151952743530273, alpha=0.5320867300033569\n",
      "Iteration: 78,\t Loss: 0.6151865124702454, alpha=0.5318013429641724\n",
      "Iteration: 79,\t Loss: 0.6151774525642395, alpha=0.5315358638763428\n",
      "Iteration: 80,\t Loss: 0.6151683330535889, alpha=0.5315309166908264\n",
      "Iteration: 81,\t Loss: 0.6151682138442993, alpha=0.5315263271331787\n",
      "Iteration: 82,\t Loss: 0.6151680946350098, alpha=0.5315220355987549\n",
      "Iteration: 83,\t Loss: 0.6151679158210754, alpha=0.5315180420875549\n",
      "Iteration: 84,\t Loss: 0.6151677370071411, alpha=0.5315142869949341\n",
      "Iteration: 85,\t Loss: 0.6151675581932068, alpha=0.5315107703208923\n",
      "Iteration: 86,\t Loss: 0.6151673793792725, alpha=0.5315074920654297\n",
      "Iteration: 87,\t Loss: 0.6151671409606934, alpha=0.5315043926239014\n",
      "Iteration: 88,\t Loss: 0.6151669025421143, alpha=0.5315014719963074\n",
      "Iteration: 89,\t Loss: 0.6151667237281799, alpha=0.5314987301826477\n",
      "Iteration: 90,\t Loss: 0.6151666641235352, alpha=0.5314961075782776\n",
      "Iteration: 91,\t Loss: 0.6151663661003113, alpha=0.5314936637878418\n",
      "Iteration: 92,\t Loss: 0.6151660680770874, alpha=0.5314913392066956\n",
      "Iteration: 93,\t Loss: 0.6151657700538635, alpha=0.5314890742301941\n",
      "Iteration: 94,\t Loss: 0.6151657104492188, alpha=0.5314869284629822\n",
      "Iteration: 95,\t Loss: 0.6151654124259949, alpha=0.531484842300415\n",
      "Iteration: 96,\t Loss: 0.6151652932167053, alpha=0.5314828753471375\n",
      "Iteration: 97,\t Loss: 0.6151649355888367, alpha=0.5314809679985046\n",
      "\t Convergence reached after 98 iterations with alpha=0.5314791202545166\n",
      "Iteration: 0,\t Loss: 0.9118608832359314, alpha=0.851980447769165\n",
      "Iteration: 1,\t Loss: 0.8077266812324524, alpha=0.7734718322753906\n",
      "Iteration: 2,\t Loss: 0.7278372645378113, alpha=0.6910974979400635\n",
      "Iteration: 3,\t Loss: 0.6627852320671082, alpha=0.6037951111793518\n",
      "Iteration: 4,\t Loss: 0.6093065142631531, alpha=0.5114072561264038\n",
      "Iteration: 5,\t Loss: 0.5652677416801453, alpha=0.42929521203041077\n",
      "Iteration: 6,\t Loss: 0.5293728709220886, alpha=0.42771413922309875\n",
      "Iteration: 7,\t Loss: 0.5012288093566895, alpha=0.46599462628364563\n",
      "Iteration: 8,\t Loss: 0.48211151361465454, alpha=0.5220810174942017\n",
      "Iteration: 9,\t Loss: 0.47808972001075745, alpha=0.564353883266449\n",
      "Iteration: 10,\t Loss: 0.5098624229431152, alpha=0.5316193699836731\n",
      "Iteration: 11,\t Loss: 0.5109281539916992, alpha=0.4904398024082184\n",
      "Iteration: 12,\t Loss: 0.4751368463039398, alpha=0.4836079180240631\n",
      "Iteration: 13,\t Loss: 0.4550679326057434, alpha=0.5006303787231445\n",
      "Iteration: 14,\t Loss: 0.44539037346839905, alpha=0.5276173949241638\n",
      "Iteration: 15,\t Loss: 0.4403795599937439, alpha=0.5521895289421082\n",
      "Iteration: 16,\t Loss: 0.4373268485069275, alpha=0.5641412734985352\n",
      "Iteration: 17,\t Loss: 0.43494004011154175, alpha=0.5604950785636902\n",
      "Iteration: 18,\t Loss: 0.4325728118419647, alpha=0.545405387878418\n",
      "Iteration: 19,\t Loss: 0.42990386486053467, alpha=0.5267046689987183\n",
      "Iteration: 20,\t Loss: 0.42678824067115784, alpha=0.5134881734848022\n",
      "Iteration: 21,\t Loss: 0.4231816232204437, alpha=0.5122127532958984\n",
      "Iteration: 22,\t Loss: 0.4191010594367981, alpha=0.5230317115783691\n",
      "Iteration: 23,\t Loss: 0.4146057665348053, alpha=0.540333092212677\n",
      "Iteration: 24,\t Loss: 0.40978291630744934, alpha=0.554806113243103\n",
      "Iteration: 25,\t Loss: 0.4047330617904663, alpha=0.5577226281166077\n",
      "Iteration: 26,\t Loss: 0.3995596766471863, alpha=0.5480441451072693\n",
      "Iteration: 27,\t Loss: 0.3943701684474945, alpha=0.5336229205131531\n",
      "Iteration: 28,\t Loss: 0.38927558064460754, alpha=0.5266377925872803\n",
      "Iteration: 29,\t Loss: 0.3843805491924286, alpha=0.5335866808891296\n",
      "Iteration: 30,\t Loss: 0.3797812759876251, alpha=0.5338796377182007\n",
      "Iteration: 31,\t Loss: 0.3796927034854889, alpha=0.5343071818351746\n",
      "Iteration: 32,\t Loss: 0.3796032965183258, alpha=0.5348490476608276\n",
      "Iteration: 33,\t Loss: 0.3795130252838135, alpha=0.5354850888252258\n",
      "Iteration: 34,\t Loss: 0.379422128200531, alpha=0.5361953377723694\n",
      "Iteration: 35,\t Loss: 0.37933045625686646, alpha=0.5369604825973511\n",
      "Iteration: 36,\t Loss: 0.37923839688301086, alpha=0.5377618670463562\n",
      "Iteration: 37,\t Loss: 0.37914568185806274, alpha=0.5385814905166626\n",
      "Iteration: 38,\t Loss: 0.3790524899959564, alpha=0.5394022464752197\n",
      "Iteration: 39,\t Loss: 0.3789590299129486, alpha=0.540208101272583\n",
      "Iteration: 40,\t Loss: 0.37886518239974976, alpha=0.5409840941429138\n",
      "Iteration: 41,\t Loss: 0.3787708878517151, alpha=0.5417166948318481\n",
      "Iteration: 42,\t Loss: 0.3786764442920685, alpha=0.5423939824104309\n",
      "Iteration: 43,\t Loss: 0.37858182191848755, alpha=0.5430057048797607\n",
      "Iteration: 44,\t Loss: 0.3784869611263275, alpha=0.5435434579849243\n",
      "Iteration: 45,\t Loss: 0.37839192152023315, alpha=0.5440009832382202\n",
      "Iteration: 46,\t Loss: 0.3782966732978821, alpha=0.5443741083145142\n",
      "Iteration: 47,\t Loss: 0.37820136547088623, alpha=0.544660747051239\n",
      "Iteration: 48,\t Loss: 0.3781060576438904, alpha=0.54486083984375\n",
      "Iteration: 49,\t Loss: 0.37801066040992737, alpha=0.5449764132499695\n",
      "Iteration: 50,\t Loss: 0.3779151737689972, alpha=0.5450111627578735\n",
      "Iteration: 51,\t Loss: 0.37781965732574463, alpha=0.5449703931808472\n",
      "Iteration: 52,\t Loss: 0.37772423028945923, alpha=0.54486083984375\n",
      "Iteration: 53,\t Loss: 0.3776286840438843, alpha=0.5446902513504028\n",
      "Iteration: 54,\t Loss: 0.3775334060192108, alpha=0.5444672107696533\n",
      "Iteration: 55,\t Loss: 0.3774380683898926, alpha=0.5442010164260864\n",
      "Iteration: 56,\t Loss: 0.37734273076057434, alpha=0.5439012050628662\n",
      "Iteration: 57,\t Loss: 0.3772476315498352, alpha=0.5435774326324463\n",
      "Iteration: 58,\t Loss: 0.3771526515483856, alpha=0.5432392358779907\n",
      "Iteration: 59,\t Loss: 0.3770577311515808, alpha=0.5428957343101501\n",
      "Iteration: 60,\t Loss: 0.37696295976638794, alpha=0.5425556302070618\n",
      "Iteration: 61,\t Loss: 0.3768683075904846, alpha=0.5422268509864807\n",
      "Iteration: 62,\t Loss: 0.3767739236354828, alpha=0.5419164299964905\n",
      "Iteration: 63,\t Loss: 0.3766796588897705, alpha=0.5416304469108582\n",
      "Iteration: 64,\t Loss: 0.37658557295799255, alpha=0.5413737893104553\n",
      "Iteration: 65,\t Loss: 0.3764917254447937, alpha=0.5411503911018372\n",
      "Iteration: 66,\t Loss: 0.3763980567455292, alpha=0.540962815284729\n",
      "Iteration: 67,\t Loss: 0.37630462646484375, alpha=0.5408125519752502\n",
      "Iteration: 68,\t Loss: 0.37621137499809265, alpha=0.5406999588012695\n",
      "Iteration: 69,\t Loss: 0.37611839175224304, alpha=0.5406242609024048\n",
      "Iteration: 70,\t Loss: 0.3760257363319397, alpha=0.5405837297439575\n",
      "Iteration: 71,\t Loss: 0.3759331703186035, alpha=0.5405758023262024\n",
      "Iteration: 72,\t Loss: 0.375840961933136, alpha=0.540597140789032\n",
      "Iteration: 73,\t Loss: 0.37574899196624756, alpha=0.5406438112258911\n",
      "Iteration: 74,\t Loss: 0.375657320022583, alpha=0.5407114624977112\n",
      "Iteration: 75,\t Loss: 0.375565767288208, alpha=0.5407955050468445\n",
      "Iteration: 76,\t Loss: 0.3754746913909912, alpha=0.540891170501709\n",
      "Iteration: 77,\t Loss: 0.37538382411003113, alpha=0.5409936308860779\n",
      "Iteration: 78,\t Loss: 0.37529322504997253, alpha=0.5410983562469482\n",
      "Iteration: 79,\t Loss: 0.3752029538154602, alpha=0.541201114654541\n",
      "Iteration: 80,\t Loss: 0.37511295080184937, alpha=0.5412030816078186\n",
      "Iteration: 81,\t Loss: 0.3751111328601837, alpha=0.5412049293518066\n",
      "Iteration: 82,\t Loss: 0.37510937452316284, alpha=0.5412066578865051\n",
      "Iteration: 83,\t Loss: 0.3751075267791748, alpha=0.5412083268165588\n",
      "Iteration: 84,\t Loss: 0.37510573863983154, alpha=0.541209876537323\n",
      "Iteration: 85,\t Loss: 0.3751039505004883, alpha=0.5412113666534424\n",
      "Iteration: 86,\t Loss: 0.3751021921634674, alpha=0.541212797164917\n",
      "Iteration: 87,\t Loss: 0.37510034441947937, alpha=0.5412141680717468\n",
      "Iteration: 88,\t Loss: 0.3750985860824585, alpha=0.5412154793739319\n",
      "Iteration: 89,\t Loss: 0.37509676814079285, alpha=0.5412167310714722\n",
      "Iteration: 90,\t Loss: 0.3750949501991272, alpha=0.5412179231643677\n",
      "Iteration: 91,\t Loss: 0.37509310245513916, alpha=0.5412191152572632\n",
      "Iteration: 92,\t Loss: 0.3750913143157959, alpha=0.5412202477455139\n",
      "Iteration: 93,\t Loss: 0.37508952617645264, alpha=0.5412213206291199\n",
      "Iteration: 94,\t Loss: 0.37508776783943176, alpha=0.5412223935127258\n",
      "Iteration: 95,\t Loss: 0.3750860095024109, alpha=0.541223406791687\n",
      "Iteration: 96,\t Loss: 0.3750841021537781, alpha=0.5412244200706482\n",
      "Iteration: 97,\t Loss: 0.3750823140144348, alpha=0.5412253737449646\n",
      "Iteration: 98,\t Loss: 0.37508049607276917, alpha=0.541226327419281\n",
      "Iteration: 99,\t Loss: 0.3750787377357483, alpha=0.5412272810935974\n",
      "Iteration: 100,\t Loss: 0.3750768005847931, alpha=0.5412272810935974\n",
      "Iteration: 101,\t Loss: 0.3750767707824707, alpha=0.5412272810935974\n",
      "Iteration: 102,\t Loss: 0.3750767409801483, alpha=0.5412272810935974\n",
      "Iteration: 103,\t Loss: 0.3750767707824707, alpha=0.5412272810935974\n",
      "\t Convergence reached after 104 iterations with alpha=0.5412272810935974\n",
      "Iteration: 0,\t Loss: 2.579664707183838, alpha=1.1050888299942017\n",
      "Iteration: 1,\t Loss: 0.9188048839569092, alpha=1.0252447128295898\n",
      "Iteration: 2,\t Loss: 0.8395267724990845, alpha=0.9387093782424927\n",
      "Iteration: 3,\t Loss: 0.7860010266304016, alpha=0.8472681045532227\n",
      "Iteration: 4,\t Loss: 0.746069610118866, alpha=0.7519742846488953\n",
      "Iteration: 5,\t Loss: 0.7146340608596802, alpha=0.6540241241455078\n",
      "Iteration: 6,\t Loss: 0.689018964767456, alpha=0.5558728575706482\n",
      "Iteration: 7,\t Loss: 0.6676343083381653, alpha=0.4635695517063141\n",
      "Iteration: 8,\t Loss: 0.6494547128677368, alpha=0.38967710733413696\n",
      "Iteration: 9,\t Loss: 0.6337790489196777, alpha=0.3479372262954712\n",
      "Iteration: 10,\t Loss: 0.6201056838035583, alpha=0.3397630751132965\n",
      "Iteration: 11,\t Loss: 0.6080634593963623, alpha=0.3573172092437744\n",
      "Iteration: 12,\t Loss: 0.5973719358444214, alpha=0.392450749874115\n",
      "Iteration: 13,\t Loss: 0.5878139734268188, alpha=0.4386298358440399\n",
      "Iteration: 14,\t Loss: 0.5792179107666016, alpha=0.4898832440376282\n",
      "Iteration: 15,\t Loss: 0.5714463591575623, alpha=0.5394032597541809\n",
      "Iteration: 16,\t Loss: 0.5643870830535889, alpha=0.5790591239929199\n",
      "Iteration: 17,\t Loss: 0.5579459071159363, alpha=0.6015948057174683\n",
      "Iteration: 18,\t Loss: 0.5520449876785278, alpha=0.6043485999107361\n",
      "Iteration: 19,\t Loss: 0.5466181039810181, alpha=0.5896309018135071\n",
      "Iteration: 20,\t Loss: 0.5416094064712524, alpha=0.5623264312744141\n",
      "Iteration: 21,\t Loss: 0.5369718670845032, alpha=0.52839195728302\n",
      "Iteration: 22,\t Loss: 0.5326660871505737, alpha=0.49460747838020325\n",
      "Iteration: 23,\t Loss: 0.5286562442779541, alpha=0.4680783450603485\n",
      "Iteration: 24,\t Loss: 0.5249121189117432, alpha=0.45436689257621765\n",
      "Iteration: 25,\t Loss: 0.521405816078186, alpha=0.45542383193969727\n",
      "Iteration: 26,\t Loss: 0.5181137323379517, alpha=0.4694911539554596\n",
      "Iteration: 27,\t Loss: 0.515015184879303, alpha=0.4923661947250366\n",
      "Iteration: 28,\t Loss: 0.5120918154716492, alpha=0.5183669328689575\n",
      "Iteration: 29,\t Loss: 0.5093285441398621, alpha=0.5410156846046448\n",
      "Iteration: 30,\t Loss: 0.5067104697227478, alpha=0.5412849187850952\n",
      "Iteration: 31,\t Loss: 0.5066604018211365, alpha=0.541387677192688\n",
      "Iteration: 32,\t Loss: 0.5066117644309998, alpha=0.5413385033607483\n",
      "Iteration: 33,\t Loss: 0.5065645575523376, alpha=0.5411513447761536\n",
      "Iteration: 34,\t Loss: 0.5065183639526367, alpha=0.5408395528793335\n",
      "Iteration: 35,\t Loss: 0.5064734220504761, alpha=0.54041588306427\n",
      "Iteration: 36,\t Loss: 0.5064293146133423, alpha=0.5398924946784973\n",
      "Iteration: 37,\t Loss: 0.5063860416412354, alpha=0.539280891418457\n",
      "Iteration: 38,\t Loss: 0.506343424320221, alpha=0.5385918021202087\n",
      "Iteration: 39,\t Loss: 0.5063014030456543, alpha=0.5378354787826538\n",
      "Iteration: 40,\t Loss: 0.5062601566314697, alpha=0.5370216369628906\n",
      "Iteration: 41,\t Loss: 0.5062192678451538, alpha=0.5361592173576355\n",
      "Iteration: 42,\t Loss: 0.5061789155006409, alpha=0.5352567434310913\n",
      "Iteration: 43,\t Loss: 0.5061389207839966, alpha=0.5343221426010132\n",
      "Iteration: 44,\t Loss: 0.5060991644859314, alpha=0.533362865447998\n",
      "Iteration: 45,\t Loss: 0.5060597658157349, alpha=0.5323858857154846\n",
      "Iteration: 46,\t Loss: 0.5060204267501831, alpha=0.5313976407051086\n",
      "Iteration: 47,\t Loss: 0.5059812664985657, alpha=0.5304040908813477\n",
      "Iteration: 48,\t Loss: 0.5059424638748169, alpha=0.5294108390808105\n",
      "Iteration: 49,\t Loss: 0.5059037208557129, alpha=0.5284231305122375\n",
      "Iteration: 50,\t Loss: 0.5058649182319641, alpha=0.5274455547332764\n",
      "Iteration: 51,\t Loss: 0.5058264136314392, alpha=0.5264825820922852\n",
      "Iteration: 52,\t Loss: 0.5057877898216248, alpha=0.5255382061004639\n",
      "Iteration: 53,\t Loss: 0.5057494044303894, alpha=0.5246159434318542\n",
      "Iteration: 54,\t Loss: 0.5057108998298645, alpha=0.5237191319465637\n",
      "Iteration: 55,\t Loss: 0.5056722164154053, alpha=0.5228505730628967\n",
      "Iteration: 56,\t Loss: 0.5056334733963013, alpha=0.5220128297805786\n",
      "Iteration: 57,\t Loss: 0.505594789981842, alpha=0.5212081074714661\n",
      "Iteration: 58,\t Loss: 0.5055559277534485, alpha=0.5204383134841919\n",
      "Iteration: 59,\t Loss: 0.5055171251296997, alpha=0.519705057144165\n",
      "Iteration: 60,\t Loss: 0.5054782032966614, alpha=0.5190095901489258\n",
      "Iteration: 61,\t Loss: 0.505439043045044, alpha=0.5183529257774353\n",
      "Iteration: 62,\t Loss: 0.5053999423980713, alpha=0.5177357792854309\n",
      "Iteration: 63,\t Loss: 0.5053606629371643, alpha=0.517158567905426\n",
      "Iteration: 64,\t Loss: 0.5053213238716125, alpha=0.5166215896606445\n",
      "Iteration: 65,\t Loss: 0.5052818655967712, alpha=0.5161247253417969\n",
      "Iteration: 66,\t Loss: 0.505242109298706, alpha=0.515667736530304\n",
      "Iteration: 67,\t Loss: 0.5052022933959961, alpha=0.5152502059936523\n",
      "Iteration: 68,\t Loss: 0.5051623582839966, alpha=0.5148714184761047\n",
      "Iteration: 69,\t Loss: 0.505122184753418, alpha=0.514530599117279\n",
      "Iteration: 70,\t Loss: 0.5050818920135498, alpha=0.5142267346382141\n",
      "Iteration: 71,\t Loss: 0.5050413608551025, alpha=0.5139586925506592\n",
      "Iteration: 72,\t Loss: 0.5050008296966553, alpha=0.513725221157074\n",
      "Iteration: 73,\t Loss: 0.5049601197242737, alpha=0.5135248899459839\n",
      "Iteration: 74,\t Loss: 0.504919171333313, alpha=0.5133562088012695\n",
      "Iteration: 75,\t Loss: 0.504878044128418, alpha=0.5132176876068115\n",
      "Iteration: 76,\t Loss: 0.5048367381095886, alpha=0.5131076574325562\n",
      "Iteration: 77,\t Loss: 0.5047954320907593, alpha=0.5130244493484497\n",
      "Iteration: 78,\t Loss: 0.5047537088394165, alpha=0.5129662752151489\n",
      "Iteration: 79,\t Loss: 0.5047119855880737, alpha=0.5129314661026001\n",
      "Iteration: 80,\t Loss: 0.5046699047088623, alpha=0.512931227684021\n",
      "Iteration: 81,\t Loss: 0.5046693086624146, alpha=0.5129313468933105\n",
      "Iteration: 82,\t Loss: 0.5046684145927429, alpha=0.5129318237304688\n",
      "Iteration: 83,\t Loss: 0.5046675205230713, alpha=0.5129326581954956\n",
      "Iteration: 84,\t Loss: 0.5046666860580444, alpha=0.5129337906837463\n",
      "Iteration: 85,\t Loss: 0.5046659708023071, alpha=0.512935221195221\n",
      "Iteration: 86,\t Loss: 0.5046650171279907, alpha=0.5129368901252747\n",
      "Iteration: 87,\t Loss: 0.5046640038490295, alpha=0.5129387974739075\n",
      "Iteration: 88,\t Loss: 0.5046632885932922, alpha=0.5129408836364746\n",
      "Iteration: 89,\t Loss: 0.5046622157096863, alpha=0.5129431486129761\n",
      "Iteration: 90,\t Loss: 0.504661500453949, alpha=0.5129455924034119\n",
      "Iteration: 91,\t Loss: 0.5046606659889221, alpha=0.512948215007782\n",
      "Iteration: 92,\t Loss: 0.5046598315238953, alpha=0.5129509568214417\n",
      "Iteration: 93,\t Loss: 0.5046587586402893, alpha=0.5129538774490356\n",
      "Iteration: 94,\t Loss: 0.5046579837799072, alpha=0.5129569172859192\n",
      "Iteration: 95,\t Loss: 0.5046570897102356, alpha=0.5129600763320923\n",
      "Iteration: 96,\t Loss: 0.504656195640564, alpha=0.5129633545875549\n",
      "Iteration: 97,\t Loss: 0.5046553015708923, alpha=0.5129666924476624\n",
      "Iteration: 98,\t Loss: 0.5046543478965759, alpha=0.5129701495170593\n",
      "Iteration: 99,\t Loss: 0.5046533346176147, alpha=0.5129736661911011\n",
      "Iteration: 100,\t Loss: 0.5046524405479431, alpha=0.5129737257957458\n",
      "Iteration: 101,\t Loss: 0.5046526193618774, alpha=0.5129737854003906\n",
      "Iteration: 102,\t Loss: 0.5046525597572327, alpha=0.5129738450050354\n",
      "Iteration: 103,\t Loss: 0.5046524405479431, alpha=0.5129739046096802\n",
      "\t Convergence reached after 104 iterations with alpha=0.512973964214325\n",
      "Iteration: 0,\t Loss: 37.03883361816406, alpha=0.8889477849006653\n",
      "Iteration: 1,\t Loss: 28.22968292236328, alpha=0.810092568397522\n",
      "Iteration: 2,\t Loss: 21.387226104736328, alpha=0.7276515960693359\n",
      "Iteration: 3,\t Loss: 15.757083892822266, alpha=0.640536367893219\n",
      "Iteration: 4,\t Loss: 11.078720092773438, alpha=0.5485073328018188\n",
      "Iteration: 5,\t Loss: 7.166711807250977, alpha=0.4571620523929596\n",
      "Iteration: 6,\t Loss: 3.893810749053955, alpha=0.42381662130355835\n",
      "Iteration: 7,\t Loss: 1.1882524490356445, alpha=0.4484453797340393\n",
      "Iteration: 8,\t Loss: 0.9990925788879395, alpha=0.47123321890830994\n",
      "Iteration: 9,\t Loss: 0.9696930050849915, alpha=0.4927375316619873\n",
      "Iteration: 10,\t Loss: 0.9791330099105835, alpha=0.5138360857963562\n",
      "Iteration: 11,\t Loss: 16.64891815185547, alpha=0.5622583031654358\n",
      "Iteration: 12,\t Loss: 0.9985460638999939, alpha=0.6049269437789917\n",
      "Iteration: 13,\t Loss: 0.9640206694602966, alpha=0.6422437429428101\n",
      "Iteration: 14,\t Loss: 0.9612920880317688, alpha=0.6746121048927307\n",
      "Iteration: 15,\t Loss: 0.9669249057769775, alpha=0.7023974061012268\n",
      "Iteration: 16,\t Loss: 0.9756561517715454, alpha=0.7259228229522705\n",
      "Iteration: 17,\t Loss: 0.9856559634208679, alpha=0.7454711198806763\n",
      "Iteration: 18,\t Loss: 0.9961641430854797, alpha=0.7612854838371277\n",
      "Iteration: 19,\t Loss: 1.00686514377594, alpha=0.7735657691955566\n",
      "Iteration: 20,\t Loss: 1.0176969766616821, alpha=0.7824534773826599\n",
      "Iteration: 21,\t Loss: 1.0288335084915161, alpha=0.7879876494407654\n",
      "Iteration: 22,\t Loss: 1.040905237197876, alpha=0.7899455428123474\n",
      "Iteration: 23,\t Loss: 1.0564656257629395, alpha=0.7867823243141174\n",
      "Iteration: 24,\t Loss: 1.1786160469055176, alpha=0.7404918670654297\n",
      "Iteration: 25,\t Loss: 1.4257245063781738, alpha=0.6753597259521484\n",
      "Iteration: 26,\t Loss: 1.6183407306671143, alpha=0.6015070080757141\n",
      "Iteration: 27,\t Loss: 1.760331392288208, alpha=0.5281397700309753\n",
      "Iteration: 28,\t Loss: 1.8553202152252197, alpha=0.4651789963245392\n",
      "Iteration: 29,\t Loss: 1.9066205024719238, alpha=0.4211968183517456\n",
      "Iteration: 30,\t Loss: 1.917290449142456, alpha=0.4207795262336731\n",
      "Iteration: 31,\t Loss: 1.9167370796203613, alpha=0.42074891924858093\n",
      "Iteration: 32,\t Loss: 1.9154787063598633, alpha=0.42104607820510864\n",
      "Iteration: 33,\t Loss: 1.9135634899139404, alpha=0.421622097492218\n",
      "Iteration: 34,\t Loss: 1.9110283851623535, alpha=0.4224359691143036\n",
      "Iteration: 35,\t Loss: 1.907952070236206, alpha=0.42345312237739563\n",
      "Iteration: 36,\t Loss: 1.9043772220611572, alpha=0.42464420199394226\n",
      "Iteration: 37,\t Loss: 1.9003159999847412, alpha=0.42598411440849304\n",
      "Iteration: 38,\t Loss: 1.8958473205566406, alpha=0.42745134234428406\n",
      "Iteration: 39,\t Loss: 1.890965461730957, alpha=0.4290272891521454\n",
      "Iteration: 40,\t Loss: 1.8857247829437256, alpha=0.43069589138031006\n",
      "Iteration: 41,\t Loss: 1.8801252841949463, alpha=0.43244314193725586\n",
      "Iteration: 42,\t Loss: 1.8742341995239258, alpha=0.43425679206848145\n",
      "Iteration: 43,\t Loss: 1.8680450916290283, alpha=0.43612614274024963\n",
      "Iteration: 44,\t Loss: 1.8615763187408447, alpha=0.4380417764186859\n",
      "Iteration: 45,\t Loss: 1.8548765182495117, alpha=0.4399954378604889\n",
      "Iteration: 46,\t Loss: 1.8479335308074951, alpha=0.4419797956943512\n",
      "Iteration: 47,\t Loss: 1.8407654762268066, alpha=0.4439883828163147\n",
      "Iteration: 48,\t Loss: 1.8334031105041504, alpha=0.44601547718048096\n",
      "Iteration: 49,\t Loss: 1.8258581161499023, alpha=0.4480559825897217\n",
      "Iteration: 50,\t Loss: 1.8181490898132324, alpha=0.4501053988933563\n",
      "Iteration: 51,\t Loss: 1.810269832611084, alpha=0.45215967297554016\n",
      "Iteration: 52,\t Loss: 1.8022387027740479, alpha=0.45421522855758667\n",
      "Iteration: 53,\t Loss: 1.7940614223480225, alpha=0.45626887679100037\n",
      "Iteration: 54,\t Loss: 1.7857506275177002, alpha=0.45831772685050964\n",
      "Iteration: 55,\t Loss: 1.7773120403289795, alpha=0.46035921573638916\n",
      "Iteration: 56,\t Loss: 1.7687640190124512, alpha=0.46239104866981506\n",
      "Iteration: 57,\t Loss: 1.760094404220581, alpha=0.464411199092865\n",
      "Iteration: 58,\t Loss: 1.7513213157653809, alpha=0.4664178192615509\n",
      "Iteration: 59,\t Loss: 1.7424511909484863, alpha=0.46840929985046387\n",
      "Iteration: 60,\t Loss: 1.7334835529327393, alpha=0.4703841209411621\n",
      "Iteration: 61,\t Loss: 1.7244188785552979, alpha=0.47234097123146057\n",
      "Iteration: 62,\t Loss: 1.715287208557129, alpha=0.47427868843078613\n",
      "Iteration: 63,\t Loss: 1.7060520648956299, alpha=0.47619619965553284\n",
      "Iteration: 64,\t Loss: 1.6967442035675049, alpha=0.47809258103370667\n",
      "Iteration: 65,\t Loss: 1.687387466430664, alpha=0.47996699810028076\n",
      "Iteration: 66,\t Loss: 1.6779215335845947, alpha=0.48181864619255066\n",
      "Iteration: 67,\t Loss: 1.6684129238128662, alpha=0.48364686965942383\n",
      "Iteration: 68,\t Loss: 1.6588191986083984, alpha=0.4854510724544525\n",
      "Iteration: 69,\t Loss: 1.649176836013794, alpha=0.48723071813583374\n",
      "Iteration: 70,\t Loss: 1.6394736766815186, alpha=0.4889853298664093\n",
      "Iteration: 71,\t Loss: 1.629697561264038, alpha=0.49071449041366577\n",
      "Iteration: 72,\t Loss: 1.6198787689208984, alpha=0.4924178421497345\n",
      "Iteration: 73,\t Loss: 1.609999418258667, alpha=0.4940950572490692\n",
      "Iteration: 74,\t Loss: 1.6000590324401855, alpha=0.4957458972930908\n",
      "Iteration: 75,\t Loss: 1.5900945663452148, alpha=0.49737006425857544\n",
      "Iteration: 76,\t Loss: 1.5800509452819824, alpha=0.49896740913391113\n",
      "Iteration: 77,\t Loss: 1.5699646472930908, alpha=0.5005376935005188\n",
      "Iteration: 78,\t Loss: 1.5598421096801758, alpha=0.5020808577537537\n",
      "Iteration: 79,\t Loss: 1.5496647357940674, alpha=0.5035967826843262\n",
      "Iteration: 80,\t Loss: 1.5394508838653564, alpha=0.5036265254020691\n",
      "Iteration: 81,\t Loss: 1.5392379760742188, alpha=0.5036557912826538\n",
      "Iteration: 82,\t Loss: 1.5390253067016602, alpha=0.5036846399307251\n",
      "Iteration: 83,\t Loss: 1.5388307571411133, alpha=0.5037131309509277\n",
      "Iteration: 84,\t Loss: 1.5386178493499756, alpha=0.5037412643432617\n",
      "Iteration: 85,\t Loss: 1.538405179977417, alpha=0.5037690997123718\n",
      "Iteration: 86,\t Loss: 1.538198471069336, alpha=0.5037966370582581\n",
      "Iteration: 87,\t Loss: 1.537973403930664, alpha=0.5038239359855652\n",
      "Iteration: 88,\t Loss: 1.537766695022583, alpha=0.503851056098938\n",
      "Iteration: 89,\t Loss: 1.5375540256500244, alpha=0.5038779973983765\n",
      "Iteration: 90,\t Loss: 1.5373411178588867, alpha=0.5039048194885254\n",
      "Iteration: 91,\t Loss: 1.537116289138794, alpha=0.50393146276474\n",
      "Iteration: 92,\t Loss: 1.5369155406951904, alpha=0.503957986831665\n",
      "Iteration: 93,\t Loss: 1.5366907119750977, alpha=0.5039844512939453\n",
      "Iteration: 94,\t Loss: 1.53647780418396, alpha=0.504010796546936\n",
      "Iteration: 95,\t Loss: 1.5362589359283447, alpha=0.504037082195282\n",
      "Iteration: 96,\t Loss: 1.536034107208252, alpha=0.5040633082389832\n",
      "Iteration: 97,\t Loss: 1.53580904006958, alpha=0.5040895342826843\n",
      "Iteration: 98,\t Loss: 1.5355842113494873, alpha=0.5041157007217407\n",
      "Iteration: 99,\t Loss: 1.535365343093872, alpha=0.5041418671607971\n",
      "Iteration: 100,\t Loss: 1.5351343154907227, alpha=0.5041423439979553\n",
      "Iteration: 101,\t Loss: 1.5351402759552002, alpha=0.5041428208351135\n",
      "Iteration: 102,\t Loss: 1.5351343154907227, alpha=0.5041432976722717\n",
      "Iteration: 103,\t Loss: 1.535128116607666, alpha=0.5041437745094299\n",
      "Iteration: 104,\t Loss: 1.5351221561431885, alpha=0.5041442513465881\n",
      "Iteration: 105,\t Loss: 1.535128116607666, alpha=0.5041447281837463\n",
      "Iteration: 106,\t Loss: 1.5351099967956543, alpha=0.5041452050209045\n",
      "Iteration: 107,\t Loss: 1.5351037979125977, alpha=0.5041456818580627\n",
      "Iteration: 108,\t Loss: 1.5351099967956543, alpha=0.504146158695221\n",
      "Iteration: 109,\t Loss: 1.5350978374481201, alpha=0.5041466355323792\n",
      "Iteration: 110,\t Loss: 1.5351037979125977, alpha=0.5041471123695374\n",
      "Iteration: 111,\t Loss: 1.5350978374481201, alpha=0.5041475892066956\n",
      "Iteration: 112,\t Loss: 1.535085678100586, alpha=0.5041480660438538\n",
      "Iteration: 113,\t Loss: 1.5350794792175293, alpha=0.504148542881012\n",
      "Iteration: 114,\t Loss: 1.535085678100586, alpha=0.5041490197181702\n",
      "Iteration: 115,\t Loss: 1.5350673198699951, alpha=0.5041494965553284\n",
      "Iteration: 116,\t Loss: 1.5350613594055176, alpha=0.5041499733924866\n",
      "Iteration: 117,\t Loss: 1.5350492000579834, alpha=0.5041504502296448\n",
      "Iteration: 118,\t Loss: 1.535055160522461, alpha=0.504150927066803\n",
      "Iteration: 119,\t Loss: 1.5350492000579834, alpha=0.5041514039039612\n",
      "Iteration: 120,\t Loss: 1.5350308418273926, alpha=0.5041518807411194\n",
      "\t Convergence reached after 121 iterations with alpha=0.5041523575782776\n",
      "\u001b[32mOptimizing PBkl for concatenated classifier-------------------------------\u001b[0m\n",
      "Iteration: 0,\t Loss: 0.5069146752357483, alpha=0.12674671411514282\n",
      "Iteration: 1,\t Loss: 0.4904040992259979, alpha=0.20047862827777863\n",
      "Iteration: 2,\t Loss: 0.48016971349716187, alpha=0.27819952368736267\n",
      "Iteration: 3,\t Loss: 0.4685460329055786, alpha=0.36253833770751953\n",
      "Iteration: 4,\t Loss: 0.460541307926178, alpha=0.4514140188694\n",
      "Iteration: 5,\t Loss: 0.4526779055595398, alpha=0.539836585521698\n",
      "Iteration: 6,\t Loss: 0.44404226541519165, alpha=0.6017917394638062\n",
      "Iteration: 7,\t Loss: 0.4354853332042694, alpha=0.6034724116325378\n",
      "Iteration: 8,\t Loss: 0.42809566855430603, alpha=0.5692192912101746\n",
      "Iteration: 9,\t Loss: 0.42200177907943726, alpha=0.5189609527587891\n",
      "Iteration: 10,\t Loss: 0.4159381687641144, alpha=0.4750446379184723\n",
      "Iteration: 11,\t Loss: 0.40944910049438477, alpha=0.4650280475616455\n",
      "Iteration: 12,\t Loss: 0.40340447425842285, alpha=0.48382338881492615\n",
      "Iteration: 13,\t Loss: 0.39812418818473816, alpha=0.5181594491004944\n",
      "Iteration: 14,\t Loss: 0.3932429552078247, alpha=0.5547226667404175\n",
      "Iteration: 15,\t Loss: 0.38839295506477356, alpha=0.5761277675628662\n",
      "Iteration: 16,\t Loss: 0.3835059106349945, alpha=0.5723059773445129\n",
      "Iteration: 17,\t Loss: 0.37875688076019287, alpha=0.5496982336044312\n",
      "Iteration: 18,\t Loss: 0.3743683993816376, alpha=0.5226765275001526\n",
      "Iteration: 19,\t Loss: 0.37037193775177, alpha=0.5107297897338867\n",
      "Iteration: 20,\t Loss: 0.3665364384651184, alpha=0.5215492844581604\n",
      "Iteration: 21,\t Loss: 0.3626689910888672, alpha=0.5457331538200378\n",
      "Iteration: 22,\t Loss: 0.35887303948402405, alpha=0.5666276216506958\n",
      "Iteration: 23,\t Loss: 0.3553248345851898, alpha=0.5691709518432617\n",
      "Iteration: 24,\t Loss: 0.35202544927597046, alpha=0.554836630821228\n",
      "Iteration: 25,\t Loss: 0.3488621413707733, alpha=0.5364067554473877\n",
      "Iteration: 26,\t Loss: 0.3457494080066681, alpha=0.5292972326278687\n",
      "Iteration: 27,\t Loss: 0.34268152713775635, alpha=0.5389925837516785\n",
      "Iteration: 28,\t Loss: 0.33971571922302246, alpha=0.5570887327194214\n",
      "Iteration: 29,\t Loss: 0.3369107246398926, alpha=0.5672571063041687\n",
      "Iteration: 30,\t Loss: 0.33424973487854004, alpha=0.56711345911026\n",
      "Iteration: 31,\t Loss: 0.3341968059539795, alpha=0.5666761994361877\n",
      "Iteration: 32,\t Loss: 0.3341425657272339, alpha=0.5659935474395752\n",
      "Iteration: 33,\t Loss: 0.3340872824192047, alpha=0.5651090145111084\n",
      "Iteration: 34,\t Loss: 0.33403080701828003, alpha=0.5640618801116943\n",
      "Iteration: 35,\t Loss: 0.3339734971523285, alpha=0.56288743019104\n",
      "Iteration: 36,\t Loss: 0.3339155316352844, alpha=0.5616177916526794\n",
      "Iteration: 37,\t Loss: 0.33385688066482544, alpha=0.5602825284004211\n",
      "Iteration: 38,\t Loss: 0.333797812461853, alpha=0.5589088797569275\n",
      "Iteration: 39,\t Loss: 0.33373838663101196, alpha=0.5575221180915833\n",
      "Iteration: 40,\t Loss: 0.3336786925792694, alpha=0.5561458468437195\n",
      "Iteration: 41,\t Loss: 0.3336188793182373, alpha=0.554801881313324\n",
      "Iteration: 42,\t Loss: 0.3335590958595276, alpha=0.5535101890563965\n",
      "Iteration: 43,\t Loss: 0.3334991931915283, alpha=0.552288830280304\n",
      "Iteration: 44,\t Loss: 0.33343935012817383, alpha=0.5511536002159119\n",
      "Iteration: 45,\t Loss: 0.3333795964717865, alpha=0.5501180291175842\n",
      "Iteration: 46,\t Loss: 0.3333200216293335, alpha=0.5491929650306702\n",
      "Iteration: 47,\t Loss: 0.33326056599617004, alpha=0.5483864545822144\n",
      "Iteration: 48,\t Loss: 0.33320125937461853, alpha=0.547703742980957\n",
      "Iteration: 49,\t Loss: 0.3331421911716461, alpha=0.5471472144126892\n",
      "Iteration: 50,\t Loss: 0.3330831825733185, alpha=0.5467164516448975\n",
      "Iteration: 51,\t Loss: 0.3330243229866028, alpha=0.5464085936546326\n",
      "Iteration: 52,\t Loss: 0.33296576142311096, alpha=0.5462184548377991\n",
      "Iteration: 53,\t Loss: 0.3329072892665863, alpha=0.5461389422416687\n",
      "Iteration: 54,\t Loss: 0.3328489065170288, alpha=0.5461613535881042\n",
      "Iteration: 55,\t Loss: 0.3327906131744385, alpha=0.5462756156921387\n",
      "Iteration: 56,\t Loss: 0.33273249864578247, alpha=0.5464706420898438\n",
      "Iteration: 57,\t Loss: 0.33267441391944885, alpha=0.5467348694801331\n",
      "Iteration: 58,\t Loss: 0.33261632919311523, alpha=0.547056257724762\n",
      "Iteration: 59,\t Loss: 0.33255836367607117, alpha=0.5474226474761963\n",
      "Iteration: 60,\t Loss: 0.33250048756599426, alpha=0.5478219389915466\n",
      "Iteration: 61,\t Loss: 0.33244258165359497, alpha=0.5482425093650818\n",
      "Iteration: 62,\t Loss: 0.33238473534584045, alpha=0.5486730933189392\n",
      "Iteration: 63,\t Loss: 0.33232685923576355, alpha=0.5491031408309937\n",
      "Iteration: 64,\t Loss: 0.33226895332336426, alpha=0.5495229959487915\n",
      "Iteration: 65,\t Loss: 0.33221104741096497, alpha=0.549923837184906\n",
      "Iteration: 66,\t Loss: 0.3321531116962433, alpha=0.5502979755401611\n",
      "Iteration: 67,\t Loss: 0.3320951759815216, alpha=0.5506389141082764\n",
      "Iteration: 68,\t Loss: 0.33203721046447754, alpha=0.5509412288665771\n",
      "Iteration: 69,\t Loss: 0.3319791257381439, alpha=0.5512009263038635\n",
      "Iteration: 70,\t Loss: 0.3319210708141327, alpha=0.5514150857925415\n",
      "Iteration: 71,\t Loss: 0.33186301589012146, alpha=0.5515821576118469\n",
      "Iteration: 72,\t Loss: 0.33180487155914307, alpha=0.5517016053199768\n",
      "Iteration: 73,\t Loss: 0.3317466378211975, alpha=0.5517742037773132\n",
      "Iteration: 74,\t Loss: 0.33168840408325195, alpha=0.5518016219139099\n",
      "Iteration: 75,\t Loss: 0.33163008093833923, alpha=0.5517864227294922\n",
      "Iteration: 76,\t Loss: 0.3315718173980713, alpha=0.5517320036888123\n",
      "Iteration: 77,\t Loss: 0.3315134644508362, alpha=0.5516423583030701\n",
      "Iteration: 78,\t Loss: 0.3314551115036011, alpha=0.5515219569206238\n",
      "Iteration: 79,\t Loss: 0.3313966989517212, alpha=0.551375687122345\n",
      "Iteration: 80,\t Loss: 0.3313383162021637, alpha=0.5513722896575928\n",
      "Iteration: 81,\t Loss: 0.3313370943069458, alpha=0.5513685345649719\n",
      "Iteration: 82,\t Loss: 0.33133599162101746, alpha=0.5513644218444824\n",
      "Iteration: 83,\t Loss: 0.33133476972579956, alpha=0.551360011100769\n",
      "Iteration: 84,\t Loss: 0.33133360743522644, alpha=0.5513553023338318\n",
      "Iteration: 85,\t Loss: 0.3313324749469757, alpha=0.5513503551483154\n",
      "Iteration: 86,\t Loss: 0.3313313126564026, alpha=0.55134516954422\n",
      "Iteration: 87,\t Loss: 0.3313301205635071, alpha=0.5513397455215454\n",
      "Iteration: 88,\t Loss: 0.33132895827293396, alpha=0.5513341426849365\n",
      "Iteration: 89,\t Loss: 0.33132773637771606, alpha=0.5513283610343933\n",
      "Iteration: 90,\t Loss: 0.33132660388946533, alpha=0.5513224005699158\n",
      "Iteration: 91,\t Loss: 0.3313254117965698, alpha=0.5513163208961487\n",
      "Iteration: 92,\t Loss: 0.3313242793083191, alpha=0.551310122013092\n",
      "Iteration: 93,\t Loss: 0.3313230872154236, alpha=0.5513038039207458\n",
      "Iteration: 94,\t Loss: 0.3313218951225281, alpha=0.5512973666191101\n",
      "Iteration: 95,\t Loss: 0.33132070302963257, alpha=0.5512908101081848\n",
      "Iteration: 96,\t Loss: 0.33131954073905945, alpha=0.5512841939926147\n",
      "Iteration: 97,\t Loss: 0.33131834864616394, alpha=0.5512774586677551\n",
      "Iteration: 98,\t Loss: 0.3313172161579132, alpha=0.5512706637382507\n",
      "Iteration: 99,\t Loss: 0.3313160240650177, alpha=0.5512638092041016\n",
      "Iteration: 100,\t Loss: 0.33131489157676697, alpha=0.5512636303901672\n",
      "Iteration: 101,\t Loss: 0.3313147723674774, alpha=0.5512634515762329\n",
      "Iteration: 102,\t Loss: 0.3313148021697998, alpha=0.5512632727622986\n",
      "Iteration: 103,\t Loss: 0.33131471276283264, alpha=0.5512630939483643\n",
      "Iteration: 104,\t Loss: 0.3313147723674774, alpha=0.5512629151344299\n",
      "Iteration: 105,\t Loss: 0.33131474256515503, alpha=0.5512627363204956\n",
      "Iteration: 106,\t Loss: 0.3313147723674774, alpha=0.5512625575065613\n",
      "Iteration: 107,\t Loss: 0.33131465315818787, alpha=0.551262378692627\n",
      "Iteration: 108,\t Loss: 0.33131468296051025, alpha=0.5512621998786926\n",
      "Iteration: 109,\t Loss: 0.33131465315818787, alpha=0.5512620210647583\n",
      "Iteration: 110,\t Loss: 0.3313145935535431, alpha=0.551261842250824\n",
      "\t Convergence reached after 111 iterations with alpha=0.5512616634368896\n",
      "### Multiview classifier gibbs risk after Optim: 0.14851082077665267\n",
      "\u001b[32mOptimization is done! -------------------------------\u001b[0m\n",
      "\u001b[32mComputing the bound values ans risks -------------------------------\u001b[0m\n",
      "right_hand_side=0.002908977997417624, empirical_gibbs_risk=0.14851082077665267\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "\u001b[34m\n",
      "----------------Run 3---------------\u001b[0m\n",
      "Training multiview classifier-------------------------------\n",
      "Training separate views classifiers-------------------------------\n",
      "Training concatenated view classifier-------------------------------\n",
      "\u001b[32mComputing the bound values ans risks -------------------------------\u001b[0m\n",
      "right_hand_side=0.00320597436600025, empirical_gibbs_risk=0.29852455546147333\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n",
      "### Multiview classifier gibbs risk before Optim: 0.29852455546147333\n",
      "\u001b[32mOptimizing PBkl for multiview classifier-------------------------------\u001b[0m\n",
      "\t Convergence reached after 0 iterations with alpha=0.21402804553508759\n",
      "\t Convergence reached after 1 iterations with alpha=0.28732794523239136\n",
      "\t Convergence reached after 2 iterations with alpha=0.363839328289032\n",
      "\t Convergence reached after 3 iterations with alpha=0.4437878131866455\n",
      "\t Convergence reached after 4 iterations with alpha=0.5294721722602844\n",
      "\t Convergence reached after 5 iterations with alpha=0.5882908701896667\n",
      "\t Convergence reached after 6 iterations with alpha=0.5851280093193054\n",
      "\t Convergence reached after 7 iterations with alpha=0.5516812205314636\n",
      "\t Convergence reached after 8 iterations with alpha=0.5072041153907776\n",
      "\t Convergence reached after 9 iterations with alpha=0.4758198857307434\n",
      "\t Convergence reached after 10 iterations with alpha=0.4801957905292511\n",
      "\t Convergence reached after 11 iterations with alpha=0.5107293725013733\n",
      "\t Convergence reached after 12 iterations with alpha=0.550669252872467\n",
      "\t Convergence reached after 13 iterations with alpha=0.5686466097831726\n",
      "\t Convergence reached after 14 iterations with alpha=0.5537939071655273\n",
      "\t Convergence reached after 15 iterations with alpha=0.5275277495384216\n",
      "\t Convergence reached after 16 iterations with alpha=0.5157203078269958\n",
      "\t Convergence reached after 17 iterations with alpha=0.5272191762924194\n",
      "\t Convergence reached after 18 iterations with alpha=0.5514146685600281\n",
      "\t Convergence reached after 19 iterations with alpha=0.5707880258560181\n",
      "\t Convergence reached after 20 iterations with alpha=0.5701503157615662\n",
      "\t Convergence reached after 21 iterations with alpha=0.5539973974227905\n",
      "\t Convergence reached after 22 iterations with alpha=0.5401482582092285\n",
      "\t Convergence reached after 23 iterations with alpha=0.5453140139579773\n",
      "\t Convergence reached after 24 iterations with alpha=0.5645339488983154\n",
      "\t Convergence reached after 25 iterations with alpha=0.5766417980194092\n",
      "\t Convergence reached after 26 iterations with alpha=0.5681127905845642\n",
      "\t Convergence reached after 27 iterations with alpha=0.554268479347229\n",
      "\t Convergence reached after 28 iterations with alpha=0.5556095242500305\n",
      "\t Convergence reached after 29 iterations with alpha=0.5699230432510376\n",
      "\t Convergence reached after 30 iterations with alpha=0.5701023936271667\n",
      "\t Convergence reached after 31 iterations with alpha=0.5701805353164673\n",
      "\t Convergence reached after 32 iterations with alpha=0.5701640248298645\n",
      "\t Convergence reached after 33 iterations with alpha=0.5700613260269165\n",
      "\t Convergence reached after 34 iterations with alpha=0.569882333278656\n",
      "\t Convergence reached after 35 iterations with alpha=0.5696381330490112\n",
      "\t Convergence reached after 36 iterations with alpha=0.5693405270576477\n",
      "\t Convergence reached after 37 iterations with alpha=0.5690018534660339\n",
      "\t Convergence reached after 38 iterations with alpha=0.5686344504356384\n",
      "\t Convergence reached after 39 iterations with alpha=0.5682506561279297\n",
      "\t Convergence reached after 40 iterations with alpha=0.5678622722625732\n",
      "\t Convergence reached after 41 iterations with alpha=0.5674804449081421\n",
      "\t Convergence reached after 42 iterations with alpha=0.567115306854248\n",
      "\t Convergence reached after 43 iterations with alpha=0.5667757987976074\n",
      "\t Convergence reached after 44 iterations with alpha=0.5664695501327515\n",
      "\t Convergence reached after 45 iterations with alpha=0.5662025213241577\n",
      "\t Convergence reached after 46 iterations with alpha=0.5659793019294739\n",
      "\t Convergence reached after 47 iterations with alpha=0.5658025145530701\n",
      "\t Convergence reached after 48 iterations with alpha=0.5656734108924866\n",
      "\t Convergence reached after 49 iterations with alpha=0.5655913949012756\n",
      "\t Convergence reached after 50 iterations with alpha=0.5655545592308044\n",
      "\t Convergence reached after 51 iterations with alpha=0.5655596256256104\n",
      "\t Convergence reached after 52 iterations with alpha=0.56560218334198\n",
      "\t Convergence reached after 53 iterations with alpha=0.5656769275665283\n",
      "\t Convergence reached after 54 iterations with alpha=0.5657778978347778\n",
      "\t Convergence reached after 55 iterations with alpha=0.565898597240448\n",
      "\t Convergence reached after 56 iterations with alpha=0.566032350063324\n",
      "\t Convergence reached after 57 iterations with alpha=0.5661726593971252\n",
      "\t Convergence reached after 58 iterations with alpha=0.5663131475448608\n",
      "\t Convergence reached after 59 iterations with alpha=0.5664478540420532\n",
      "\t Convergence reached after 60 iterations with alpha=0.5665715336799622\n",
      "\t Convergence reached after 61 iterations with alpha=0.5666795969009399\n",
      "\t Convergence reached after 62 iterations with alpha=0.5667684674263\n",
      "\t Convergence reached after 63 iterations with alpha=0.5668355226516724\n",
      "\t Convergence reached after 64 iterations with alpha=0.5668790340423584\n",
      "\t Convergence reached after 65 iterations with alpha=0.5668983459472656\n",
      "\t Convergence reached after 66 iterations with alpha=0.5668937563896179\n",
      "\t Convergence reached after 67 iterations with alpha=0.566866397857666\n",
      "\t Convergence reached after 68 iterations with alpha=0.5668181777000427\n",
      "\t Convergence reached after 69 iterations with alpha=0.5667516589164734\n",
      "\t Convergence reached after 70 iterations with alpha=0.5666699409484863\n",
      "\t Convergence reached after 71 iterations with alpha=0.5665764808654785\n",
      "\t Convergence reached after 72 iterations with alpha=0.5664748549461365\n",
      "\t Convergence reached after 73 iterations with alpha=0.5663686990737915\n",
      "\t Convergence reached after 74 iterations with alpha=0.5662616491317749\n",
      "\t Convergence reached after 75 iterations with alpha=0.5661569833755493\n",
      "\t Convergence reached after 76 iterations with alpha=0.566057562828064\n",
      "\t Convergence reached after 77 iterations with alpha=0.5659659504890442\n",
      "\t Convergence reached after 78 iterations with alpha=0.5658840537071228\n",
      "\t Convergence reached after 79 iterations with alpha=0.5658133029937744\n",
      "\t Convergence reached after 80 iterations with alpha=0.5658121109008789\n",
      "\t Convergence reached after 81 iterations with alpha=0.5658111572265625\n",
      "\t Convergence reached after 82 iterations with alpha=0.5658103823661804\n",
      "\t Convergence reached after 83 iterations with alpha=0.5658097863197327\n",
      "\t Convergence reached after 84 iterations with alpha=0.5658093690872192\n",
      "\t Convergence reached after 85 iterations with alpha=0.5658091306686401\n",
      "\t Convergence reached after 86 iterations with alpha=0.5658090114593506\n",
      "\t Convergence reached after 87 iterations with alpha=0.5658090114593506\n",
      "\t Convergence reached after 88 iterations with alpha=0.5658091306686401\n",
      "\t Convergence reached after 89 iterations with alpha=0.5658093690872192\n",
      "\t Convergence reached after 90 iterations with alpha=0.5658096671104431\n",
      "\t Convergence reached after 91 iterations with alpha=0.5658100843429565\n",
      "\t Convergence reached after 92 iterations with alpha=0.5658105611801147\n",
      "\t Convergence reached after 93 iterations with alpha=0.5658110976219177\n",
      "\t Convergence reached after 94 iterations with alpha=0.5658116936683655\n",
      "\t Convergence reached after 95 iterations with alpha=0.565812349319458\n",
      "\t Convergence reached after 96 iterations with alpha=0.5658130645751953\n",
      "\t Convergence reached after 97 iterations with alpha=0.5658138394355774\n",
      "\t Convergence reached after 98 iterations with alpha=0.5658146739006042\n",
      "\t Convergence reached after 99 iterations with alpha=0.5658155083656311\n",
      "\t Convergence reached after 100 iterations with alpha=0.5658155083656311\n",
      "\t Convergence reached after 101 iterations, alpha=0.5658155083656311\n",
      "\u001b[33mOptimization took 0:00:04.507346 -------------------------------\u001b[0m\n",
      "\u001b[32mOptimizing PBkl for separate views classifiers-------------------------------\u001b[0m\n",
      "Iteration: 0,\t Loss: 170.24319458007812, alpha=1.5369465351104736\n",
      "Iteration: 1,\t Loss: 114.18659210205078, alpha=1.4548490047454834\n",
      "Iteration: 2,\t Loss: 83.71662139892578, alpha=1.36897611618042\n",
      "Iteration: 3,\t Loss: 63.61043930053711, alpha=1.2793840169906616\n",
      "Iteration: 4,\t Loss: 49.3908805847168, alpha=1.186583161354065\n",
      "Iteration: 5,\t Loss: 38.820655822753906, alpha=1.0909473896026611\n",
      "Iteration: 6,\t Loss: 30.66327667236328, alpha=0.9927736520767212\n",
      "Iteration: 7,\t Loss: 24.182565689086914, alpha=0.8923506736755371\n",
      "Iteration: 8,\t Loss: 18.91408348083496, alpha=0.7900657653808594\n",
      "Iteration: 9,\t Loss: 14.550785064697266, alpha=0.6866409182548523\n",
      "Iteration: 10,\t Loss: 10.881709098815918, alpha=0.5837016701698303\n",
      "Iteration: 11,\t Loss: 7.757194519042969, alpha=0.48510342836380005\n",
      "Iteration: 12,\t Loss: 5.068221092224121, alpha=0.3992142081260681\n",
      "Iteration: 13,\t Loss: 2.7334136962890625, alpha=0.33891135454177856\n",
      "Iteration: 14,\t Loss: 1.0470608472824097, alpha=0.28556862473487854\n",
      "Iteration: 15,\t Loss: 0.9969919323921204, alpha=0.23826609551906586\n",
      "Iteration: 16,\t Loss: 0.9633410573005676, alpha=0.1966341733932495\n",
      "Iteration: 17,\t Loss: 0.9361616373062134, alpha=0.1604069620370865\n",
      "Iteration: 18,\t Loss: 0.9134618043899536, alpha=0.12938319146633148\n",
      "Iteration: 19,\t Loss: 0.894231379032135, alpha=0.10340750217437744\n",
      "Iteration: 20,\t Loss: 0.8778120875358582, alpha=0.08235775679349899\n",
      "Iteration: 21,\t Loss: 0.863726794719696, alpha=0.06613536924123764\n",
      "Iteration: 22,\t Loss: 0.851611316204071, alpha=0.05465732514858246\n",
      "Iteration: 23,\t Loss: 0.841177225112915, alpha=0.047849297523498535\n",
      "Iteration: 24,\t Loss: 0.8321917057037354, alpha=0.0456392727792263\n",
      "Iteration: 25,\t Loss: 0.8244631886482239, alpha=0.04795137792825699\n",
      "Iteration: 26,\t Loss: 0.817831814289093, alpha=0.05469971150159836\n",
      "Iteration: 27,\t Loss: 0.8121629357337952, alpha=0.06578195095062256\n",
      "Iteration: 28,\t Loss: 0.8073413372039795, alpha=0.08107268810272217\n",
      "Iteration: 29,\t Loss: 0.8032680749893188, alpha=0.10041635483503342\n",
      "Iteration: 30,\t Loss: 0.7998580932617188, alpha=0.10088042169809341\n",
      "Iteration: 31,\t Loss: 0.7997986674308777, alpha=0.10141703486442566\n",
      "Iteration: 32,\t Loss: 0.799744188785553, alpha=0.10202021896839142\n",
      "Iteration: 33,\t Loss: 0.7996940016746521, alpha=0.10268451273441315\n",
      "Iteration: 34,\t Loss: 0.7996475100517273, alpha=0.1034049540758133\n",
      "Iteration: 35,\t Loss: 0.7996049523353577, alpha=0.10417699068784714\n",
      "Iteration: 36,\t Loss: 0.7995653748512268, alpha=0.1049964651465416\n",
      "Iteration: 37,\t Loss: 0.7995288968086243, alpha=0.10585957765579224\n",
      "Iteration: 38,\t Loss: 0.7994950413703918, alpha=0.1067628487944603\n",
      "Iteration: 39,\t Loss: 0.79946368932724, alpha=0.10770308971405029\n",
      "Iteration: 40,\t Loss: 0.7994345426559448, alpha=0.10867738723754883\n",
      "Iteration: 41,\t Loss: 0.7994073629379272, alpha=0.1096830666065216\n",
      "Iteration: 42,\t Loss: 0.7993820309638977, alpha=0.11071769148111343\n",
      "Iteration: 43,\t Loss: 0.7993583679199219, alpha=0.11177900433540344\n",
      "Iteration: 44,\t Loss: 0.799336314201355, alpha=0.11286494135856628\n",
      "Iteration: 45,\t Loss: 0.7993154525756836, alpha=0.11397361755371094\n",
      "Iteration: 46,\t Loss: 0.7992961406707764, alpha=0.11510329693555832\n",
      "Iteration: 47,\t Loss: 0.7992777228355408, alpha=0.11625238507986069\n",
      "Iteration: 48,\t Loss: 0.7992604374885559, alpha=0.11741941422224045\n",
      "Iteration: 49,\t Loss: 0.7992440462112427, alpha=0.11860304325819016\n",
      "Iteration: 50,\t Loss: 0.7992286086082458, alpha=0.11980202794075012\n",
      "Iteration: 51,\t Loss: 0.7992135286331177, alpha=0.12101523578166962\n",
      "Iteration: 52,\t Loss: 0.7991995215415955, alpha=0.12224160879850388\n",
      "Iteration: 53,\t Loss: 0.7991859912872314, alpha=0.1234801858663559\n",
      "Iteration: 54,\t Loss: 0.7991732954978943, alpha=0.12473007291555405\n",
      "Iteration: 55,\t Loss: 0.7991607785224915, alpha=0.12599045038223267\n",
      "Iteration: 56,\t Loss: 0.7991489768028259, alpha=0.12726056575775146\n",
      "Iteration: 57,\t Loss: 0.799137532711029, alpha=0.12853971123695374\n",
      "Iteration: 58,\t Loss: 0.7991263270378113, alpha=0.12982724606990814\n",
      "Iteration: 59,\t Loss: 0.7991155982017517, alpha=0.13112257421016693\n",
      "Iteration: 60,\t Loss: 0.7991052865982056, alpha=0.13242512941360474\n",
      "Iteration: 61,\t Loss: 0.7990952134132385, alpha=0.13373439013957977\n",
      "Iteration: 62,\t Loss: 0.799085259437561, alpha=0.13504989445209503\n",
      "Iteration: 63,\t Loss: 0.7990756034851074, alpha=0.1363712102174759\n",
      "Iteration: 64,\t Loss: 0.7990660667419434, alpha=0.13769792020320892\n",
      "Iteration: 65,\t Loss: 0.799056887626648, alpha=0.1390296369791031\n",
      "Iteration: 66,\t Loss: 0.7990480661392212, alpha=0.14036600291728973\n",
      "Iteration: 67,\t Loss: 0.7990389466285706, alpha=0.1417066901922226\n",
      "Iteration: 68,\t Loss: 0.7990301847457886, alpha=0.1430513858795166\n",
      "Iteration: 69,\t Loss: 0.7990216612815857, alpha=0.14439980685710907\n",
      "Iteration: 70,\t Loss: 0.7990130186080933, alpha=0.1457516849040985\n",
      "Iteration: 71,\t Loss: 0.7990046143531799, alpha=0.14710676670074463\n",
      "Iteration: 72,\t Loss: 0.7989963293075562, alpha=0.14846479892730713\n",
      "Iteration: 73,\t Loss: 0.7989879846572876, alpha=0.1498255729675293\n",
      "Iteration: 74,\t Loss: 0.7989799380302429, alpha=0.15118886530399323\n",
      "Iteration: 75,\t Loss: 0.7989718317985535, alpha=0.1525544971227646\n",
      "Iteration: 76,\t Loss: 0.7989636659622192, alpha=0.15392227470874786\n",
      "Iteration: 77,\t Loss: 0.7989555597305298, alpha=0.15529203414916992\n",
      "Iteration: 78,\t Loss: 0.7989479303359985, alpha=0.15666359663009644\n",
      "Iteration: 79,\t Loss: 0.7989398241043091, alpha=0.15803679823875427\n",
      "Iteration: 80,\t Loss: 0.7989319562911987, alpha=0.15806429088115692\n",
      "Iteration: 81,\t Loss: 0.7989317774772644, alpha=0.15809182822704315\n",
      "Iteration: 82,\t Loss: 0.7989317178726196, alpha=0.15811939537525177\n",
      "Iteration: 83,\t Loss: 0.7989315390586853, alpha=0.15814700722694397\n",
      "Iteration: 84,\t Loss: 0.7989313006401062, alpha=0.15817466378211975\n",
      "\t Convergence reached after 85 iterations with alpha=0.1582023650407791\n",
      "Iteration: 0,\t Loss: 36.871421813964844, alpha=1.288199543952942\n",
      "Iteration: 1,\t Loss: 21.2726993560791, alpha=1.2073463201522827\n",
      "Iteration: 2,\t Loss: 10.695318222045898, alpha=1.1236200332641602\n",
      "Iteration: 3,\t Loss: 2.7681407928466797, alpha=1.0363048315048218\n",
      "Iteration: 4,\t Loss: 0.9440141916275024, alpha=0.9605607986450195\n",
      "Iteration: 5,\t Loss: 0.8526725769042969, alpha=0.8932331204414368\n",
      "Iteration: 6,\t Loss: 0.7866679430007935, alpha=0.8323171734809875\n",
      "Iteration: 7,\t Loss: 0.736260175704956, alpha=0.7764960527420044\n",
      "Iteration: 8,\t Loss: 0.6965068578720093, alpha=0.7248972058296204\n",
      "Iteration: 9,\t Loss: 0.664433479309082, alpha=0.6769554018974304\n",
      "Iteration: 10,\t Loss: 0.6381047964096069, alpha=0.632329523563385\n",
      "Iteration: 11,\t Loss: 0.6161965727806091, alpha=0.590846836566925\n",
      "Iteration: 12,\t Loss: 0.5977670550346375, alpha=0.5524632334709167\n",
      "Iteration: 13,\t Loss: 0.5821267366409302, alpha=0.5172317028045654\n",
      "Iteration: 14,\t Loss: 0.5687575936317444, alpha=0.48527562618255615\n",
      "Iteration: 15,\t Loss: 0.5572622418403625, alpha=0.45676448941230774\n",
      "Iteration: 16,\t Loss: 0.547330379486084, alpha=0.4318913221359253\n",
      "Iteration: 17,\t Loss: 0.538715660572052, alpha=0.41085076332092285\n",
      "Iteration: 18,\t Loss: 0.5312194228172302, alpha=0.39381837844848633\n",
      "Iteration: 19,\t Loss: 0.5246800780296326, alpha=0.3809314966201782\n",
      "Iteration: 20,\t Loss: 0.5189642310142517, alpha=0.372272253036499\n",
      "Iteration: 21,\t Loss: 0.5139608979225159, alpha=0.3678533434867859\n",
      "Iteration: 22,\t Loss: 0.5095764994621277, alpha=0.36760714650154114\n",
      "Iteration: 23,\t Loss: 0.505732536315918, alpha=0.37137821316719055\n",
      "Iteration: 24,\t Loss: 0.5023614168167114, alpha=0.3789193630218506\n",
      "Iteration: 25,\t Loss: 0.49940595030784607, alpha=0.389891117811203\n",
      "Iteration: 26,\t Loss: 0.496816486120224, alpha=0.4038640260696411\n",
      "Iteration: 27,\t Loss: 0.4945501983165741, alpha=0.4203239679336548\n",
      "Iteration: 28,\t Loss: 0.49257004261016846, alpha=0.43868017196655273\n",
      "Iteration: 29,\t Loss: 0.49084383249282837, alpha=0.45827698707580566\n",
      "Iteration: 30,\t Loss: 0.48934316635131836, alpha=0.4586796164512634\n",
      "Iteration: 31,\t Loss: 0.489316463470459, alpha=0.4590940475463867\n",
      "Iteration: 32,\t Loss: 0.48929208517074585, alpha=0.4595194160938263\n",
      "Iteration: 33,\t Loss: 0.4892696142196655, alpha=0.45995503664016724\n",
      "Iteration: 34,\t Loss: 0.4892490804195404, alpha=0.46040016412734985\n",
      "Iteration: 35,\t Loss: 0.4892301559448242, alpha=0.4608541429042816\n",
      "Iteration: 36,\t Loss: 0.4892128109931946, alpha=0.4613163471221924\n",
      "Iteration: 37,\t Loss: 0.48919689655303955, alpha=0.4617861807346344\n",
      "Iteration: 38,\t Loss: 0.48918208479881287, alpha=0.4622630774974823\n",
      "Iteration: 39,\t Loss: 0.4891684651374817, alpha=0.4627465307712555\n",
      "Iteration: 40,\t Loss: 0.48915591835975647, alpha=0.463236004114151\n",
      "Iteration: 41,\t Loss: 0.48914429545402527, alpha=0.4637310206890106\n",
      "Iteration: 42,\t Loss: 0.48913338780403137, alpha=0.4642311930656433\n",
      "Iteration: 43,\t Loss: 0.4891233444213867, alpha=0.46473604440689087\n",
      "Iteration: 44,\t Loss: 0.4891139268875122, alpha=0.46524515748023987\n",
      "Iteration: 45,\t Loss: 0.4891051650047302, alpha=0.46575817465782166\n",
      "Iteration: 46,\t Loss: 0.48909690976142883, alpha=0.4662747383117676\n",
      "Iteration: 47,\t Loss: 0.4890892207622528, alpha=0.4667944610118866\n",
      "Iteration: 48,\t Loss: 0.489082008600235, alpha=0.46731701493263245\n",
      "Iteration: 49,\t Loss: 0.48907509446144104, alpha=0.46784213185310364\n",
      "Iteration: 50,\t Loss: 0.4890686273574829, alpha=0.4683694839477539\n",
      "Iteration: 51,\t Loss: 0.4890625476837158, alpha=0.4688987731933594\n",
      "Iteration: 52,\t Loss: 0.48905667662620544, alpha=0.46942973136901855\n",
      "Iteration: 53,\t Loss: 0.48905110359191895, alpha=0.46996212005615234\n",
      "Iteration: 54,\t Loss: 0.48904556035995483, alpha=0.47049564123153687\n",
      "Iteration: 55,\t Loss: 0.4890405237674713, alpha=0.4710300862789154\n",
      "Iteration: 56,\t Loss: 0.4890354573726654, alpha=0.47156521677970886\n",
      "Iteration: 57,\t Loss: 0.4890305995941162, alpha=0.4721008241176605\n",
      "Iteration: 58,\t Loss: 0.4890258312225342, alpha=0.47263669967651367\n",
      "Iteration: 59,\t Loss: 0.4890214204788208, alpha=0.4731726348400116\n",
      "Iteration: 60,\t Loss: 0.48901689052581787, alpha=0.47370845079421997\n",
      "Iteration: 61,\t Loss: 0.48901253938674927, alpha=0.4742439389228821\n",
      "Iteration: 62,\t Loss: 0.48900818824768066, alpha=0.4747789204120636\n",
      "Iteration: 63,\t Loss: 0.4890039563179016, alpha=0.475313276052475\n",
      "Iteration: 64,\t Loss: 0.48899975419044495, alpha=0.4758467972278595\n",
      "Iteration: 65,\t Loss: 0.48899561166763306, alpha=0.4763793349266052\n",
      "Iteration: 66,\t Loss: 0.48899149894714355, alpha=0.4769107401371002\n",
      "Iteration: 67,\t Loss: 0.4889874756336212, alpha=0.47744086384773254\n",
      "Iteration: 68,\t Loss: 0.4889834225177765, alpha=0.47796961665153503\n",
      "Iteration: 69,\t Loss: 0.4889794886112213, alpha=0.47849681973457336\n",
      "Iteration: 70,\t Loss: 0.4889754056930542, alpha=0.479022353887558\n",
      "Iteration: 71,\t Loss: 0.4889713525772095, alpha=0.47954609990119934\n",
      "Iteration: 72,\t Loss: 0.4889673590660095, alpha=0.4800679385662079\n",
      "Iteration: 73,\t Loss: 0.4889632761478424, alpha=0.48058775067329407\n",
      "Iteration: 74,\t Loss: 0.4889591634273529, alpha=0.4811054468154907\n",
      "Iteration: 75,\t Loss: 0.48895519971847534, alpha=0.4816209077835083\n",
      "Iteration: 76,\t Loss: 0.4889511466026306, alpha=0.48213404417037964\n",
      "Iteration: 77,\t Loss: 0.48894694447517395, alpha=0.4826447367668152\n",
      "Iteration: 78,\t Loss: 0.48894286155700684, alpha=0.48315292596817017\n",
      "Iteration: 79,\t Loss: 0.48893865942955017, alpha=0.4836585223674774\n",
      "Iteration: 80,\t Loss: 0.4889344871044159, alpha=0.48366856575012207\n",
      "Iteration: 81,\t Loss: 0.4889344274997711, alpha=0.48367857933044434\n",
      "Iteration: 82,\t Loss: 0.48893439769744873, alpha=0.4836885333061218\n",
      "Iteration: 83,\t Loss: 0.4889342188835144, alpha=0.4836984872817993\n",
      "Iteration: 84,\t Loss: 0.48893412947654724, alpha=0.4837084114551544\n",
      "\t Convergence reached after 85 iterations with alpha=0.4837183356285095\n",
      "Iteration: 0,\t Loss: 7.481499671936035, alpha=0.8444805145263672\n",
      "Iteration: 1,\t Loss: 1.5026991367340088, alpha=0.7658479809761047\n",
      "Iteration: 2,\t Loss: 0.9475927948951721, alpha=0.7003400921821594\n",
      "Iteration: 3,\t Loss: 0.880665123462677, alpha=0.6410305500030518\n",
      "Iteration: 4,\t Loss: 0.8330100774765015, alpha=0.585394561290741\n",
      "Iteration: 5,\t Loss: 0.7967125177383423, alpha=0.5332298874855042\n",
      "Iteration: 6,\t Loss: 0.7680128216743469, alpha=0.48597848415374756\n",
      "Iteration: 7,\t Loss: 0.744766354560852, alpha=0.44641169905662537\n",
      "Iteration: 8,\t Loss: 0.7256270051002502, alpha=0.4181099832057953\n",
      "Iteration: 9,\t Loss: 0.7096955180168152, alpha=0.4042888879776001\n",
      "Iteration: 10,\t Loss: 0.696340799331665, alpha=0.40630266070365906\n",
      "Iteration: 11,\t Loss: 0.6851075887680054, alpha=0.4230450987815857\n",
      "Iteration: 12,\t Loss: 0.6756582260131836, alpha=0.4515334367752075\n",
      "Iteration: 13,\t Loss: 0.66773921251297, alpha=0.48756879568099976\n",
      "Iteration: 14,\t Loss: 0.6611575484275818, alpha=0.5256344079971313\n",
      "Iteration: 15,\t Loss: 0.6557650566101074, alpha=0.5583496689796448\n",
      "Iteration: 16,\t Loss: 0.6514465808868408, alpha=0.5774815678596497\n",
      "Iteration: 17,\t Loss: 0.6481117010116577, alpha=0.5782450437545776\n",
      "Iteration: 18,\t Loss: 0.6456876993179321, alpha=0.5621463656425476\n",
      "Iteration: 19,\t Loss: 0.6441193222999573, alpha=0.5351529717445374\n",
      "Iteration: 20,\t Loss: 0.643362820148468, alpha=0.5061239004135132\n",
      "Iteration: 21,\t Loss: 0.6433796882629395, alpha=0.4860883951187134\n",
      "Iteration: 22,\t Loss: 0.6441254615783691, alpha=0.4833112955093384\n",
      "Iteration: 23,\t Loss: 0.6455429792404175, alpha=0.49760621786117554\n",
      "Iteration: 24,\t Loss: 0.6475576758384705, alpha=0.5219361186027527\n",
      "Iteration: 25,\t Loss: 0.6500667333602905, alpha=0.5447435975074768\n",
      "Iteration: 26,\t Loss: 0.6529256701469421, alpha=0.5532847046852112\n",
      "Iteration: 27,\t Loss: 0.6559370160102844, alpha=0.5436796545982361\n",
      "Iteration: 28,\t Loss: 0.6588571667671204, alpha=0.5232693552970886\n",
      "Iteration: 29,\t Loss: 0.6614217758178711, alpha=0.5053728818893433\n",
      "Iteration: 30,\t Loss: 0.6633757948875427, alpha=0.5053026080131531\n",
      "Iteration: 31,\t Loss: 0.6633980870246887, alpha=0.5054845809936523\n",
      "Iteration: 32,\t Loss: 0.6634030342102051, alpha=0.5058865547180176\n",
      "Iteration: 33,\t Loss: 0.6633923053741455, alpha=0.5064780116081238\n",
      "Iteration: 34,\t Loss: 0.663366973400116, alpha=0.5072306394577026\n",
      "Iteration: 35,\t Loss: 0.6633284687995911, alpha=0.5081182718276978\n",
      "Iteration: 36,\t Loss: 0.663277268409729, alpha=0.5091166496276855\n",
      "Iteration: 37,\t Loss: 0.6632149815559387, alpha=0.5102035999298096\n",
      "Iteration: 38,\t Loss: 0.6631422638893127, alpha=0.5113584399223328\n",
      "Iteration: 39,\t Loss: 0.6630600094795227, alpha=0.512562096118927\n",
      "Iteration: 40,\t Loss: 0.6629693508148193, alpha=0.5137969255447388\n",
      "Iteration: 41,\t Loss: 0.6628708839416504, alpha=0.5150465965270996\n",
      "Iteration: 42,\t Loss: 0.6627650856971741, alpha=0.516295850276947\n",
      "Iteration: 43,\t Loss: 0.6626527905464172, alpha=0.5175307393074036\n",
      "Iteration: 44,\t Loss: 0.6625346541404724, alpha=0.518738329410553\n",
      "Iteration: 45,\t Loss: 0.6624113321304321, alpha=0.5199069380760193\n",
      "Iteration: 46,\t Loss: 0.6622833013534546, alpha=0.5210260152816772\n",
      "Iteration: 47,\t Loss: 0.6621509790420532, alpha=0.5220862030982971\n",
      "Iteration: 48,\t Loss: 0.6620146632194519, alpha=0.5230793952941895\n",
      "Iteration: 49,\t Loss: 0.6618751883506775, alpha=0.5239987373352051\n",
      "Iteration: 50,\t Loss: 0.66173255443573, alpha=0.5248388051986694\n",
      "Iteration: 51,\t Loss: 0.6615874171257019, alpha=0.5255953669548035\n",
      "Iteration: 52,\t Loss: 0.6614396572113037, alpha=0.5262656807899475\n",
      "Iteration: 53,\t Loss: 0.661290168762207, alpha=0.5268481373786926\n",
      "Iteration: 54,\t Loss: 0.6611387133598328, alpha=0.5273424386978149\n",
      "Iteration: 55,\t Loss: 0.6609857678413391, alpha=0.5277495384216309\n",
      "Iteration: 56,\t Loss: 0.6608317494392395, alpha=0.5280713438987732\n",
      "Iteration: 57,\t Loss: 0.6606763005256653, alpha=0.5283109545707703\n",
      "Iteration: 58,\t Loss: 0.6605203747749329, alpha=0.5284721851348877\n",
      "Iteration: 59,\t Loss: 0.6603633165359497, alpha=0.528559684753418\n",
      "Iteration: 60,\t Loss: 0.660206139087677, alpha=0.5285787582397461\n",
      "Iteration: 61,\t Loss: 0.6600486040115356, alpha=0.5285351276397705\n",
      "Iteration: 62,\t Loss: 0.6598905920982361, alpha=0.5284350514411926\n",
      "Iteration: 63,\t Loss: 0.6597326397895813, alpha=0.5282849073410034\n",
      "Iteration: 64,\t Loss: 0.6595745086669922, alpha=0.528091311454773\n",
      "Iteration: 65,\t Loss: 0.6594167351722717, alpha=0.5278608798980713\n",
      "Iteration: 66,\t Loss: 0.6592590808868408, alpha=0.5276002287864685\n",
      "Iteration: 67,\t Loss: 0.6591019630432129, alpha=0.5273158550262451\n",
      "Iteration: 68,\t Loss: 0.6589450836181641, alpha=0.5270139575004578\n",
      "Iteration: 69,\t Loss: 0.6587884426116943, alpha=0.526700496673584\n",
      "Iteration: 70,\t Loss: 0.6586326360702515, alpha=0.5263811349868774\n",
      "Iteration: 71,\t Loss: 0.6584775447845459, alpha=0.5260611772537231\n",
      "Iteration: 72,\t Loss: 0.658322811126709, alpha=0.5257453322410583\n",
      "Iteration: 73,\t Loss: 0.658168613910675, alpha=0.5254379510879517\n",
      "Iteration: 74,\t Loss: 0.6580154895782471, alpha=0.5251429080963135\n",
      "Iteration: 75,\t Loss: 0.6578631401062012, alpha=0.5248635411262512\n",
      "Iteration: 76,\t Loss: 0.6577114462852478, alpha=0.5246026515960693\n",
      "Iteration: 77,\t Loss: 0.6575607061386108, alpha=0.5243625640869141\n",
      "Iteration: 78,\t Loss: 0.6574108600616455, alpha=0.5241450667381287\n",
      "Iteration: 79,\t Loss: 0.6572620272636414, alpha=0.5239514112472534\n",
      "Iteration: 80,\t Loss: 0.6571138501167297, alpha=0.5239480137825012\n",
      "Iteration: 81,\t Loss: 0.6571111083030701, alpha=0.5239450335502625\n",
      "Iteration: 82,\t Loss: 0.657107949256897, alpha=0.5239424705505371\n",
      "Iteration: 83,\t Loss: 0.6571052074432373, alpha=0.5239403247833252\n",
      "Iteration: 84,\t Loss: 0.657102108001709, alpha=0.5239384770393372\n",
      "Iteration: 85,\t Loss: 0.657099187374115, alpha=0.5239369869232178\n",
      "Iteration: 86,\t Loss: 0.6570963263511658, alpha=0.5239357352256775\n",
      "Iteration: 87,\t Loss: 0.6570932269096375, alpha=0.5239347815513611\n",
      "Iteration: 88,\t Loss: 0.6570902466773987, alpha=0.5239340662956238\n",
      "Iteration: 89,\t Loss: 0.6570873856544495, alpha=0.5239335298538208\n",
      "Iteration: 90,\t Loss: 0.6570842266082764, alpha=0.5239331722259521\n",
      "Iteration: 91,\t Loss: 0.6570812463760376, alpha=0.5239329934120178\n",
      "Iteration: 92,\t Loss: 0.6570783257484436, alpha=0.5239329934120178\n",
      "Iteration: 93,\t Loss: 0.6570752263069153, alpha=0.5239331722259521\n",
      "Iteration: 94,\t Loss: 0.6570719480514526, alpha=0.523933470249176\n",
      "Iteration: 95,\t Loss: 0.6570689678192139, alpha=0.5239338874816895\n",
      "Iteration: 96,\t Loss: 0.6570658683776855, alpha=0.5239344239234924\n",
      "Iteration: 97,\t Loss: 0.657062828540802, alpha=0.523935079574585\n",
      "Iteration: 98,\t Loss: 0.6570596694946289, alpha=0.5239357948303223\n",
      "Iteration: 99,\t Loss: 0.6570565700531006, alpha=0.5239366292953491\n",
      "Iteration: 100,\t Loss: 0.6570534706115723, alpha=0.5239366292953491\n",
      "Iteration: 101,\t Loss: 0.6570535898208618, alpha=0.5239366292953491\n",
      "Iteration: 102,\t Loss: 0.6570534706115723, alpha=0.5239366292953491\n",
      "\t Convergence reached after 103 iterations with alpha=0.5239366292953491\n",
      "Iteration: 0,\t Loss: 8.482122421264648, alpha=1.090417742729187\n",
      "Iteration: 1,\t Loss: 1.0321564674377441, alpha=1.010617733001709\n",
      "Iteration: 2,\t Loss: 0.9269925355911255, alpha=0.9210958480834961\n",
      "Iteration: 3,\t Loss: 0.8628417253494263, alpha=0.8273670077323914\n",
      "Iteration: 4,\t Loss: 0.8157066106796265, alpha=0.7304410934448242\n",
      "Iteration: 5,\t Loss: 0.778877317905426, alpha=0.631443977355957\n",
      "Iteration: 6,\t Loss: 0.7490230798721313, alpha=0.5332393646240234\n",
      "Iteration: 7,\t Loss: 0.7242047190666199, alpha=0.4433736503124237\n",
      "Iteration: 8,\t Loss: 0.7031841278076172, alpha=0.3765261769294739\n",
      "Iteration: 9,\t Loss: 0.6851193308830261, alpha=0.34475356340408325\n",
      "Iteration: 10,\t Loss: 0.6694103479385376, alpha=0.34543576836586\n",
      "Iteration: 11,\t Loss: 0.6556154489517212, alpha=0.36953797936439514\n",
      "Iteration: 12,\t Loss: 0.6434004902839661, alpha=0.40915077924728394\n",
      "Iteration: 13,\t Loss: 0.6325085163116455, alpha=0.4579288065433502\n",
      "Iteration: 14,\t Loss: 0.6227359771728516, alpha=0.5096556544303894\n",
      "Iteration: 15,\t Loss: 0.6139197945594788, alpha=0.5569040179252625\n",
      "Iteration: 16,\t Loss: 0.6059276461601257, alpha=0.5913028717041016\n",
      "Iteration: 17,\t Loss: 0.598648726940155, alpha=0.6068390607833862\n",
      "Iteration: 18,\t Loss: 0.591991126537323, alpha=0.602906346321106\n",
      "Iteration: 19,\t Loss: 0.5858777761459351, alpha=0.5831252932548523\n",
      "Iteration: 20,\t Loss: 0.5802436470985413, alpha=0.5529041290283203\n",
      "Iteration: 21,\t Loss: 0.5750343203544617, alpha=0.5184913873672485\n",
      "Iteration: 22,\t Loss: 0.5702027082443237, alpha=0.48679837584495544\n",
      "Iteration: 23,\t Loss: 0.5657084584236145, alpha=0.46445751190185547\n",
      "Iteration: 24,\t Loss: 0.5615153312683105, alpha=0.4557356834411621\n",
      "Iteration: 25,\t Loss: 0.5575915575027466, alpha=0.4611625075340271\n",
      "Iteration: 26,\t Loss: 0.5539098381996155, alpha=0.4781153202056885\n",
      "Iteration: 27,\t Loss: 0.5504468679428101, alpha=0.5020370483398438\n",
      "Iteration: 28,\t Loss: 0.5471814870834351, alpha=0.5271942019462585\n",
      "Iteration: 29,\t Loss: 0.5440956950187683, alpha=0.5474487543106079\n",
      "Iteration: 30,\t Loss: 0.541172981262207, alpha=0.5476564168930054\n",
      "Iteration: 31,\t Loss: 0.541117250919342, alpha=0.547685980796814\n",
      "Iteration: 32,\t Loss: 0.5410629510879517, alpha=0.5475538969039917\n",
      "Iteration: 33,\t Loss: 0.5410103797912598, alpha=0.5472759008407593\n",
      "Iteration: 34,\t Loss: 0.5409588813781738, alpha=0.5468669533729553\n",
      "Iteration: 35,\t Loss: 0.540908694267273, alpha=0.5463410019874573\n",
      "Iteration: 36,\t Loss: 0.5408596992492676, alpha=0.54571133852005\n",
      "Iteration: 37,\t Loss: 0.5408115386962891, alpha=0.5449904203414917\n",
      "Iteration: 38,\t Loss: 0.5407641530036926, alpha=0.544189989566803\n",
      "Iteration: 39,\t Loss: 0.540717601776123, alpha=0.5433208346366882\n",
      "Iteration: 40,\t Loss: 0.5406714677810669, alpha=0.542393147945404\n",
      "Iteration: 41,\t Loss: 0.5406261682510376, alpha=0.5414165258407593\n",
      "Iteration: 42,\t Loss: 0.540581226348877, alpha=0.5403997898101807\n",
      "Iteration: 43,\t Loss: 0.540536642074585, alpha=0.5393511652946472\n",
      "Iteration: 44,\t Loss: 0.5404926538467407, alpha=0.5382784008979797\n",
      "Iteration: 45,\t Loss: 0.5404486060142517, alpha=0.537188708782196\n",
      "Iteration: 46,\t Loss: 0.5404050350189209, alpha=0.5360886454582214\n",
      "Iteration: 47,\t Loss: 0.5403615236282349, alpha=0.5349843502044678\n",
      "Iteration: 48,\t Loss: 0.5403183698654175, alpha=0.5338814854621887\n",
      "Iteration: 49,\t Loss: 0.5402752161026001, alpha=0.5327853560447693\n",
      "Iteration: 50,\t Loss: 0.5402319431304932, alpha=0.5317007899284363\n",
      "Iteration: 51,\t Loss: 0.5401889681816101, alpha=0.5306321978569031\n",
      "Iteration: 52,\t Loss: 0.5401461720466614, alpha=0.5295835733413696\n",
      "Iteration: 53,\t Loss: 0.5401029586791992, alpha=0.5285585522651672\n",
      "Iteration: 54,\t Loss: 0.5400598645210266, alpha=0.5275604128837585\n",
      "Iteration: 55,\t Loss: 0.5400168895721436, alpha=0.5265921354293823\n",
      "Iteration: 56,\t Loss: 0.5399736762046814, alpha=0.5256562232971191\n",
      "Iteration: 57,\t Loss: 0.5399304628372192, alpha=0.5247550010681152\n",
      "Iteration: 58,\t Loss: 0.5398871898651123, alpha=0.5238903164863586\n",
      "Iteration: 59,\t Loss: 0.5398436784744263, alpha=0.5230638980865479\n",
      "Iteration: 60,\t Loss: 0.5397998690605164, alpha=0.5222769975662231\n",
      "Iteration: 61,\t Loss: 0.5397562980651855, alpha=0.5215307474136353\n",
      "Iteration: 62,\t Loss: 0.5397123098373413, alpha=0.5208258628845215\n",
      "Iteration: 63,\t Loss: 0.5396682024002075, alpha=0.5201628804206848\n",
      "Iteration: 64,\t Loss: 0.5396242141723633, alpha=0.5195420384407043\n",
      "Iteration: 65,\t Loss: 0.5395796895027161, alpha=0.5189634561538696\n",
      "Iteration: 66,\t Loss: 0.5395349860191345, alpha=0.5184268951416016\n",
      "Iteration: 67,\t Loss: 0.5394904613494873, alpha=0.5179319381713867\n",
      "Iteration: 68,\t Loss: 0.5394452810287476, alpha=0.5174780488014221\n",
      "Iteration: 69,\t Loss: 0.5394002199172974, alpha=0.5170644521713257\n",
      "Iteration: 70,\t Loss: 0.5393550992012024, alpha=0.516690194606781\n",
      "Iteration: 71,\t Loss: 0.5393093824386597, alpha=0.5163542628288269\n",
      "Iteration: 72,\t Loss: 0.5392636656761169, alpha=0.5160554051399231\n",
      "Iteration: 73,\t Loss: 0.5392175912857056, alpha=0.5157923102378845\n",
      "Iteration: 74,\t Loss: 0.5391716361045837, alpha=0.5155635476112366\n",
      "Iteration: 75,\t Loss: 0.5391250848770142, alpha=0.5153676271438599\n",
      "Iteration: 76,\t Loss: 0.5390787124633789, alpha=0.5152028799057007\n",
      "Iteration: 77,\t Loss: 0.5390317440032959, alpha=0.5150677561759949\n",
      "Iteration: 78,\t Loss: 0.5389847755432129, alpha=0.5149605870246887\n",
      "Iteration: 79,\t Loss: 0.5389376282691956, alpha=0.5148795247077942\n",
      "Iteration: 80,\t Loss: 0.5388901233673096, alpha=0.5148783326148987\n",
      "Iteration: 81,\t Loss: 0.5388892292976379, alpha=0.5148776173591614\n",
      "Iteration: 82,\t Loss: 0.5388882756233215, alpha=0.5148773193359375\n",
      "Iteration: 83,\t Loss: 0.5388873219490051, alpha=0.5148773789405823\n",
      "Iteration: 84,\t Loss: 0.538886308670044, alpha=0.5148777365684509\n",
      "Iteration: 85,\t Loss: 0.5388855338096619, alpha=0.5148784518241882\n",
      "Iteration: 86,\t Loss: 0.5388844609260559, alpha=0.5148794054985046\n",
      "Iteration: 87,\t Loss: 0.5388834476470947, alpha=0.5148806571960449\n",
      "Iteration: 88,\t Loss: 0.538882315158844, alpha=0.5148821473121643\n",
      "Iteration: 89,\t Loss: 0.5388814210891724, alpha=0.514883816242218\n",
      "Iteration: 90,\t Loss: 0.5388804078102112, alpha=0.5148857235908508\n",
      "Iteration: 91,\t Loss: 0.5388795137405396, alpha=0.514887809753418\n",
      "Iteration: 92,\t Loss: 0.5388785600662231, alpha=0.5148900151252747\n",
      "Iteration: 93,\t Loss: 0.5388774275779724, alpha=0.5148923993110657\n",
      "Iteration: 94,\t Loss: 0.5388764142990112, alpha=0.5148949027061462\n",
      "Iteration: 95,\t Loss: 0.5388754606246948, alpha=0.5148975253105164\n",
      "Iteration: 96,\t Loss: 0.5388744473457336, alpha=0.514900267124176\n",
      "Iteration: 97,\t Loss: 0.5388732552528381, alpha=0.5149031281471252\n",
      "Iteration: 98,\t Loss: 0.5388721823692322, alpha=0.514906108379364\n",
      "Iteration: 99,\t Loss: 0.5388714075088501, alpha=0.5149091482162476\n",
      "Iteration: 100,\t Loss: 0.5388703346252441, alpha=0.5149091482162476\n",
      "Iteration: 101,\t Loss: 0.5388702750205994, alpha=0.5149091482162476\n",
      "Iteration: 102,\t Loss: 0.5388701558113098, alpha=0.5149091482162476\n",
      "\t Convergence reached after 103 iterations with alpha=0.5149091482162476\n",
      "Iteration: 0,\t Loss: 0.5355768203735352, alpha=0.30414798855781555\n",
      "Iteration: 1,\t Loss: 0.4972577691078186, alpha=0.37685227394104004\n",
      "Iteration: 2,\t Loss: 0.47569528222084045, alpha=0.45240670442581177\n",
      "Iteration: 3,\t Loss: 0.47406917810440063, alpha=0.5341426134109497\n",
      "Iteration: 4,\t Loss: 0.47892189025878906, alpha=0.5386860370635986\n",
      "Iteration: 5,\t Loss: 0.46239927411079407, alpha=0.504984974861145\n",
      "Iteration: 6,\t Loss: 0.4527917504310608, alpha=0.478887140750885\n",
      "Iteration: 7,\t Loss: 0.44881781935691833, alpha=0.4900817573070526\n",
      "Iteration: 8,\t Loss: 0.44668182730674744, alpha=0.5203773975372314\n",
      "Iteration: 9,\t Loss: 0.4444470703601837, alpha=0.5427592992782593\n",
      "Iteration: 10,\t Loss: 0.44136908650398254, alpha=0.534507155418396\n",
      "Iteration: 11,\t Loss: 0.4373376965522766, alpha=0.5124974846839905\n",
      "Iteration: 12,\t Loss: 0.4325951039791107, alpha=0.5096948146820068\n",
      "Iteration: 13,\t Loss: 0.4275725781917572, alpha=0.5311931371688843\n",
      "Iteration: 14,\t Loss: 0.42280009388923645, alpha=0.5482349395751953\n",
      "Iteration: 15,\t Loss: 0.4188278913497925, alpha=0.5330201387405396\n",
      "Iteration: 16,\t Loss: 0.4160653054714203, alpha=0.5230122804641724\n",
      "Iteration: 17,\t Loss: 0.4144720137119293, alpha=0.544544517993927\n",
      "Iteration: 18,\t Loss: 0.4132121801376343, alpha=0.5513749718666077\n",
      "Iteration: 19,\t Loss: 0.4111660122871399, alpha=0.5339791774749756\n",
      "Iteration: 20,\t Loss: 0.4082246422767639, alpha=0.5414650440216064\n",
      "Iteration: 21,\t Loss: 0.40525323152542114, alpha=0.5580862760543823\n",
      "Iteration: 22,\t Loss: 0.4029054343700409, alpha=0.5518431067466736\n",
      "Iteration: 23,\t Loss: 0.40122494101524353, alpha=0.5425488352775574\n",
      "Iteration: 24,\t Loss: 0.399942547082901, alpha=0.5519519448280334\n",
      "Iteration: 25,\t Loss: 0.39875462651252747, alpha=0.5633477568626404\n",
      "Iteration: 26,\t Loss: 0.39746180176734924, alpha=0.5564502477645874\n",
      "Iteration: 27,\t Loss: 0.3959885835647583, alpha=0.5498778223991394\n",
      "Iteration: 28,\t Loss: 0.39437612891197205, alpha=0.560476541519165\n",
      "Iteration: 29,\t Loss: 0.39273884892463684, alpha=0.5656932592391968\n",
      "Iteration: 30,\t Loss: 0.3912241458892822, alpha=0.5654763579368591\n",
      "Iteration: 31,\t Loss: 0.39119595289230347, alpha=0.5649934411048889\n",
      "Iteration: 32,\t Loss: 0.39116764068603516, alpha=0.5643005967140198\n",
      "Iteration: 33,\t Loss: 0.39113906025886536, alpha=0.5634525418281555\n",
      "Iteration: 34,\t Loss: 0.3911105692386627, alpha=0.5625024437904358\n",
      "Iteration: 35,\t Loss: 0.39108186960220337, alpha=0.5615017414093018\n",
      "Iteration: 36,\t Loss: 0.3910531997680664, alpha=0.56050044298172\n",
      "Iteration: 37,\t Loss: 0.3910245895385742, alpha=0.5595458149909973\n",
      "Iteration: 38,\t Loss: 0.39099591970443726, alpha=0.5586811304092407\n",
      "Iteration: 39,\t Loss: 0.3909671902656555, alpha=0.5579431653022766\n",
      "Iteration: 40,\t Loss: 0.39093872904777527, alpha=0.5573598742485046\n",
      "Iteration: 41,\t Loss: 0.39091023802757263, alpha=0.5569486021995544\n",
      "Iteration: 42,\t Loss: 0.39088180661201477, alpha=0.5567154884338379\n",
      "Iteration: 43,\t Loss: 0.3908534049987793, alpha=0.5566561222076416\n",
      "Iteration: 44,\t Loss: 0.39082518219947815, alpha=0.556756854057312\n",
      "Iteration: 45,\t Loss: 0.3907969295978546, alpha=0.5569970011711121\n",
      "Iteration: 46,\t Loss: 0.39076876640319824, alpha=0.5573506355285645\n",
      "Iteration: 47,\t Loss: 0.3907407522201538, alpha=0.5577885508537292\n",
      "Iteration: 48,\t Loss: 0.39071282744407654, alpha=0.5582796335220337\n",
      "Iteration: 49,\t Loss: 0.39068490266799927, alpha=0.5587924122810364\n",
      "Iteration: 50,\t Loss: 0.3906572163105011, alpha=0.5592964291572571\n",
      "Iteration: 51,\t Loss: 0.39062952995300293, alpha=0.5597636103630066\n",
      "Iteration: 52,\t Loss: 0.3906019926071167, alpha=0.5601696372032166\n",
      "Iteration: 53,\t Loss: 0.3905746638774872, alpha=0.560495138168335\n",
      "Iteration: 54,\t Loss: 0.3905472457408905, alpha=0.5607268214225769\n",
      "Iteration: 55,\t Loss: 0.390519917011261, alpha=0.5608577728271484\n",
      "Iteration: 56,\t Loss: 0.3904927968978882, alpha=0.5608876347541809\n",
      "Iteration: 57,\t Loss: 0.3904656767845154, alpha=0.560822069644928\n",
      "Iteration: 58,\t Loss: 0.39043858647346497, alpha=0.5606719851493835\n",
      "Iteration: 59,\t Loss: 0.3904115855693817, alpha=0.5604522824287415\n",
      "Iteration: 60,\t Loss: 0.39038461446762085, alpha=0.5601810216903687\n",
      "Iteration: 61,\t Loss: 0.39035770297050476, alpha=0.5598777532577515\n",
      "Iteration: 62,\t Loss: 0.39033085107803345, alpha=0.5595625638961792\n",
      "Iteration: 63,\t Loss: 0.3903040587902069, alpha=0.5592547059059143\n",
      "Iteration: 64,\t Loss: 0.39027726650238037, alpha=0.5589715242385864\n",
      "Iteration: 65,\t Loss: 0.390250563621521, alpha=0.558727502822876\n",
      "Iteration: 66,\t Loss: 0.39022380113601685, alpha=0.5585334897041321\n",
      "Iteration: 67,\t Loss: 0.3901970386505127, alpha=0.5583963394165039\n",
      "Iteration: 68,\t Loss: 0.3901704251766205, alpha=0.5583186149597168\n",
      "Iteration: 69,\t Loss: 0.3901437222957611, alpha=0.5582987666130066\n",
      "Iteration: 70,\t Loss: 0.39011698961257935, alpha=0.5583316087722778\n",
      "Iteration: 71,\t Loss: 0.39009028673171997, alpha=0.558408796787262\n",
      "Iteration: 72,\t Loss: 0.390063613653183, alpha=0.558519721031189\n",
      "Iteration: 73,\t Loss: 0.3900369107723236, alpha=0.5586524605751038\n",
      "Iteration: 74,\t Loss: 0.39001017808914185, alpha=0.5587944388389587\n",
      "Iteration: 75,\t Loss: 0.3899833559989929, alpha=0.558933436870575\n",
      "Iteration: 76,\t Loss: 0.38995665311813354, alpha=0.5590584874153137\n",
      "Iteration: 77,\t Loss: 0.3899299204349518, alpha=0.5591604113578796\n",
      "Iteration: 78,\t Loss: 0.38990306854248047, alpha=0.5592324137687683\n",
      "Iteration: 79,\t Loss: 0.38987624645233154, alpha=0.5592703223228455\n",
      "Iteration: 80,\t Loss: 0.3898494243621826, alpha=0.5592703819274902\n",
      "Iteration: 81,\t Loss: 0.38984885811805725, alpha=0.5592697858810425\n",
      "Iteration: 82,\t Loss: 0.38984835147857666, alpha=0.559268593788147\n",
      "Iteration: 83,\t Loss: 0.3898477554321289, alpha=0.5592668652534485\n",
      "Iteration: 84,\t Loss: 0.38984715938568115, alpha=0.5592646598815918\n",
      "Iteration: 85,\t Loss: 0.38984668254852295, alpha=0.5592619776725769\n",
      "Iteration: 86,\t Loss: 0.38984614610671997, alpha=0.5592589378356934\n",
      "Iteration: 87,\t Loss: 0.3898455500602722, alpha=0.5592555403709412\n",
      "Iteration: 88,\t Loss: 0.3898450434207916, alpha=0.5592517852783203\n",
      "Iteration: 89,\t Loss: 0.38984453678131104, alpha=0.5592477321624756\n",
      "Iteration: 90,\t Loss: 0.38984400033950806, alpha=0.5592434406280518\n",
      "Iteration: 91,\t Loss: 0.3898434042930603, alpha=0.5592389106750488\n",
      "Iteration: 92,\t Loss: 0.3898429274559021, alpha=0.5592341423034668\n",
      "Iteration: 93,\t Loss: 0.38984233140945435, alpha=0.5592291951179504\n",
      "Iteration: 94,\t Loss: 0.38984179496765137, alpha=0.5592241287231445\n",
      "Iteration: 95,\t Loss: 0.389841228723526, alpha=0.5592188835144043\n",
      "Iteration: 96,\t Loss: 0.3898407518863678, alpha=0.5592135190963745\n",
      "Iteration: 97,\t Loss: 0.38984018564224243, alpha=0.5592080354690552\n",
      "Iteration: 98,\t Loss: 0.3898395895957947, alpha=0.5592024922370911\n",
      "Iteration: 99,\t Loss: 0.3898390531539917, alpha=0.5591968297958374\n",
      "Iteration: 100,\t Loss: 0.3898385167121887, alpha=0.5591966509819031\n",
      "Iteration: 101,\t Loss: 0.38983842730522156, alpha=0.5591964721679688\n",
      "\t Convergence reached after 102 iterations with alpha=0.5591962933540344\n",
      "Iteration: 0,\t Loss: 12.027557373046875, alpha=0.4363974332809448\n",
      "Iteration: 1,\t Loss: 7.837634086608887, alpha=0.5086160898208618\n",
      "Iteration: 2,\t Loss: 4.394124984741211, alpha=0.5345547795295715\n",
      "Iteration: 3,\t Loss: 1.6206326484680176, alpha=0.4786366820335388\n",
      "Iteration: 4,\t Loss: 1.0187997817993164, alpha=0.43539777398109436\n",
      "Iteration: 5,\t Loss: 1.0751045942306519, alpha=0.4773877263069153\n",
      "Iteration: 6,\t Loss: 128.5849609375, alpha=0.5262221693992615\n",
      "Iteration: 7,\t Loss: 1.205409288406372, alpha=0.5679517388343811\n",
      "Iteration: 8,\t Loss: 1.006415605545044, alpha=0.6044235825538635\n",
      "Iteration: 9,\t Loss: 1.004518985748291, alpha=0.6363863945007324\n",
      "Iteration: 10,\t Loss: 1.0131630897521973, alpha=0.6644247174263\n",
      "Iteration: 11,\t Loss: 1.0261589288711548, alpha=0.6889965534210205\n",
      "Iteration: 12,\t Loss: 1.0430994033813477, alpha=0.7104250192642212\n",
      "Iteration: 13,\t Loss: 1.105897068977356, alpha=0.7213622331619263\n",
      "Iteration: 14,\t Loss: 1.587528944015503, alpha=0.7225831747055054\n",
      "Iteration: 15,\t Loss: 2.0331385135650635, alpha=0.7151184678077698\n",
      "Iteration: 16,\t Loss: 2.439887046813965, alpha=0.700185239315033\n",
      "Iteration: 17,\t Loss: 2.807178258895874, alpha=0.6791302561759949\n",
      "Iteration: 18,\t Loss: 3.135626792907715, alpha=0.6533911824226379\n",
      "Iteration: 19,\t Loss: 3.4266486167907715, alpha=0.62447190284729\n",
      "Iteration: 20,\t Loss: 3.682098150253296, alpha=0.5939252376556396\n",
      "Iteration: 21,\t Loss: 3.904109477996826, alpha=0.5633341670036316\n",
      "Iteration: 22,\t Loss: 4.095005035400391, alpha=0.5342811346054077\n",
      "Iteration: 23,\t Loss: 4.257162094116211, alpha=0.5082968473434448\n",
      "Iteration: 24,\t Loss: 4.392982482910156, alpha=0.486784428358078\n",
      "Iteration: 25,\t Loss: 4.504787445068359, alpha=0.4709226191043854\n",
      "Iteration: 26,\t Loss: 4.594809055328369, alpha=0.46156105399131775\n",
      "Iteration: 27,\t Loss: 4.665229797363281, alpha=0.4591275453567505\n",
      "Iteration: 28,\t Loss: 4.718026161193848, alpha=0.4635653495788574\n",
      "Iteration: 29,\t Loss: 4.755069255828857, alpha=0.4743075966835022\n",
      "Iteration: 30,\t Loss: 4.77810525894165, alpha=0.4746270775794983\n",
      "Iteration: 31,\t Loss: 4.778287887573242, alpha=0.4750426113605499\n",
      "Iteration: 32,\t Loss: 4.778184413909912, alpha=0.4755452871322632\n",
      "Iteration: 33,\t Loss: 4.77781343460083, alpha=0.47612684965133667\n",
      "Iteration: 34,\t Loss: 4.777211666107178, alpha=0.4767796993255615\n",
      "Iteration: 35,\t Loss: 4.77639102935791, alpha=0.47749680280685425\n",
      "Iteration: 36,\t Loss: 4.775381565093994, alpha=0.4782717227935791\n",
      "Iteration: 37,\t Loss: 4.774183750152588, alpha=0.4790984094142914\n",
      "Iteration: 38,\t Loss: 4.772815704345703, alpha=0.47997134923934937\n",
      "Iteration: 39,\t Loss: 4.771307945251465, alpha=0.48088544607162476\n",
      "Iteration: 40,\t Loss: 4.769654273986816, alpha=0.4818359315395355\n",
      "Iteration: 41,\t Loss: 4.767885208129883, alpha=0.48281845450401306\n",
      "Iteration: 42,\t Loss: 4.766000747680664, alpha=0.48382896184921265\n",
      "Iteration: 43,\t Loss: 4.764018535614014, alpha=0.48486366868019104\n",
      "Iteration: 44,\t Loss: 4.761920928955078, alpha=0.48591914772987366\n",
      "Iteration: 45,\t Loss: 4.759762763977051, alpha=0.48699215054512024\n",
      "Iteration: 46,\t Loss: 4.75750732421875, alpha=0.4880797266960144\n",
      "Iteration: 47,\t Loss: 4.7552032470703125, alpha=0.4891791045665741\n",
      "Iteration: 48,\t Loss: 4.752820014953613, alpha=0.490287721157074\n",
      "Iteration: 49,\t Loss: 4.750375747680664, alpha=0.4914032518863678\n",
      "Iteration: 50,\t Loss: 4.747883319854736, alpha=0.4925234615802765\n",
      "Iteration: 51,\t Loss: 4.745335578918457, alpha=0.4936462938785553\n",
      "Iteration: 52,\t Loss: 4.742745876312256, alpha=0.4947699010372162\n",
      "Iteration: 53,\t Loss: 4.740113258361816, alpha=0.49589255452156067\n",
      "Iteration: 54,\t Loss: 4.737438201904297, alpha=0.4970126152038574\n",
      "Iteration: 55,\t Loss: 4.734732627868652, alpha=0.4981285631656647\n",
      "Iteration: 56,\t Loss: 4.732003211975098, alpha=0.4992390275001526\n",
      "Iteration: 57,\t Loss: 4.72924280166626, alpha=0.5003427863121033\n",
      "Iteration: 58,\t Loss: 4.72646427154541, alpha=0.501438558101654\n",
      "Iteration: 59,\t Loss: 4.723649501800537, alpha=0.5025253295898438\n",
      "Iteration: 60,\t Loss: 4.720816612243652, alpha=0.5036020278930664\n",
      "Iteration: 61,\t Loss: 4.717965126037598, alpha=0.5046677589416504\n",
      "Iteration: 62,\t Loss: 4.715095520019531, alpha=0.5057216286659241\n",
      "Iteration: 63,\t Loss: 4.712219715118408, alpha=0.5067628622055054\n",
      "Iteration: 64,\t Loss: 4.709313869476318, alpha=0.5077908039093018\n",
      "Iteration: 65,\t Loss: 4.706395626068115, alpha=0.5088047385215759\n",
      "Iteration: 66,\t Loss: 4.703477382659912, alpha=0.5098040699958801\n",
      "Iteration: 67,\t Loss: 4.700540542602539, alpha=0.5107882618904114\n",
      "Iteration: 68,\t Loss: 4.697598457336426, alpha=0.5117567777633667\n",
      "Iteration: 69,\t Loss: 4.694643497467041, alpha=0.5127092003822327\n",
      "Iteration: 70,\t Loss: 4.691664695739746, alpha=0.5136451721191406\n",
      "Iteration: 71,\t Loss: 4.688685417175293, alpha=0.5145642161369324\n",
      "Iteration: 72,\t Loss: 4.6857123374938965, alpha=0.5154660940170288\n",
      "Iteration: 73,\t Loss: 4.682727336883545, alpha=0.516350507736206\n",
      "Iteration: 74,\t Loss: 4.67972993850708, alpha=0.517217218875885\n",
      "Iteration: 75,\t Loss: 4.676738739013672, alpha=0.5180659890174866\n",
      "Iteration: 76,\t Loss: 4.673723220825195, alpha=0.5188966989517212\n",
      "Iteration: 77,\t Loss: 4.6707258224487305, alpha=0.5197091102600098\n",
      "Iteration: 78,\t Loss: 4.667698383331299, alpha=0.5205031633377075\n",
      "Iteration: 79,\t Loss: 4.664689064025879, alpha=0.5212787985801697\n",
      "Iteration: 80,\t Loss: 4.661667346954346, alpha=0.5212939381599426\n",
      "Iteration: 81,\t Loss: 4.661612510681152, alpha=0.5213087201118469\n",
      "Iteration: 82,\t Loss: 4.661545753479004, alpha=0.5213232040405273\n",
      "Iteration: 83,\t Loss: 4.661484718322754, alpha=0.5213374495506287\n",
      "Iteration: 84,\t Loss: 4.6614179611206055, alpha=0.5213514566421509\n",
      "Iteration: 85,\t Loss: 4.66136360168457, alpha=0.5213652849197388\n",
      "Iteration: 86,\t Loss: 4.661308765411377, alpha=0.5213789343833923\n",
      "Iteration: 87,\t Loss: 4.66124153137207, alpha=0.5213924050331116\n",
      "Iteration: 88,\t Loss: 4.661174774169922, alpha=0.5214057564735413\n",
      "Iteration: 89,\t Loss: 4.66111421585083, alpha=0.5214189887046814\n",
      "Iteration: 90,\t Loss: 4.66105318069458, alpha=0.521432101726532\n",
      "Iteration: 91,\t Loss: 4.660998344421387, alpha=0.521445095539093\n",
      "Iteration: 92,\t Loss: 4.660919666290283, alpha=0.5214580297470093\n",
      "Iteration: 93,\t Loss: 4.660877227783203, alpha=0.5214709043502808\n",
      "Iteration: 94,\t Loss: 4.66080379486084, alpha=0.5214837193489075\n",
      "Iteration: 95,\t Loss: 4.660743236541748, alpha=0.5214965343475342\n",
      "Iteration: 96,\t Loss: 4.660682678222656, alpha=0.5215092897415161\n",
      "Iteration: 97,\t Loss: 4.660621643066406, alpha=0.521522045135498\n",
      "Iteration: 98,\t Loss: 4.660548686981201, alpha=0.5215347409248352\n",
      "Iteration: 99,\t Loss: 4.660493850708008, alpha=0.5215474367141724\n",
      "Iteration: 100,\t Loss: 4.660427093505859, alpha=0.5215476751327515\n",
      "\t Convergence reached after 101 iterations with alpha=0.5215479135513306\n",
      "\u001b[32mOptimizing PBkl for concatenated classifier-------------------------------\u001b[0m\n",
      "Iteration: 0,\t Loss: 0.7097437977790833, alpha=0.7117785215377808\n",
      "Iteration: 1,\t Loss: 0.6464622020721436, alpha=0.6341020464897156\n",
      "Iteration: 2,\t Loss: 0.5882985591888428, alpha=0.550550639629364\n",
      "Iteration: 3,\t Loss: 0.5402116775512695, alpha=0.4601144790649414\n",
      "Iteration: 4,\t Loss: 0.50077885389328, alpha=0.4521043002605438\n",
      "Iteration: 5,\t Loss: 0.46943947672843933, alpha=0.4926954507827759\n",
      "Iteration: 6,\t Loss: 0.44704490900039673, alpha=0.546394407749176\n",
      "Iteration: 7,\t Loss: 0.4383489787578583, alpha=0.5247761011123657\n",
      "Iteration: 8,\t Loss: 0.4597906470298767, alpha=0.48428425192832947\n",
      "Iteration: 9,\t Loss: 0.46242544054985046, alpha=0.5138838887214661\n",
      "Iteration: 10,\t Loss: 0.4340074360370636, alpha=0.5485562682151794\n",
      "Iteration: 11,\t Loss: 0.41691434383392334, alpha=0.5568621158599854\n",
      "Iteration: 12,\t Loss: 0.4087471663951874, alpha=0.5433157086372375\n",
      "Iteration: 13,\t Loss: 0.4048372209072113, alpha=0.5233891010284424\n",
      "Iteration: 14,\t Loss: 0.4027775228023529, alpha=0.512824296951294\n",
      "Iteration: 15,\t Loss: 0.4013719856739044, alpha=0.5189584493637085\n",
      "Iteration: 16,\t Loss: 0.4000014066696167, alpha=0.5374031662940979\n",
      "Iteration: 17,\t Loss: 0.3983469605445862, alpha=0.5575900673866272\n",
      "Iteration: 18,\t Loss: 0.3962585926055908, alpha=0.5669963359832764\n",
      "Iteration: 19,\t Loss: 0.39368635416030884, alpha=0.5610531568527222\n",
      "Iteration: 20,\t Loss: 0.39064648747444153, alpha=0.5469770431518555\n",
      "Iteration: 21,\t Loss: 0.38720521330833435, alpha=0.5380350351333618\n",
      "Iteration: 22,\t Loss: 0.38345855474472046, alpha=0.543282151222229\n",
      "Iteration: 23,\t Loss: 0.37952157855033875, alpha=0.5585662722587585\n",
      "Iteration: 24,\t Loss: 0.37552663683891296, alpha=0.5696635842323303\n",
      "Iteration: 25,\t Loss: 0.3716144561767578, alpha=0.5648850202560425\n",
      "Iteration: 26,\t Loss: 0.36792483925819397, alpha=0.551769495010376\n",
      "Iteration: 27,\t Loss: 0.3645961880683899, alpha=0.5494410991668701\n",
      "Iteration: 28,\t Loss: 0.3617447018623352, alpha=0.5618836283683777\n",
      "Iteration: 29,\t Loss: 0.35943931341171265, alpha=0.568888247013092\n",
      "Iteration: 30,\t Loss: 0.3576664328575134, alpha=0.5686585903167725\n",
      "Iteration: 31,\t Loss: 0.3576343357563019, alpha=0.5681247711181641\n",
      "Iteration: 32,\t Loss: 0.35759982466697693, alpha=0.567347526550293\n",
      "Iteration: 33,\t Loss: 0.35756319761276245, alpha=0.5663824081420898\n",
      "Iteration: 34,\t Loss: 0.3575245141983032, alpha=0.5652803778648376\n",
      "Iteration: 35,\t Loss: 0.35748395323753357, alpha=0.564088761806488\n",
      "Iteration: 36,\t Loss: 0.35744160413742065, alpha=0.5628522634506226\n",
      "Iteration: 37,\t Loss: 0.3573976159095764, alpha=0.5616133809089661\n",
      "Iteration: 38,\t Loss: 0.35735222697257996, alpha=0.5604122281074524\n",
      "Iteration: 39,\t Loss: 0.3573054075241089, alpha=0.5592859983444214\n",
      "Iteration: 40,\t Loss: 0.3572573661804199, alpha=0.5582678914070129\n",
      "Iteration: 41,\t Loss: 0.3572080135345459, alpha=0.5573855042457581\n",
      "Iteration: 42,\t Loss: 0.35715779662132263, alpha=0.5566598176956177\n",
      "Iteration: 43,\t Loss: 0.35710644721984863, alpha=0.5561039447784424\n",
      "Iteration: 44,\t Loss: 0.35705435276031494, alpha=0.5557230114936829\n",
      "Iteration: 45,\t Loss: 0.35700151324272156, alpha=0.5555144548416138\n",
      "Iteration: 46,\t Loss: 0.35694795846939087, alpha=0.5554692149162292\n",
      "Iteration: 47,\t Loss: 0.3568936288356781, alpha=0.5555724501609802\n",
      "Iteration: 48,\t Loss: 0.3568388819694519, alpha=0.5558052659034729\n",
      "Iteration: 49,\t Loss: 0.35678374767303467, alpha=0.5561457276344299\n",
      "Iteration: 50,\t Loss: 0.35672807693481445, alpha=0.5565700531005859\n",
      "Iteration: 51,\t Loss: 0.3566721975803375, alpha=0.5570535063743591\n",
      "Iteration: 52,\t Loss: 0.35661590099334717, alpha=0.557571291923523\n",
      "Iteration: 53,\t Loss: 0.3565594255924225, alpha=0.5580992102622986\n",
      "Iteration: 54,\t Loss: 0.35650280117988586, alpha=0.5586145520210266\n",
      "Iteration: 55,\t Loss: 0.3564460277557373, alpha=0.5590965747833252\n",
      "Iteration: 56,\t Loss: 0.3563891351222992, alpha=0.5595273375511169\n",
      "Iteration: 57,\t Loss: 0.3563322126865387, alpha=0.5598921775817871\n",
      "Iteration: 58,\t Loss: 0.35627517104148865, alpha=0.5601801872253418\n",
      "Iteration: 59,\t Loss: 0.356218159198761, alpha=0.5603843927383423\n",
      "Iteration: 60,\t Loss: 0.3561611473560333, alpha=0.5605019927024841\n",
      "Iteration: 61,\t Loss: 0.35610416531562805, alpha=0.5605337619781494\n",
      "Iteration: 62,\t Loss: 0.35604721307754517, alpha=0.5604842901229858\n",
      "Iteration: 63,\t Loss: 0.35599038004875183, alpha=0.5603610277175903\n",
      "Iteration: 64,\t Loss: 0.3559335470199585, alpha=0.5601739287376404\n",
      "Iteration: 65,\t Loss: 0.3558768033981323, alpha=0.5599347352981567\n",
      "Iteration: 66,\t Loss: 0.3558202087879181, alpha=0.5596564412117004\n",
      "Iteration: 67,\t Loss: 0.35576364398002625, alpha=0.5593526363372803\n",
      "Iteration: 68,\t Loss: 0.3557072877883911, alpha=0.5590368509292603\n",
      "Iteration: 69,\t Loss: 0.35565105080604553, alpha=0.5587220191955566\n",
      "Iteration: 70,\t Loss: 0.3555949032306671, alpha=0.5584198832511902\n",
      "Iteration: 71,\t Loss: 0.35553881525993347, alpha=0.5581408739089966\n",
      "Iteration: 72,\t Loss: 0.35548293590545654, alpha=0.5578933358192444\n",
      "Iteration: 73,\t Loss: 0.3554271459579468, alpha=0.5576837062835693\n",
      "Iteration: 74,\t Loss: 0.35537150502204895, alpha=0.5575160384178162\n",
      "Iteration: 75,\t Loss: 0.35531604290008545, alpha=0.5573922395706177\n",
      "Iteration: 76,\t Loss: 0.3552606403827667, alpha=0.5573120713233948\n",
      "Iteration: 77,\t Loss: 0.35520535707473755, alpha=0.5572733283042908\n",
      "Iteration: 78,\t Loss: 0.3551502525806427, alpha=0.5572719573974609\n",
      "Iteration: 79,\t Loss: 0.3550952970981598, alpha=0.557302713394165\n",
      "Iteration: 80,\t Loss: 0.35504043102264404, alpha=0.5573038458824158\n",
      "Iteration: 81,\t Loss: 0.3550392985343933, alpha=0.5573053956031799\n",
      "Iteration: 82,\t Loss: 0.3550381362438202, alpha=0.5573074221611023\n",
      "Iteration: 83,\t Loss: 0.3550370931625366, alpha=0.5573098063468933\n",
      "Iteration: 84,\t Loss: 0.35503602027893066, alpha=0.557312548160553\n",
      "Iteration: 85,\t Loss: 0.3550349175930023, alpha=0.5573155879974365\n",
      "Iteration: 86,\t Loss: 0.3550337851047516, alpha=0.557318925857544\n",
      "Iteration: 87,\t Loss: 0.35503271222114563, alpha=0.5573225021362305\n",
      "Iteration: 88,\t Loss: 0.3550316393375397, alpha=0.5573263168334961\n",
      "Iteration: 89,\t Loss: 0.35503053665161133, alpha=0.5573303699493408\n",
      "Iteration: 90,\t Loss: 0.3550293743610382, alpha=0.5573346018791199\n",
      "Iteration: 91,\t Loss: 0.355028361082077, alpha=0.5573390126228333\n",
      "Iteration: 92,\t Loss: 0.3550272285938263, alpha=0.557343602180481\n",
      "Iteration: 93,\t Loss: 0.3550260663032532, alpha=0.5573483109474182\n",
      "Iteration: 94,\t Loss: 0.3550249934196472, alpha=0.557353138923645\n",
      "Iteration: 95,\t Loss: 0.35502392053604126, alpha=0.5573580861091614\n",
      "Iteration: 96,\t Loss: 0.3550227880477905, alpha=0.5573630928993225\n",
      "Iteration: 97,\t Loss: 0.35502177476882935, alpha=0.5573682188987732\n",
      "Iteration: 98,\t Loss: 0.35502058267593384, alpha=0.5573734045028687\n",
      "Iteration: 99,\t Loss: 0.3550195097923279, alpha=0.5573786497116089\n",
      "Iteration: 100,\t Loss: 0.35501837730407715, alpha=0.5573787093162537\n",
      "Iteration: 101,\t Loss: 0.35501834750175476, alpha=0.5573787689208984\n",
      "Iteration: 102,\t Loss: 0.35501837730407715, alpha=0.5573788285255432\n",
      "\t Convergence reached after 103 iterations with alpha=0.557378888130188\n",
      "### Multiview classifier gibbs risk after Optim: 0.1510148179827557\n",
      "\u001b[32mOptimization is done! -------------------------------\u001b[0m\n",
      "\u001b[32mComputing the bound values ans risks -------------------------------\u001b[0m\n",
      "right_hand_side=0.002880073656098655, empirical_gibbs_risk=0.1510148179827557\n",
      "\u001b[32mEntering save and stats zone-------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "before_merge = (Xs_train, y_train, Xs_test, y_test)\n",
    "Xs, y = train_test_merge(Xs_train, y_train, Xs_test, y_test)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "\n",
    "def metric_dict(mv_metric, v_metrics):\n",
    "    metric_dict = {f\"View{i+1}\": v_metrics[i] for i in range(len(v_metrics)-1)}\n",
    "    metric_dict.update({\"Concatenated\": v_metrics[-1]})\n",
    "    metric_dict.update({\"Multiview\": mv_metric})\n",
    "    return metric_dict\n",
    "\n",
    "# iterate over the labeled data sizes #\n",
    "for i, s1_size in enumerate(s_labeled_sizes):\n",
    "    print(colored(f\"############ Using {s1_size*100}% labeled data ############\", 'black', on_color='on_blue'))\n",
    "    s_labeled_dir = 'results'+f\"/s_labeled-{int(s1_size*100)}\"\n",
    "    os.makedirs(s_labeled_dir, exist_ok=True)\n",
    "### iterate over the alpha values ###\n",
    "    \n",
    "    for j, alpha in enumerate(ALPHA):\n",
    "        print(colored(f\"\\t############ Using {alpha=} ############\", 'black', on_color='on_blue'))\n",
    "        alpha_dir = s_labeled_dir+ f\"/alpha-{alpha}\"\n",
    "        os.makedirs(alpha_dir, exist_ok=True)\n",
    "        \n",
    "#### iterate over the configurations ####\n",
    "        for k, config in enumerate(CFG):\n",
    "            print(colored(f\"\\t\\t############ Using {config['name']} ############\", 'black', on_color='on_blue'))\n",
    "            for run in RUNS:\n",
    "                print(colored(f\"\\n----------------Run {run+1}---------------\", 'blue'))\n",
    "\n",
    "                # Shuffle and split the dataset into training and testing\n",
    "                # if not dataset.split:\n",
    "                Xs_train, y_train, Xs_test, y_test = train_test_split(Xs, y, test_size=test_size, random_state=run*(i+1)*(j+1)*(k+1))\n",
    "                # else:\n",
    "                # Xs_train, y_train, Xs_test, y_test = before_merge\n",
    "\n",
    "                # Split the dataset into labeled and unlabeled\n",
    "                Xs_train, y_train, UlX, _ = s1_s2_split(Xs_train, y_train, s1_size=s1_size, random_state=run*(i+1)*(j+1)*(k+1))\n",
    "                X_train_concat = np.concatenate(Xs_train, axis=1)\n",
    "                X_test_concat = np.concatenate(Xs_test, axis=1)\n",
    "                    \n",
    "                # instantiate multiview dNDF classifier\n",
    "                dNDF_mv = MultiViewMajorityVoteLearner(nb_estimators=config[\"n_estimators\"],\n",
    "                                                        nb_views=len(Xs_train),\n",
    "                                                        depth =config[\"max_depth\"],\n",
    "                                                        used_feature_rate=config[\"max_features\"],\n",
    "                                                        random_state=run,\n",
    "                                                        epochs=EPOCHS,\n",
    "                                                        use_dndf=USE_DNDF)\n",
    "                \n",
    "                # instantiate dNDF classifier for separate views and concatenated view\n",
    "                dNDF_per_view = []\n",
    "                for v in range(len(Xs_train)+1):\n",
    "                    dNDF_per_view.append(MajorityVoteLearner(nb_estimators=config[\"n_estimators\"],\n",
    "                                                            depth =config[\"max_depth\"],\n",
    "                                                            used_feature_rate=config[\"max_features\"],\n",
    "                                                            random_state=run,\n",
    "                                                            epochs=EPOCHS,\n",
    "                                                            use_dndf=USE_DNDF))\n",
    "                \n",
    "                print(\"Training multiview classifier-------------------------------\")\n",
    "                dNDF_mv = dNDF_mv.fit(Xs_train, y_train)\n",
    "                \n",
    "                print(\"Training separate views classifiers-------------------------------\")\n",
    "                for v in range(len(Xs_train)):\n",
    "                    dNDF_per_view[v] = dNDF_per_view[v].fit(Xs_train[v], y_train)\n",
    "\n",
    "                print(\"Training concatenated view classifier-------------------------------\")\n",
    "                dNDF_per_view[-1] = dNDF_per_view[-1].fit(X_train_concat, y_train)\n",
    "                \n",
    "                \n",
    "                # Optimize the posterior distributions for the each bound\n",
    "                for bound in BOUNDS:\n",
    "                    # Clear the posteriors (reset to uniform distribution)\n",
    "                    dNDF_mv.clear_posteriors()\n",
    "                    for v in range(len(Xs_train)+1):\n",
    "                        dNDF_per_view[v].clear_posteriors()\n",
    "                    \n",
    "                    # use the unlabeled data for DIS\n",
    "                    unlabeled_data, c_unlabeled_data = None, None\n",
    "                    if USE_UNLABELED and bound in ['DIS', 'DIS_inv', 'TND_DIS', 'TND_DIS_inv',]:\n",
    "                        unlabeled_data = UlX\n",
    "                        c_unlabeled_data = np.concatenate(UlX, axis=1)\n",
    "                    \n",
    "                    if bound == \"Uniform\":\n",
    "                        posterior_Qv = dNDF_mv.posterior_Qv\n",
    "                        posterior_rho = dNDF_mv.posterior_rho\n",
    "                        posterior_Qs = [dNDF_per_view[v].posterior_Q.tolist() for v in range(len(dNDF_per_view))]\n",
    "                    else:\n",
    "                        _, gibbs_risk, _ = dNDF_mv.mv_risk((Xs_train, y_train), incl_oob=False)\n",
    "                        print(f\"### Multiview classifier gibbs risk before Optim: {gibbs_risk}\")\n",
    "                        print(colored(f\"Optimizing {bound} for multiview classifier-------------------------------\", 'green'))\n",
    "                        prev_time = datetime.now()\n",
    "                        posterior_Qv , posterior_rho = dNDF_mv.optimize_rho(bound,\n",
    "                                                                            labeled_data=(Xs_train, y_train),\n",
    "                                                                            unlabeled_data=unlabeled_data,\n",
    "                                                                            incl_oob=False,\n",
    "                                                                            max_iter=MAX_ITER,\n",
    "                                                                            optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                            alpha=alpha)\n",
    "                        print(colored(f\"Optimization took {datetime.now() - prev_time} -------------------------------\", 'yellow'))\n",
    "                        \n",
    "                        print(colored(f\"Optimizing {bound} for separate views classifiers-------------------------------\", 'green'))\n",
    "                        posterior_Qs = []\n",
    "                        for v in range(len(Xs_train)):\n",
    "                            posterior_Q = dNDF_per_view[v].optimize_Q(bound,\n",
    "                                                                            labeled_data=(Xs_train[v], y_train),\n",
    "                                                                            unlabeled_data=unlabeled_data[v] if unlabeled_data else None,\n",
    "                                                                            incl_oob=False,\n",
    "                                                                            max_iter=MAX_ITER,\n",
    "                                                                            optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                            alpha=alpha)\n",
    "                            posterior_Qs.append(posterior_Q.tolist())\n",
    "                        print(colored(f\"Optimizing {bound} for concatenated classifier-------------------------------\", 'green'))\n",
    "                        posterior_Q_concat = dNDF_per_view[-1].optimize_Q(bound,\n",
    "                                                                            labeled_data=(X_train_concat, y_train),\n",
    "                                                                            unlabeled_data=c_unlabeled_data,\n",
    "                                                                            incl_oob=False,\n",
    "                                                                            max_iter=MAX_ITER,\n",
    "                                                                            optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                            alpha=alpha)\n",
    "                        posterior_Qs.append(posterior_Q_concat.tolist())\n",
    "                        \n",
    "                        _, gibbs_riska, _ = dNDF_mv.mv_risk((Xs_train, y_train), incl_oob=False)\n",
    "                        print(f\"### Multiview classifier gibbs risk after Optim: {gibbs_riska}\")\n",
    "                        # Compute the bound for the multiview classifier\n",
    "                        print(colored(f\"Optimization is done! -------------------------------\", 'green'))\n",
    "                    \n",
    "                    print(colored(f\"Computing the bound values ans risks -------------------------------\", 'green'))\n",
    "                    mv_bound, mv_grisk, mv_eS, mv_dS, mv_klqp, klrpi, ng, _, nd = dNDF_mv.bound(\n",
    "                                        bound=bound,\n",
    "                                        labeled_data=(Xs_train, y_train),\n",
    "                                        unlabeled_data=unlabeled_data,\n",
    "                                        incl_oob=False,\n",
    "                                        alpha=alpha)\n",
    "                    # Compute the risk of the multiview classifier\n",
    "                    P, mv_risk = dNDF_mv.predict_MV(Xs_test, y_test)\n",
    "                    \n",
    "                    # Compute the bounds and risks for the separate views classifiers\n",
    "                    v_bounds = []\n",
    "                    v_grisks = []\n",
    "                    v_eSs = []\n",
    "                    v_dSs = []\n",
    "                    v_klqps = []\n",
    "                    for v in range(len(Xs_test)+1):\n",
    "                        if v == len(Xs_test):\n",
    "                            # Compute the bound for the concatenated view\n",
    "                            concat_bound, grisk, eS, dS, klqp, _, _, _ = dNDF_per_view[v].bound(\n",
    "                                        bound=bound,\n",
    "                                        labeled_data=(X_train_concat, y_train),\n",
    "                                        unlabeled_data=c_unlabeled_data,\n",
    "                                        incl_oob=False,\n",
    "                                        alpha=1)\n",
    "                            v_bounds.append(concat_bound)\n",
    "                        else:\n",
    "                            # Compute the bound for the separate views\n",
    "                            v_bound, grisk, eS, dS, klqp, _, _, _ = dNDF_per_view[v].bound(\n",
    "                                            bound=bound,\n",
    "                                            labeled_data=(Xs_train[v], y_train),\n",
    "                                            unlabeled_data=unlabeled_data[v] if unlabeled_data else None,\n",
    "                                            incl_oob=False,\n",
    "                                            alpha=1)\n",
    "                            v_bounds.append(v_bound)\n",
    "                        v_grisks.append(grisk)\n",
    "                        v_eSs.append(eS)\n",
    "                        v_dSs.append(dS)\n",
    "                        v_klqps.append(klqp)\n",
    "                    \n",
    "                    v_risks = [dNDF_per_view[v].predict(Xs_test[v], y_test)[1] for v in range(len(Xs_test))]\n",
    "                    v_risks.append(dNDF_per_view[-1].predict(X_test_concat, y_test)[1])\n",
    "                    # print(f\"{dNDF_mv.posterior_Qv=} {dNDF_mv.posterior_rho=}\")\n",
    "                    \n",
    "\n",
    "                    # Save the results\n",
    "                    print(colored(f\"Entering save and stats zone-------------------------------\", 'green'))\n",
    "                    views_risks = metric_dict(mv_risk, v_risks)\n",
    "                    views_gibbs_risks = metric_dict(mv_grisk, v_grisks)\n",
    "                    views_eSs = metric_dict(mv_eS, v_eSs)\n",
    "                    views_dSs = metric_dict(mv_dS, v_dSs)\n",
    "                    views_bounds = metric_dict(mv_bound, v_bounds)\n",
    "                    views_klqps = metric_dict(mv_klqp, v_klqps)\n",
    "                    views_kl_rhopi = metric_dict(klrpi, [np.nan for _ in range(len(Xs_test)+1)])\n",
    "                    \n",
    "                    list_posterior_Qv = [q.tolist() for q in posterior_Qv]\n",
    "                    rounded_posterior_Qv = [[\"{:.9f}\".format(q) for q in Q] for Q in list_posterior_Qv]\n",
    "                    rounded_posterior_rho = [\"{:.9f}\".format(rho) for rho in posterior_rho.tolist()]\n",
    "                    rounded_posterior_Qs = [[\"{:.9f}\".format(q) for q in Q] for Q in posterior_Qs]\n",
    "                    views_posterior_Qs = metric_dict(rounded_posterior_Qv, rounded_posterior_Qs)\n",
    "                    views_posterior_rho = metric_dict(rounded_posterior_rho, [np.nan for _ in range(len(Xs_test)+1)])\n",
    "                    \n",
    "                    \n",
    "                    assert len(views_risks) == len(views_bounds)\n",
    "                    assert len(views_risks) == len(views_gibbs_risks) and len(views_risks) == len(views_eSs)\n",
    "                    assert len(views_dSs) == len(views_eSs)\n",
    "                    \n",
    "                    for (kr, r), (kb, b) in zip(views_risks.items(), views_bounds.items()):\n",
    "                        assert kr == kb # check if the keys are the same\n",
    "                        exp = {\"Run\": run+1, \n",
    "                            \"Bound_name\": bound, \n",
    "                            \"View\": kr, \n",
    "                            \"Risk\": \"{:.3f}\".format(r),\n",
    "                            \"Gibbs_Risk\": \"{:.3f}\".format(views_gibbs_risks[kr]),\n",
    "                            \"Bound\": \"{:.3f}\".format(b),\n",
    "                            \"Join_Error\": \"{:.3f}\".format(views_eSs[kr]),\n",
    "                            \"Disagreement\": \"{:.3f}\".format(views_dSs[kr]),\n",
    "                            \"KL_QP\": \"{:.3f}\".format(views_klqps[kr]),\n",
    "                            \"KL_RhoPi\": \"{:.3f}\".format(views_kl_rhopi[kr]),\n",
    "                            \"n_labeled\": ng,\n",
    "                            \"n_all\": nd,\n",
    "                            \"Posterior_Qv\": views_posterior_Qs[kr],\n",
    "                            \"Posterior_rho\": views_posterior_rho[kr]}\n",
    "                        experiments[s1_size][alpha][config[\"name\"]].append(exp)\n",
    "                    # TODO: add the posterior_Qv and posterior_rho to the experiment\n",
    "                # del dNDF_mv, dNDF_per_view\n",
    "                \n",
    "            cfg_dir = alpha_dir + \"/\" + config[\"name\"]\n",
    "            os.makedirs(cfg_dir, exist_ok=True)\n",
    "            experiment_df = pd.DataFrame(experiments[s1_size][alpha][config[\"name\"]])\n",
    "            # example: results/s_labeled-5/alpha-1/stump/MNIST_4vs9_20runs.csv\n",
    "            file_name = f\"{cfg_dir}/{dataset._name}_alpha-all_{label_1}vs{label_2}_{len(RUNS)}runs.csv\"\n",
    "            experiment_df.to_csv(file_name, sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicating the results (not necessary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results-dup\", exist_ok=True)\n",
    "\n",
    "for s_labeled_size, size_exp in experiments.items():\n",
    "    s_labeled_dir = 'results-dup'+f\"/s_labeled-{int(s_labeled_size*100)}\"\n",
    "    os.makedirs(s_labeled_dir, exist_ok=True)\n",
    "    for alpha, alpha_exp in size_exp.items():\n",
    "        alpha_dir = s_labeled_dir+ f\"/alpha-{alpha}\"\n",
    "        os.makedirs(alpha_dir, exist_ok=True)\n",
    "        for cfg, cfg_exp in alpha_exp.items():\n",
    "            if cfg_exp == []:\n",
    "                continue\n",
    "            cfg_dir = alpha_dir + \"/\" + cfg\n",
    "            os.makedirs(cfg_dir, exist_ok=True)\n",
    "            experiment_df = pd.DataFrame(cfg_exp)\n",
    "            # example: results-dup/s_labeled-5/alpha-1/stump/MNIST_4vs9_20runs.csv\n",
    "            file_name = f\"{cfg_dir}/{dataset._name}_{label_1}vs{label_2}_{len(RUNS)}runs.csv\"\n",
    "            experiment_df.to_csv(file_name, sep=\" \", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Bound_name</th>\n",
       "      <th>View</th>\n",
       "      <th>Risk</th>\n",
       "      <th>Gibbs_Risk</th>\n",
       "      <th>Bound</th>\n",
       "      <th>Join_Error</th>\n",
       "      <th>Disagreement</th>\n",
       "      <th>KL_QP</th>\n",
       "      <th>KL_RhoPi</th>\n",
       "      <th>n_labeled</th>\n",
       "      <th>n_all</th>\n",
       "      <th>Posterior_Qv</th>\n",
       "      <th>Posterior_rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View1</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View2</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View3</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View4</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View5</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View6</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>Concatenated</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>Multiview</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[[0.010000000, 0.010000000, 0.010000000, 0.010...</td>\n",
       "      <td>[0.166666672, 0.166666672, 0.166666672, 0.1666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>View1</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.307</td>\n",
       "      <td>2.005</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.000437399, 0.002739322, 0.070695564, 0.0003...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>View2</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.668</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.002922207, 0.023197588, 0.022175055, 0.0019...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>View3</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.199</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.004244578, 0.004196422, 0.013965365, 0.0044...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>View4</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.158</td>\n",
       "      <td>1.675</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.002964397, 0.000461231, 0.000474736, 0.0003...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>View5</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.427</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.002842372, 0.001688982, 0.011625829, 0.0192...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>View6</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.993</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.000757615, 0.000793829, 0.002388754, 0.0264...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>Concatenated</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.174</td>\n",
       "      <td>1.480</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.052353144, 0.000450353, 0.000542949, 0.0005...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>PBkl</td>\n",
       "      <td>Multiview</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[[0.003838615, 0.018835545, 0.022640023, 0.003...</td>\n",
       "      <td>[0.037434552, 0.286180615, 0.065453216, 0.2834...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View1</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View2</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View3</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>Uniform</td>\n",
       "      <td>View4</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>[0.010000000, 0.010000000, 0.010000000, 0.0100...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Run Bound_name          View   Risk Gibbs_Risk  Bound Join_Error  \\\n",
       "0     1    Uniform         View1  0.195      0.381  0.840      0.159   \n",
       "1     1    Uniform         View2  0.141      0.246  0.563      0.096   \n",
       "2     1    Uniform         View3  0.135      0.285  0.644      0.107   \n",
       "3     1    Uniform         View4  0.102      0.226  0.520      0.079   \n",
       "4     1    Uniform         View5  0.113      0.198  0.463      0.077   \n",
       "5     1    Uniform         View6  0.429      0.442  0.965      0.292   \n",
       "6     1    Uniform  Concatenated  0.082      0.194  0.454      0.063   \n",
       "7     1    Uniform     Multiview  0.072      0.297  0.670      0.101   \n",
       "8     1       PBkl         View1  0.259      0.326  0.737      0.172   \n",
       "9     1       PBkl         View2  0.164      0.186  0.439      0.093   \n",
       "10    1       PBkl         View3  0.142      0.263  0.600      0.102   \n",
       "11    1       PBkl         View4  0.114      0.151  0.368      0.071   \n",
       "12    1       PBkl         View5  0.135      0.176  0.418      0.079   \n",
       "13    1       PBkl         View6  0.427      0.429  0.942      0.354   \n",
       "14    1       PBkl  Concatenated  0.092      0.144  0.353      0.057   \n",
       "15    1       PBkl     Multiview  0.068      0.200  0.465      0.070   \n",
       "16    2    Uniform         View1  0.215      0.371  0.819      0.157   \n",
       "17    2    Uniform         View2  0.123      0.262  0.595      0.095   \n",
       "18    2    Uniform         View3  0.153      0.281  0.636      0.105   \n",
       "19    2    Uniform         View4  0.115      0.226  0.521      0.080   \n",
       "\n",
       "   Disagreement   KL_QP KL_RhoPi  n_labeled   n_all  \\\n",
       "0         0.443   0.000      nan     2362.0  2362.0   \n",
       "1         0.300   0.000      nan     2362.0  2362.0   \n",
       "2         0.356   0.000      nan     2362.0  2362.0   \n",
       "3         0.292   0.000      nan     2362.0  2362.0   \n",
       "4         0.243   0.000      nan     2362.0  2362.0   \n",
       "5         0.300   0.000      nan     2362.0  2362.0   \n",
       "6         0.261   0.000      nan     2362.0  2362.0   \n",
       "7         0.394   0.000    0.000     2362.0  2362.0   \n",
       "8         0.307   2.005      nan     2362.0  2362.0   \n",
       "9         0.185   0.668      nan     2362.0  2362.0   \n",
       "10        0.323   0.199      nan     2362.0  2362.0   \n",
       "11        0.158   1.675      nan     2362.0  2362.0   \n",
       "12        0.195   0.427      nan     2362.0  2362.0   \n",
       "13        0.150   0.993      nan     2362.0  2362.0   \n",
       "14        0.174   1.480      nan     2362.0  2362.0   \n",
       "15        0.299  -0.146   -0.085     2362.0  2362.0   \n",
       "16        0.427   0.000      nan     2362.0  2362.0   \n",
       "17        0.333   0.000      nan     2362.0  2362.0   \n",
       "18        0.352   0.000      nan     2362.0  2362.0   \n",
       "19        0.292   0.000      nan     2362.0  2362.0   \n",
       "\n",
       "                                         Posterior_Qv  \\\n",
       "0   [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "1   [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "2   [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "3   [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "4   [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "5   [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "6   [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "7   [[0.010000000, 0.010000000, 0.010000000, 0.010...   \n",
       "8   [0.000437399, 0.002739322, 0.070695564, 0.0003...   \n",
       "9   [0.002922207, 0.023197588, 0.022175055, 0.0019...   \n",
       "10  [0.004244578, 0.004196422, 0.013965365, 0.0044...   \n",
       "11  [0.002964397, 0.000461231, 0.000474736, 0.0003...   \n",
       "12  [0.002842372, 0.001688982, 0.011625829, 0.0192...   \n",
       "13  [0.000757615, 0.000793829, 0.002388754, 0.0264...   \n",
       "14  [0.052353144, 0.000450353, 0.000542949, 0.0005...   \n",
       "15  [[0.003838615, 0.018835545, 0.022640023, 0.003...   \n",
       "16  [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "17  [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "18  [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "19  [0.010000000, 0.010000000, 0.010000000, 0.0100...   \n",
       "\n",
       "                                        Posterior_rho  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7   [0.166666672, 0.166666672, 0.166666672, 0.1666...  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14                                                NaN  \n",
       "15  [0.037434552, 0.286180615, 0.065453216, 0.2834...  \n",
       "16                                                NaN  \n",
       "17                                                NaN  \n",
       "18                                                NaN  \n",
       "19                                                NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(experiments[0.2][0.5][\"weak_learner\"])\n",
    "df[\"Risk\"] = df[\"Risk\"].astype(float)\n",
    "df[\"Bound\"] = df[\"Bound\"].astype(float)\n",
    "# df['Bound'] = df['Bound'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)\n",
    "# df['Risk'] = df['Risk'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Bound_name</th>\n",
       "      <th>View</th>\n",
       "      <th>Risk</th>\n",
       "      <th>Gibbs_Risk</th>\n",
       "      <th>Bound</th>\n",
       "      <th>Join_Error</th>\n",
       "      <th>Disagreement</th>\n",
       "      <th>KL_QP</th>\n",
       "      <th>KL_RhoPi</th>\n",
       "      <th>n_labeled</th>\n",
       "      <th>n_all</th>\n",
       "      <th>Posterior_Qv</th>\n",
       "      <th>Posterior_rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Run, Bound_name, View, Risk, Gibbs_Risk, Bound, Join_Error, Disagreement, KL_QP, KL_RhoPi, n_labeled, n_all, Posterior_Qv, Posterior_rho]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Risk\"] > df[\"Bound\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_df = df.groupby([\"Bound_name\", \"View\"]).mean()\n",
    "# agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the data (use the plot.ipynb notebook it's well structured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_grid(exps, m, runs=len(RUNS), bounds=BOUNDS):\n",
    "\n",
    "    sns.set_style(style=\"ticks\")\n",
    "    bounds_palette = sns.color_palette(\"viridis\", n_colors=len(bounds))\n",
    "    risk_palette = sns.color_palette(\"flare\", n_colors=len(bounds))\n",
    "    risk_palette.reverse()\n",
    "\n",
    "    num_views = len(exps['View'].unique())\n",
    "    num_cols = num_views // 2 + num_views % 2  # Calculate number of columns for subplots\n",
    "\n",
    "    fig, ax = plt.subplots(2, num_cols, figsize=(14, 10), sharey=True)\n",
    "\n",
    "    for i, view in enumerate(exps['View'].unique()):\n",
    "        view_data = exps[exps['View'] == view]\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        \n",
    "        risk = view_data.groupby([\"Bound_name\", \"View\"])['Risk'].mean()\n",
    "        # mean_risk = risk.median()\n",
    "        # up_risk = mean_risk+risk.std()\n",
    "        # lw_risk = mean_risk-risk.std()\n",
    "        # print(mean_risk, up_risk, lw_risk)\n",
    "        # Plot Bound\n",
    "        sns.barplot(x='Bound_name', y='Bound', data=view_data, ax=ax[row, col], hue='Bound_name', hatch='.', palette=bounds_palette)        # Create a horizontal line at the median risk\n",
    "        sns.barplot(x='Bound_name', y='Risk', data=view_data, ax=ax[row, col], hue='Bound_name', hatch='\\\\', palette=risk_palette)\n",
    "\n",
    "        # ax[row, col].axhline(y=up_risk, color='b', linestyle='-', label='Q3 Risk')\n",
    "        # ax[row, col].axhline(y=lw_risk, color='r', linestyle='--', label='Median Risk')\n",
    "        # ax[row, col].axhline(y=mean_risk, color='g', linestyle='-', label='Q1 Risk')\n",
    "        ax[row, col].set_title(f'{view}')\n",
    "        ax[row, col].set_xlabel('Bounds')\n",
    "        ax[row, col].set_ylabel('Means')\n",
    "        plt.setp( ax[row, col].xaxis.get_majorticklabels(), rotation=70 )\n",
    "\n",
    "        \n",
    "\n",
    "    # handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "    # labels = [labels[i]+\" bound\" if i < len(labels)/2 else labels[i]+\" Gibbs risk\" for i in range(len(labels))]\n",
    "    # ax[0, 0].legend(handles, labels, title='Bounds', loc='upper right', fontsize='medium')\n",
    "    fig.suptitle(f'Test error rates and multiview PAC-Bayesian bound values, {m=},\\naveraged over {runs} runs')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results_grid(df, len(Xs_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_rho\n",
    "sns.heatmap(posterior_Qv, fmt=\".2f\", cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(experiments, m, runs=len(RUNS), bounds=BOUNDS):\n",
    "    sns.set_style(style=\"ticks\")\n",
    "    bounds_palette = sns.mpl_palette(\"viridis\", n_colors=len(bounds))\n",
    "    risk_palette = sns.color_palette(\"flare\", n_colors=len(bounds))\n",
    "    risk_palette.reverse()\n",
    "    \n",
    "    ax = sns.barplot(experiments, x='View', hue='Bound_name', y='Bound', errorbar=\"sd\", width=0.8, hatch='.', palette=bounds_palette)\n",
    "    ax = sns.barplot(experiments, x='View', hue='Bound_name', y='Risk', errorbar=\"sd\", width=0.8, hatch='\\\\', palette=risk_palette)\n",
    "\n",
    "    plt.title(f'Test error rates and multiview PAC-Bayesian bound values, {m=},\\naveraged over {runs} runs')\n",
    "    plt.xlabel('Views')\n",
    "    plt.ylabel('Means')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels = [labels[i]+\" bound\" if i < len(labels)/2 else labels[i]+\" Gibbs risk\" for i in range(len(labels))]\n",
    "\n",
    "    # Creating a unified legend for both plots\n",
    "    plt.legend(handles, labels, title=\"Bounds and risks\", loc='upper right', fontsize='medium')\n",
    "    plt.tight_layout() \n",
    "    plt.gcf().set_size_inches(24, 12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df, len(Xs_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
