{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains everything necessary to reproduce the experiments in our paper:  \n",
    "\n",
    "*Multi-View Majority Vote Learning Algorithms: Direct Minimization of PAC-Bayesian Bounds*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from mvpb import MultiViewMajorityVoteLearner, MajorityVoteLearner\n",
    "from mvpb.util import uniform_distribution\n",
    "\n",
    "\n",
    "# Import data\n",
    "from data import (SampleData,\n",
    "                           Nhanes,\n",
    "                           MultipleFeatures,\n",
    "                           MNIST_MV_Datasets,\n",
    "                           Fash_MNIST_MV_Datasets,\n",
    "                           EMNIST_Letters_MV_Datasets,\n",
    "                           Mushrooms,\n",
    "                           PTB_XL_plus,\n",
    "                           Nutrimouse,\n",
    "                           ReutersEN,\n",
    "                           IS,\n",
    "                           CorelImageFeatures,\n",
    "                           NUS_WIDE_OBJECT,\n",
    "                           ALOI,\n",
    "                           train_test_split,\n",
    "                           train_test_merge,\n",
    "                           s1_s2_split,\n",
    "                           multiclass_to_binary,\n",
    "                           balance_dataset,\n",
    "                           other_binary_options,\n",
    "                           poison_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the multiview datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MultipleFeatures(size=\"large\")\n",
    "X_train, y_train, X_test, y_test = dataset.get_data()\n",
    "if isinstance(dataset, PTB_XL_plus):\n",
    "    real_classes = dataset.get_real_classes(np.unique(y_train))\n",
    "\n",
    "Xs_train = []\n",
    "Xs_test = []\n",
    "for xtr, xts in zip(X_train, X_test):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(xtr)\n",
    "    Xs_train.append(scaler.transform(xtr))\n",
    "    Xs_test.append(scaler.transform(xts))\n",
    "\n",
    "X_train_concat = [np.concatenate(Xs_train, axis=1)]\n",
    "X_test_concat = [np.concatenate(Xs_test, axis=1)]\n",
    "\n",
    "np.unique(y_train), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 200), (60000,), (10000, 200), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape, y_train.shape, X_test[1].shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = range(1)\n",
    "\n",
    "OPTIMIZE_LAMBDA_GAMMA = True\n",
    "# ALPHA = [1, 0.5, 1.1, 2]\n",
    "ALPHA = [1.1]\n",
    "MAX_ITER = 1000\n",
    "\n",
    "stump_config = {\n",
    "    \"name\": \"stump\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 1,\n",
    "    \"max_features\": 0.5,\n",
    "}\n",
    "weak_learners_config = {\n",
    "    \"name\": \"weak_learner\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 3,\n",
    "    \"max_features\": 0.5,\n",
    "}\n",
    "strong_learners_config = {\n",
    "    \"name\": \"strong_learner\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 6,\n",
    "    \"max_features\": 0.8,\n",
    "}\n",
    "\n",
    "# CFG = [stump_config, weak_learners_config, strong_learners_config]\n",
    "CFG = [weak_learners_config]\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "TO_BINARY  = \"ovo\" # One of [\"ovr\", \"ovo\", \"other\",  None]\n",
    "label_1 = 4\n",
    "# if isinstance(dataset, PTB_XL_plus):\n",
    "#     label_1 = np.unique(y_test)[np.where(real_classes == \"['NORM']\")[0][0]]\n",
    "label_2 = 9\n",
    "\n",
    "POISON = False\n",
    "\n",
    "USE_UNLABELED = False\n",
    "s_labeled_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5] if USE_UNLABELED else [0.1]\n",
    "\n",
    "# BOUNDS = ['PBkl', 'PBkl_inv', 'TND_DIS', 'TND_DIS_inv', 'TND', 'TND_inv', 'DIS', 'DIS_inv', 'Cbound', 'C_TND']\n",
    "BOUNDS = ['Cbound']\n",
    "MV_SPECIFIC_BOUNDS = ['TND_DIS', 'TND_DIS_inv']\n",
    "\n",
    "m = y_train.size #350\n",
    "test_size = 1 - (m  / (y_test.size+y_train.size))\n",
    "experiments = {}\n",
    "for s_labeled_size in s_labeled_sizes:\n",
    "    experiments[s_labeled_size] = {}\n",
    "    for alpha in ALPHA:\n",
    "        experiments[s_labeled_size][alpha] = {}\n",
    "        for cfg in CFG:\n",
    "            experiments[s_labeled_size][alpha][cfg[\"name\"]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: {1.1: {'weak_learner': []}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if POISON:\n",
    "#     Xs_train, y_train = poison_dataset(Xs_train, y_train, poison_label=label_1, target_label=label_2, target_view=3, num_samples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to binary OVR (One Vs Rest) or OVO (One Vs One) if needed\n",
    "if TO_BINARY == \"ovr\":\n",
    "    Xs_train, y_train, Xs_test, y_test = multiclass_to_binary(Xs_train, y_train, Xs_test, y_test, type=TO_BINARY, label_1=label_1)\n",
    "elif TO_BINARY == \"ovo\":\n",
    "    Xs_train, y_train, Xs_test, y_test = multiclass_to_binary(Xs_train, y_train, Xs_test, y_test, type=TO_BINARY, label_1=label_1, label_2=label_2)\n",
    "elif TO_BINARY == \"other\":\n",
    "    y_train, y_test = other_binary_options(dataset, y_train, y_test)\n",
    "else:\n",
    "    print(colored(f\"WARNING: TO_BINARY={TO_BINARY}, continuing\", 'yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44m\u001b[30m############ Using 10.0% labeled data ############\u001b[0m\n",
      "\u001b[44m\u001b[30m\t############ Using alpha=1.1 ############\u001b[0m\n",
      "\u001b[44m\u001b[30m\t\t############ Using weak_learner ############\u001b[0m\n",
      "\u001b[34m\n",
      "----------------Run 1---------------\u001b[0m\n",
      "Training multiview classifier-------------------------------\n",
      "Training separate views classifiers-------------------------------\n",
      "Training concatenated view classifier-------------------------------\n",
      "### Multiview classifier gibbs risk before Optim: 0.29796217894439736\n",
      "\u001b[32mOptimizing Cbound for multiview classifier-------------------------------\u001b[0m\n",
      "device=device(type='cuda', index=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c001d83589c8475f9a20ba70c560e070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0,\t Loss: -820.3153076171875\n",
      "Iteration: 1,\t Loss: -820.3182983398438\n",
      "Iteration: 2,\t Loss: -820.3214111328125\n",
      "Iteration: 3,\t Loss: -820.3245849609375\n",
      "Iteration: 4,\t Loss: -820.3280639648438\n",
      "Iteration: 5,\t Loss: -820.331787109375\n",
      "Iteration: 6,\t Loss: -820.3359375\n",
      "Iteration: 7,\t Loss: -820.3406982421875\n",
      "Iteration: 8,\t Loss: -820.345947265625\n",
      "Iteration: 9,\t Loss: -820.3519897460938\n",
      "Iteration: 10,\t Loss: -820.3587646484375\n",
      "Iteration: 11,\t Loss: -820.3665771484375\n",
      "Iteration: 12,\t Loss: -820.3754272460938\n",
      "Iteration: 13,\t Loss: -820.3854370117188\n",
      "Iteration: 14,\t Loss: -820.3968505859375\n",
      "Iteration: 15,\t Loss: -820.4097290039062\n",
      "Iteration: 16,\t Loss: -820.4242553710938\n",
      "Iteration: 17,\t Loss: -820.4404296875\n",
      "Iteration: 18,\t Loss: -820.4557495117188\n",
      "Iteration: 19,\t Loss: -820.4767456054688\n",
      "Iteration: 20,\t Loss: -820.49658203125\n",
      "Iteration: 21,\t Loss: -820.5167236328125\n",
      "Iteration: 22,\t Loss: -820.5403442382812\n",
      "Iteration: 23,\t Loss: -820.5565795898438\n",
      "Iteration: 24,\t Loss: -820.5532836914062\n",
      "Iteration: 25,\t Loss: -820.6318359375\n",
      "Iteration: 26,\t Loss: -820.5936889648438\n",
      "Iteration: 27,\t Loss: -820.6790771484375\n",
      "Iteration: 28,\t Loss: -820.6946411132812\n",
      "Iteration: 29,\t Loss: -820.772705078125\n",
      "Iteration: 30,\t Loss: -820.7722778320312\n",
      "Iteration: 31,\t Loss: -820.9183959960938\n",
      "Iteration: 32,\t Loss: -820.7916870117188\n",
      "Iteration: 33,\t Loss: -820.8502197265625\n",
      "Iteration: 34,\t Loss: -820.9920654296875\n",
      "Iteration: 35,\t Loss: -821.0843505859375\n",
      "Iteration: 36,\t Loss: -821.1036376953125\n",
      "Iteration: 37,\t Loss: -821.3591918945312\n",
      "Iteration: 38,\t Loss: -821.09716796875\n",
      "Iteration: 39,\t Loss: -821.2349853515625\n",
      "Iteration: 40,\t Loss: -821.2664184570312\n",
      "Iteration: 41,\t Loss: -821.5136108398438\n",
      "Iteration: 42,\t Loss: -821.376953125\n",
      "Iteration: 43,\t Loss: -821.7489013671875\n",
      "Iteration: 44,\t Loss: -821.2576293945312\n",
      "Iteration: 45,\t Loss: -821.3933715820312\n",
      "Iteration: 46,\t Loss: -821.6568603515625\n",
      "Iteration: 47,\t Loss: -821.9429931640625\n",
      "Iteration: 48,\t Loss: -821.664306640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(colored(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbound\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for multiview classifier-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     82\u001b[0m prev_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 83\u001b[0m posterior_Qv , posterior_rho \u001b[38;5;241m=\u001b[39m \u001b[43mdNDF_mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_rho\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mlabeled_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mXs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43munlabeled_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlabeled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mincl_oob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_ITER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43moptimise_lambda_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPTIMIZE_LAMBDA_GAMMA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(colored(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mprev_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MV_SPECIFIC_BOUNDS:\n",
      "File \u001b[0;32m~/Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds/mvpb/multiview_learner.py:245\u001b[0m, in \u001b[0;36mMultiViewMajorityVoteLearner.optimize_rho\u001b[0;34m(self, bound, labeled_data, unlabeled_data, incl_oob, max_iter, optimise_lambda_gamma, alpha)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m posterior_Qv, posterior_rho\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m bound \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCbound\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 245\u001b[0m     posterior_Qv, posterior_rho \u001b[38;5;241m=\u001b[39m \u001b[43mbounds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizeCBound_mv_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrisks_views\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdS_views\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_posteriors(posterior_rho, posterior_Qv)\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m posterior_Qv, posterior_rho\n",
      "File \u001b[0;32m~/Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds/mvpb/bounds/c_bound/c_bound.py:203\u001b[0m, in \u001b[0;36moptimizeCBound_mv_torch\u001b[0;34m(grisks_views, dS_views, ng, nd, device, max_iter, delta, eps, alpha, t)\u001b[0m\n\u001b[1;32m    200\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Calculating the loss\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m loss, constraint_risk, constraint_dis \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mv_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrisks_views\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdS_views\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_Qv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_rho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_Pv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m log_barrier(constraint_risk\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    205\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds/mvpb/bounds/c_bound/c_bound.py:137\u001b[0m, in \u001b[0;36mcompute_mv_loss\u001b[0;34m(grisks_views, dS_views, posterior_Qv, posterior_rho, prior_Pv, prior_pi, ng, nd, delta, alpha)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# print(f\"phi_r: {phi_r}, phi_d: {phi_d}\")\u001b[39;00m\n\u001b[1;32m    136\u001b[0m loss_r \u001b[38;5;241m=\u001b[39m klinv(emp_mv_risk, phi_r, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m loss_d \u001b[38;5;241m=\u001b[39m \u001b[43mklinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdS_mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMIN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m loss_r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(loss_r\u001b[38;5;241m.\u001b[39mdevice), loss_r)\n\u001b[1;32m    140\u001b[0m loss_d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(loss_d\u001b[38;5;241m.\u001b[39mdevice), loss_d)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds/mvpb/bounds/tools.py:210\u001b[0m, in \u001b[0;36mklInvFunction.forward\u001b[0;34m(ctx, q, epsilon, mode)\u001b[0m\n\u001b[1;32m    207\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(q, epsilon)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# We solve the optimization problem to find the optimal p\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mkl_inv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(out \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n",
      "File \u001b[0;32m~/Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds/mvpb/bounds/tools.py:181\u001b[0m, in \u001b[0;36mkl_inv\u001b[0;34m(q, epsilon, mode, tol, nb_iter_max)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_iter_max):\n\u001b[1;32m    179\u001b[0m     p \u001b[38;5;241m=\u001b[39m (p_min \u001b[38;5;241m+\u001b[39m p_max) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mkl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m epsilon \u001b[38;5;129;01mor\u001b[39;00m (p_max \u001b[38;5;241m-\u001b[39m p_min) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m<\u001b[39m tol:\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m p\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m kl(q, p) \u001b[38;5;241m>\u001b[39m epsilon:\n",
      "File \u001b[0;32m~/Multi-View-Majority-Vote-Learning-Algorithms-Direct-Minimization-of-PAC-Bayesian-Bounds/mvpb/bounds/tools.py:168\u001b[0m, in \u001b[0;36mkl_inv.<locals>.kl\u001b[0;34m(q, p)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkl\u001b[39m(q, p):\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Compute the KL divergence between two Bernoulli distributions\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    (denoted kl divergence) using PyTorch\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m        The parameter of the prior Bernoulli distribution\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(q \u001b[38;5;241m/\u001b[39m p) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m q) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog((\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:941\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "before_merge = (Xs_train, y_train, Xs_test, y_test)\n",
    "Xs, y = train_test_merge(Xs_train, y_train, Xs_test, y_test)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "\n",
    "# iterate over the labeled data sizes #\n",
    "for i, s1_size in enumerate(s_labeled_sizes):\n",
    "    print(colored(f\"############ Using {s1_size*100}% labeled data ############\", 'black', on_color='on_blue'))\n",
    "    s_labeled_dir = 'results'+f\"/s_labeled-{int(s1_size*100)}\"\n",
    "    os.makedirs(s_labeled_dir, exist_ok=True)\n",
    "### iterate over the alpha values ###\n",
    "    \n",
    "    for j, alpha in enumerate(ALPHA):\n",
    "        print(colored(f\"\\t############ Using {alpha=} ############\", 'black', on_color='on_blue'))\n",
    "        alpha_dir = s_labeled_dir+ f\"/alpha-{alpha}\"\n",
    "        os.makedirs(alpha_dir, exist_ok=True)\n",
    "        \n",
    "#### iterate over the configurations ####\n",
    "        for k, config in enumerate(CFG):\n",
    "            print(colored(f\"\\t\\t############ Using {config['name']} ############\", 'black', on_color='on_blue'))\n",
    "            for run in RUNS:\n",
    "                print(colored(f\"\\n----------------Run {run+1}---------------\", 'blue'))\n",
    "\n",
    "                # Shuffle and split the dataset into training and testing\n",
    "                # if not dataset.split:\n",
    "                Xs_train, y_train, Xs_test, y_test = train_test_split(Xs, y, test_size=test_size, random_state=run*(i+1)*(j+1)*(k+1))\n",
    "                # else:\n",
    "                # Xs_train, y_train, Xs_test, y_test = before_merge\n",
    "\n",
    "                # Split the dataset into labeled and unlabeled\n",
    "                Xs_train, y_train, UlX, _ = s1_s2_split(Xs_train, y_train, s1_size=s1_size, random_state=run*(i+1)*(j+1)*(k+1))\n",
    "                X_train_concat = np.concatenate(Xs_train, axis=1)\n",
    "                X_test_concat = np.concatenate(Xs_test, axis=1)\n",
    "                    \n",
    "                # instantiate multiview dNDF classifier\n",
    "                dNDF_mv = MultiViewMajorityVoteLearner(nb_estimators=config[\"n_estimators\"],\n",
    "                                                        nb_views=len(Xs_train),\n",
    "                                                        depth =config[\"max_depth\"],\n",
    "                                                        used_feature_rate=config[\"max_features\"],\n",
    "                                                        random_state=run,\n",
    "                                                        epochs=EPOCHS,\n",
    "                                                        use_dndf=False)\n",
    "                \n",
    "                # instantiate dNDF classifier for separate views and concatenated view\n",
    "                dNDF_per_view = []\n",
    "                for v in range(len(Xs_train)+1):\n",
    "                    dNDF_per_view.append(MajorityVoteLearner(nb_estimators=config[\"n_estimators\"],\n",
    "                                                            depth =config[\"max_depth\"],\n",
    "                                                            used_feature_rate=config[\"max_features\"],\n",
    "                                                            random_state=run,\n",
    "                                                            epochs=EPOCHS,\n",
    "                                                            use_dndf=False))\n",
    "                \n",
    "                print(\"Training multiview classifier-------------------------------\")\n",
    "                dNDF_mv = dNDF_mv.fit(Xs_train, y_train)\n",
    "                \n",
    "                print(\"Training separate views classifiers-------------------------------\")\n",
    "                for v in range(len(Xs_train)):\n",
    "                    dNDF_per_view[v] = dNDF_per_view[v].fit(Xs_train[v], y_train)\n",
    "\n",
    "                print(\"Training concatenated view classifier-------------------------------\")\n",
    "                dNDF_per_view[-1] = dNDF_per_view[-1].fit(X_train_concat, y_train)\n",
    "                \n",
    "                \n",
    "                # Optimize the posterior distributions for the each bound\n",
    "                for bound in BOUNDS:\n",
    "                    # Clear the posteriors (reset to uniform distribution)\n",
    "                    dNDF_mv.clear_posteriors()\n",
    "                    for v in range(len(Xs_train)):\n",
    "                        dNDF_per_view[v].clear_posteriors()\n",
    "                    \n",
    "                    # use the unlabeled data for DIS\n",
    "                    unlabeled_data, c_unlabeled_data = None, None\n",
    "                    if USE_UNLABELED and bound in ['DIS', 'DIS_inv', 'TND_DIS', 'TND_DIS_inv',]:\n",
    "                        unlabeled_data = UlX\n",
    "                        c_unlabeled_data = np.concatenate(UlX, axis=1)\n",
    "                        \n",
    "                    if bound != \"Uniform\":\n",
    "                        _, gibbs_risk, _ = dNDF_mv.mv_risk((Xs_train, y_train), incl_oob=False)\n",
    "                        print(f\"### Multiview classifier gibbs risk before Optim: {gibbs_risk}\")\n",
    "                        print(colored(f\"Optimizing {bound} for multiview classifier-------------------------------\", 'green'))\n",
    "                        prev_time = datetime.now()\n",
    "                        posterior_Qv , posterior_rho = dNDF_mv.optimize_rho(bound,\n",
    "                                                                            labeled_data=(Xs_train, y_train),\n",
    "                                                                            unlabeled_data=unlabeled_data,\n",
    "                                                                            incl_oob=False,\n",
    "                                                                            max_iter=MAX_ITER,\n",
    "                                                                            optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                            alpha=alpha)\n",
    "                        print(colored(f\"Optimization took {datetime.now() - prev_time} -------------------------------\", 'yellow'))\n",
    "                        if bound not in MV_SPECIFIC_BOUNDS:\n",
    "                            print(colored(f\"Optimizing {bound} for separate views classifiers-------------------------------\", 'green'))\n",
    "                            posterior_Qs = []\n",
    "                            for v in range(len(Xs_train)):\n",
    "                                posterior_Q = dNDF_per_view[v].optimize_Q(bound,\n",
    "                                                                                labeled_data=(Xs_train[v], y_train),\n",
    "                                                                                unlabeled_data=unlabeled_data[v] if unlabeled_data else None,\n",
    "                                                                                incl_oob=False,\n",
    "                                                                                max_iter=MAX_ITER,\n",
    "                                                                                optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                                alpha=1)\n",
    "                                posterior_Qs.append(posterior_Q)\n",
    "                            print(colored(f\"Optimizing {bound} for concatenated classifier-------------------------------\", 'green'))\n",
    "                            posterior_Q_concat = dNDF_per_view[-1].optimize_Q(bound,\n",
    "                                                                                labeled_data=(X_train_concat, y_train),\n",
    "                                                                                unlabeled_data=c_unlabeled_data,\n",
    "                                                                                incl_oob=False,\n",
    "                                                                                max_iter=MAX_ITER,\n",
    "                                                                                optimise_lambda_gamma=OPTIMIZE_LAMBDA_GAMMA,\n",
    "                                                                                alpha=1)\n",
    "                            posterior_Qs.append(posterior_Q_concat)\n",
    "                        \n",
    "                        _, gibbs_riska, _ = dNDF_mv.mv_risk((Xs_train, y_train), incl_oob=False)\n",
    "                        print(f\"### Multiview classifier gibbs risk after Optim: {gibbs_riska}\")\n",
    "                        # Compute the bound for the multiview classifier\n",
    "                        print(colored(f\"Optimization is done! -------------------------------\", 'green'))\n",
    "                    \n",
    "                    print(colored(f\"Computing the bound values ans risks -------------------------------\", 'green'))\n",
    "                    mv_bound, r1, r2, klqp, klrpi,  n1, n2 = dNDF_mv.bound(\n",
    "                                        bound=bound,\n",
    "                                        labeled_data=(Xs_train, y_train),\n",
    "                                        unlabeled_data=unlabeled_data,\n",
    "                                        incl_oob=False,\n",
    "                                        alpha=alpha)\n",
    "                    # Compute the risk of the multiview classifier\n",
    "                    P, mv_risk = dNDF_mv.predict_MV(Xs_test, y_test)\n",
    "                    \n",
    "                    # Compute the bounds and risks for the separate views classifiers\n",
    "                    if bound not in MV_SPECIFIC_BOUNDS:\n",
    "                        v_bounds = []\n",
    "                        for v in range(len(Xs_test)):\n",
    "                            v_bound, _, _, _, _, _ = dNDF_per_view[v].bound(\n",
    "                                            bound=bound,\n",
    "                                            labeled_data=(Xs_train[v], y_train),\n",
    "                                            unlabeled_data=unlabeled_data[v] if unlabeled_data else None,\n",
    "                                            incl_oob=False,\n",
    "                                            alpha=1)\n",
    "                            v_bounds.append(v_bound)\n",
    "                        concat_bound, _, _, _, _, _ = dNDF_per_view[-1].bound(\n",
    "                                            bound=bound,\n",
    "                                            labeled_data=(X_train_concat, y_train),\n",
    "                                            unlabeled_data=c_unlabeled_data,\n",
    "                                            incl_oob=False,\n",
    "                                            alpha=1)\n",
    "                        v_bounds.append(concat_bound)\n",
    "                        v_risks = [dNDF_per_view[v].predict(Xs_test[v], y_test)[1] for v in range(len(Xs_test))]\n",
    "                        v_risks.append(dNDF_per_view[-1].predict(X_test_concat, y_test)[1])\n",
    "                    else:\n",
    "                        v_bounds = [np.nan for _ in range(len(Xs_test)+1)]\n",
    "                        v_risks = [np.nan for _ in range(len(Xs_test)+1)]\n",
    "                    # print(f\"{dNDF_mv.posterior_Qv=} {dNDF_mv.posterior_rho=}\")\n",
    "                    \n",
    "\n",
    "                    # Save the results\n",
    "                    print(colored(f\"Entering save and stats zone-------------------------------\", 'green'))\n",
    "                    views_risks = {f\"View{i+1}\": v_risks[i] for i in range(len(v_risks)-1)}\n",
    "                    views_risks.update({\"Concatenated\": v_risks[-1]})\n",
    "                    views_risks.update({\"Multiview\": mv_risk})\n",
    "                    views_bounds = {f\"View{i+1}\": v_bounds[i] for i in range(len(v_bounds)-1)}\n",
    "                    views_bounds.update({\"Concatenated\": v_bounds[-1]})\n",
    "                    views_bounds.update({\"Multiview\": mv_bound})\n",
    "                    for (kr, r), (kb, b) in zip(views_risks.items(), views_bounds.items()):\n",
    "                        assert kr == kb # check if the keys are the same\n",
    "                        exp = {\"Run\": run+1, \n",
    "                            \"Bound_name\": bound, \n",
    "                            \"View\": kr, \n",
    "                            \"Risk\": \"{:.3f}\".format(r),\n",
    "                            \"Bound\": \"{:.3f}\".format(b),\n",
    "                            \"R1\": \"{:.3f}\".format(r1),\n",
    "                            \"R2\": \"{:.3f}\".format(r2),\n",
    "                            \"KLqp\": \"{:.3f}\".format(klqp),\n",
    "                            \"KLRpi\": \"{:.3f}\".format(klrpi),\n",
    "                            \"N1\": \"{:.3f}\".format(n1),\n",
    "                            \"N2\": \"{:.3f}\".format(n2),}\n",
    "                        experiments[s1_size][alpha][config[\"name\"]].append(exp)\n",
    "                    # TODO: add the posterior_Qv and posterior_rho to the experiment\n",
    "                # del dNDF_mv, dNDF_per_view\n",
    "                \n",
    "            cfg_dir = alpha_dir + \"/\" + config[\"name\"]\n",
    "            os.makedirs(cfg_dir, exist_ok=True)\n",
    "            experiment_df = pd.DataFrame(experiments[s1_size][alpha][config[\"name\"]])\n",
    "            # example: results/s_labeled-5/alpha-1/stump/MNIST_4vs9_20runs.csv\n",
    "            file_name = f\"{cfg_dir}/{dataset._name}_{label_1}vs{label_2}_{len(RUNS)}runs.csv\"\n",
    "            experiment_df.to_csv(file_name, sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results-dup\", exist_ok=True)\n",
    "\n",
    "for s_labeled_size, size_exp in experiments.items():\n",
    "    s_labeled_dir = 'results-dup'+f\"/s_labeled-{int(s_labeled_size*100)}\"\n",
    "    os.makedirs(s_labeled_dir, exist_ok=True)\n",
    "    for alpha, alpha_exp in size_exp.items():\n",
    "        alpha_dir = s_labeled_dir+ f\"/alpha-{alpha}\"\n",
    "        os.makedirs(alpha_dir, exist_ok=True)\n",
    "        for cfg, cfg_exp in alpha_exp.items():\n",
    "            if cfg_exp == []:\n",
    "                continue\n",
    "            cfg_dir = alpha_dir + \"/\" + cfg\n",
    "            os.makedirs(cfg_dir, exist_ok=True)\n",
    "            experiment_df = pd.DataFrame(cfg_exp)\n",
    "            # example: results-dup/s_labeled-5/alpha-1/stump/MNIST_4vs9_20runs.csv\n",
    "            file_name = f\"{cfg_dir}/{dataset._name}_{label_1}vs{label_2}_{len(RUNS)}runs.csv\"\n",
    "            experiment_df.to_csv(file_name, sep=\" \", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(experiments[0.1][1.1][\"weak_learner\"])\n",
    "df[\"Risk\"] = df[\"Risk\"].astype(float)\n",
    "df[\"Bound\"] = df[\"Bound\"].astype(float)\n",
    "# df['Bound'] = df['Bound'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)\n",
    "# df['Risk'] = df['Risk'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)\n",
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Risk\"] > df[\"Bound\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_df = df.groupby([\"Bound_name\", \"View\"]).mean()\n",
    "# agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_grid(exps, m, runs=len(RUNS), bounds=BOUNDS):\n",
    "\n",
    "    sns.set_style(style=\"ticks\")\n",
    "    bounds_palette = sns.color_palette(\"viridis\", n_colors=len(bounds))\n",
    "    risk_palette = sns.color_palette(\"flare\", n_colors=len(bounds))\n",
    "    risk_palette.reverse()\n",
    "\n",
    "    num_views = len(exps['View'].unique())\n",
    "    num_cols = num_views // 2 + num_views % 2  # Calculate number of columns for subplots\n",
    "\n",
    "    fig, ax = plt.subplots(2, num_cols, figsize=(14, 10), sharey=True)\n",
    "\n",
    "    for i, view in enumerate(exps['View'].unique()):\n",
    "        view_data = exps[exps['View'] == view]\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        \n",
    "        risk = view_data.groupby([\"Bound_name\", \"View\"])['Risk'].mean()\n",
    "        mean_risk = risk.median()\n",
    "        # up_risk = mean_risk+risk.std()\n",
    "        # lw_risk = mean_risk-risk.std()\n",
    "        # print(mean_risk, up_risk, lw_risk)\n",
    "        # Plot Bound\n",
    "        sns.barplot(x='Bound_name', y='Bound', data=view_data, ax=ax[row, col], hue='Bound_name', hatch='.', palette=bounds_palette)        # Create a horizontal line at the median risk\n",
    "        sns.barplot(x='Bound_name', y='Risk', data=view_data, ax=ax[row, col], hue='Bound_name', hatch='\\\\', palette=risk_palette)\n",
    "\n",
    "        # ax[row, col].axhline(y=up_risk, color='b', linestyle='-', label='Q3 Risk')\n",
    "        # ax[row, col].axhline(y=lw_risk, color='r', linestyle='--', label='Median Risk')\n",
    "        # ax[row, col].axhline(y=mean_risk, color='g', linestyle='-', label='Q1 Risk')\n",
    "        ax[row, col].set_title(f'{view}')\n",
    "        ax[row, col].set_xlabel('Bounds')\n",
    "        ax[row, col].set_ylabel('Means')\n",
    "\n",
    "        \n",
    "\n",
    "    # handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "    # labels = [labels[i]+\" bound\" if i < len(labels)/2 else labels[i]+\" Gibbs risk\" for i in range(len(labels))]\n",
    "    # ax[0, 0].legend(handles, labels, title='Bounds', loc='upper right', fontsize='medium')\n",
    "    fig.suptitle(f'Test error rates and multiview PAC-Bayesian bound values, {m=},\\naveraged over {runs} runs')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_grid(df, len(Xs_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(experiments, m, runs=len(RUNS), bounds=BOUNDS):\n",
    "    sns.set_style(style=\"ticks\")\n",
    "    bounds_palette = sns.mpl_palette(\"viridis\", n_colors=len(bounds))\n",
    "    risk_palette = sns.color_palette(\"flare\", n_colors=len(bounds))\n",
    "    risk_palette.reverse()\n",
    "    \n",
    "    ax = sns.barplot(experiments, x='View', hue='Bound_name', y='Bound', errorbar=\"sd\", width=0.8, hatch='.', palette=bounds_palette)\n",
    "    ax = sns.barplot(experiments, x='View', hue='Bound_name', y='Risk', errorbar=\"sd\", width=0.8, hatch='\\\\', palette=risk_palette)\n",
    "\n",
    "    plt.title(f'Test error rates and multiview PAC-Bayesian bound values, {m=},\\naveraged over {runs} runs')\n",
    "    plt.xlabel('Views')\n",
    "    plt.ylabel('Means')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels = [labels[i]+\" bound\" if i < len(labels)/2 else labels[i]+\" Gibbs risk\" for i in range(len(labels))]\n",
    "\n",
    "    # Creating a unified legend for both plots\n",
    "    plt.legend(handles, labels, title=\"Bounds and risks\", loc='upper right', fontsize='medium')\n",
    "    plt.tight_layout() \n",
    "    plt.gcf().set_size_inches(24, 12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df, len(Xs_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
