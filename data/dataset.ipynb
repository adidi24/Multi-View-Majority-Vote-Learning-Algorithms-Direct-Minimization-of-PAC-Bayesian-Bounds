{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=53c06781e98038133a6fb642ae48f5d846a253fa89c087357ee47ae3241091f7\n",
      "  Stored in directory: /Users/abdelkrimzitouni/Library/Caches/pip/wheels/00/23/31/5e562fce1f95aabe57f2a7320d07433ba1cd152bcde2f6a002\n",
      "Successfully built liac-arff\n",
      "Installing collected packages: liac-arff\n",
      "Successfully installed liac-arff-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -c ./ALOI/aloi-8d.csv.gz > ./ALOI/aloi-8d.csv\n",
    "!gunzip -c ./ALOI/aloi-haralick-1.csv.gz > ./ALOI/aloi-haralick-1.csv\n",
    "!gunzip -c ./ALOI/aloi-hsb-2x2x2.csv.gz > ./ALOI/aloi-hsb-2x2x2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arff\n",
    "\n",
    "def read_aloi_directory(directory):\n",
    "    file_list = os.listdir(directory)\n",
    "    dataset_list = []\n",
    "    last_column_values = []\n",
    "    objs = arff.load(open(directory+'/objs.arff'))\n",
    "    labels = np.array(objs['data'])\n",
    "    sorted_labels = labels[np.argsort(labels[:, -1].astype(str))]\n",
    "    sorted_labels[:, :-1] = sorted_labels[:, :-1].astype(float)\n",
    "    last_column_label_values = sorted_labels[:, -1]\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path, delimiter=' ', header=None)\n",
    "            df = df.dropna(axis=1, how='all')\n",
    "            data = df.values\n",
    "        elif file == \"aloi-colorsim77.arff\":\n",
    "            dataset = arff.load(open(file_path))\n",
    "            data = np.array(dataset['data'])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        sorted_dataset = data[np.argsort(data[:, -1].astype(str))]\n",
    "        last_column_values = sorted_dataset[:, -1]\n",
    "        \n",
    "        if np.all(last_column_values == last_column_label_values):\n",
    "            dataset_list.append(sorted_dataset)\n",
    "    \n",
    "    dataset_list.append(sorted_labels)\n",
    "    return dataset_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloi = read_aloi_directory('./ALOI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aloi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('aloi_csv', exist_ok=True)\n",
    "# Save each array as a CSV file\n",
    "for i, arr in enumerate(aloi):\n",
    "    df = pd.DataFrame(arr)\n",
    "    # df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    arr = df.to_numpy()\n",
    "    filename = f'aloi_csv/aloi_array_{i}.csv'\n",
    "    print(i)\n",
    "    np.savetxt(filename, arr, delimiter=',', fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[8.86246293e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 2.13464808e-02, 1.41262478e-02],\n",
       "        [8.90708641e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 1.85275608e-02, 1.68773510e-02],\n",
       "        [8.92001682e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 1.45331489e-02, 2.14549877e-02],\n",
       "        ...,\n",
       "        [9.85243056e-01, 4.36288339e-04, 3.25520833e-04, ...,\n",
       "         0.00000000e+00, 1.23652705e-03, 1.18295175e-02],\n",
       "        [9.85206887e-01, 5.53837529e-04, 4.06901042e-04, ...,\n",
       "         3.61689815e-05, 1.47614656e-03, 1.12734194e-02],\n",
       "        [9.85866970e-01, 6.12612124e-04, 6.87210648e-04, ...,\n",
       "         2.48661748e-05, 1.38346354e-03, 1.03194625e-02]]),\n",
       " array([[0.52106334, 0.5549817 , 0.55797701, ..., 0.65483649, 0.59232047,\n",
       "         0.51372448],\n",
       "        [0.51680537, 0.55127803, 0.55560497, ..., 0.65723221, 0.59390096,\n",
       "         0.51455813],\n",
       "        [0.51327497, 0.54751493, 0.55261007, ..., 0.6617243 , 0.59765731,\n",
       "         0.51766337],\n",
       "        ...,\n",
       "        [0.43211503, 0.47109387, 0.49400604, ..., 0.69059443, 0.60842636,\n",
       "         0.51472406],\n",
       "        [0.43135114, 0.4707066 , 0.49390079, ..., 0.68830559, 0.60636211,\n",
       "         0.51297224],\n",
       "        [0.43046736, 0.469222  , 0.4920494 , ..., 0.6894209 , 0.60706375,\n",
       "         0.51344914]]),\n",
       " array([[ 1.07696785e-01,  4.38631411e-01, -2.66645251e+03, ...,\n",
       "          7.07723808e-01, -6.42185194e-01,  9.72326007e-01],\n",
       "        [ 1.07753558e-01,  4.34191668e-01, -2.68249209e+03, ...,\n",
       "          7.06114178e-01, -6.43020287e-01,  9.72087454e-01],\n",
       "        [ 1.05207246e-01,  4.48866668e-01, -2.68070552e+03, ...,\n",
       "          7.18560911e-01, -6.34048951e-01,  9.70957606e-01],\n",
       "        ...,\n",
       "        [ 2.05728092e-01,  6.09402692e-01, -4.18360189e+03, ...,\n",
       "          6.79261173e-01, -5.50429687e-01,  9.12310764e-01],\n",
       "        [ 2.06667002e-01,  6.52382745e-01, -4.30403666e+03, ...,\n",
       "          6.73621677e-01, -5.51512524e-01,  9.10632676e-01],\n",
       "        [ 2.06342783e-01,  6.80982237e-01, -4.45759336e+03, ...,\n",
       "          6.73485090e-01, -5.47440759e-01,  9.07453595e-01]]),\n",
       " array([[2.06375687e-01, 1.86767578e-02, 6.69331868e-01, ...,\n",
       "         0.00000000e+00, 3.78191913e-03, 2.86887840e-02],\n",
       "        [2.20162851e-01, 2.36839012e-02, 6.60922580e-01, ...,\n",
       "         0.00000000e+00, 3.29589844e-03, 2.65977648e-02],\n",
       "        [2.47513383e-01, 3.13833731e-02, 6.34790491e-01, ...,\n",
       "         0.00000000e+00, 3.02463108e-03, 2.45903863e-02],\n",
       "        ...,\n",
       "        [4.23848470e-01, 1.29846644e-02, 5.12537073e-01, ...,\n",
       "         1.13028067e-03, 6.51041667e-04, 1.65020978e-04],\n",
       "        [4.13809317e-01, 1.24647352e-02, 5.19992405e-01, ...,\n",
       "         1.39702691e-03, 7.95717593e-04, 3.43605324e-04],\n",
       "        [4.20749240e-01, 1.14972150e-02, 5.10554561e-01, ...,\n",
       "         1.50553385e-03, 7.95717593e-04, 4.02379919e-04]]),\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_dataset_files = os.listdir(\"./aloi_csv\")\n",
    "mv_dataset_files.sort()\n",
    "dataset = []\n",
    "for file in mv_dataset_files:\n",
    "    df = pd.read_csv(f\"./aloi_csv/{file}\", header=None)\n",
    "    numerical_df = df.select_dtypes(include=[np.number])\n",
    "    dataset.append(numerical_df.to_numpy())\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColorMoments.asc\n",
      "(68040, 10)\n",
      "CoocTexture.asc\n",
      "(68040, 17)\n",
      "ColorHistogram.asc\n",
      "(68040, 33)\n"
     ]
    }
   ],
   "source": [
    "corel_directory = \"./corel+image+features\"\n",
    "file_list = os.listdir(corel_directory)\n",
    "dataset_list = []\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(corel_directory, file)\n",
    "    \n",
    "    if file.endswith('.asc') and file != 'LayoutHistogram.asc':\n",
    "        print(file)\n",
    "        data = np.loadtxt(file_path)\n",
    "        dataset_list.append(data[:, 1:])\n",
    "        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: [array([[1.361033, 1.719371, 8.339537, ..., 0.      , 0.      , 0.      ],\n",
      "       [0.937685, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       ...,\n",
      "       [0.881949, 0.953139, 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]]), array([[1.361033, 1.719371, 8.339537, ..., 0.      , 0.      , 0.      ],\n",
      "       [0.937685, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       ...,\n",
      "       [0.881949, 0.953139, 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]]), array([[1.361033, 1.719371, 8.339537, ..., 0.      , 0.      , 0.      ],\n",
      "       [0.937685, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       ...,\n",
      "       [0.881949, 0.953139, 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]]), array([[1.361033, 1.719371, 8.339537, ..., 0.      , 0.      , 0.      ],\n",
      "       [0.937685, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       ...,\n",
      "       [0.881949, 0.953139, 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]]), array([[1.361033, 1.719371, 8.339537, ..., 0.      , 0.      , 0.      ],\n",
      "       [0.937685, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       ...,\n",
      "       [0.881949, 0.953139, 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
      "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.      ]])]\n",
      "\n",
      "Labels: ['E21', 'CCAT', 'M11', 'GCAT', 'C15', 'ECAT']\n",
      "\n",
      "Affectations: ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def load_data(sample, views):\n",
    "    data = []\n",
    "    for view in views:\n",
    "        mtx_file = f\"./ReutersEN/reutersEN/reutersEN_{sample}_{view}.mtx\"\n",
    "        maprow_file = f\"./ReutersEN/reutersEN/reutersEN_{sample}_{view}.maprow.txt\"\n",
    "        mapcol_file = f\"./ReutersEN/reutersEN/reutersEN_{sample}_{view}.mapcol.txt\"\n",
    "        \n",
    "        # Load documents-words matrix\n",
    "        with open(mtx_file, 'r') as f:\n",
    "            # Skip header lines\n",
    "            for _ in range(2):\n",
    "                next(f)\n",
    "\n",
    "            # Read matrix dimensions and number of non-zero entries\n",
    "            num_rows, num_cols, num_entries = map(int, next(f).split())\n",
    "\n",
    "            row_indices = []\n",
    "            col_indices = []\n",
    "            data_values = []\n",
    "\n",
    "            # Read each line in the file and extract row index, column index, and data value\n",
    "            for line in f:\n",
    "                row, col, val = map(float, line.split())\n",
    "                row_indices.append(int(row) - 1)  # Convert to 0-based indexing\n",
    "                col_indices.append(int(col) - 1)  # Convert to 0-based indexing\n",
    "                data_values.append(val)\n",
    "\n",
    "            # Construct the sparse matrix\n",
    "            sparse_mtx = sp.coo_matrix((data_values, (row_indices, col_indices)), shape=(num_rows, num_cols))\n",
    "            dense_array = sparse_mtx.toarray()\n",
    "            data.append(dense_array)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_labels():\n",
    "    labels_file = \"./ReutersEN/reutersEN/labels.txt\"\n",
    "    with open(labels_file, 'r') as f:\n",
    "        labels = [line.strip() for line in f]\n",
    "    return labels\n",
    "\n",
    "def load_affectations():\n",
    "    act_file = \"./ReutersEN/reutersEN/reutersEN_act.txt\"\n",
    "    with open(act_file, 'r') as f:\n",
    "        affectations = [line.strip() for line in f]\n",
    "    return affectations\n",
    "\n",
    "sample = 1\n",
    "views = ['EN', 'FR', 'GR', 'IT', 'SP']\n",
    "\n",
    "# Load data\n",
    "data = load_data(sample, views)\n",
    "\n",
    "# Load labels\n",
    "labels = load_labels()\n",
    "\n",
    "# Load affectations\n",
    "affectations = load_affectations()\n",
    "\n",
    "# Display loaded data\n",
    "print(\"Loaded data:\", data)\n",
    "print(\"\\nLabels:\", labels)\n",
    "print(\"\\nAffectations:\", affectations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage import feature\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage\n",
    "from mahotas.features import zernike_moments\n",
    "from mahotas.features import haralick\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute HOG features\n",
    "    features, _ = hog(gray, orientations=9, pixels_per_cell=(32, 32), cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "def extract_lbp_features(image, bins=8, eps = 1e-7):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute LBP features\n",
    "    lbp = feature.local_binary_pattern(gray, P=8, R=1, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, bins))\n",
    "\n",
    "    # Normalize the histogram\n",
    "    hist = hist.astype('float')\n",
    "    hist /= (hist.sum() + eps)\n",
    "\n",
    "    return hist, lbp\n",
    "\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def extract_sift_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return descriptors\n",
    "\n",
    "# surf = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "# def extract_surf_features(image):\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     keypoints, descriptors = surf.detectAndCompute(gray, None)\n",
    "#     return descriptors\n",
    "\n",
    "# texture\n",
    "def extract_gabor_features(image, frequencies=[0.1, 0.5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4]):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    filters = []\n",
    "    ksize = 31\n",
    "    for lambd in frequencies:\n",
    "        for theta in angles:\n",
    "            kern = cv2.getGaborKernel((ksize, ksize), 4.0, theta, lambd, 0.5, 0, ktype=cv2.CV_32F)\n",
    "            kern /= 1.5*kern.sum()\n",
    "            filters.append(kern)\n",
    "            \n",
    "    def process(img, filters):\n",
    "        accum = np.zeros_like(img)\n",
    "        for kern in filters:\n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "            np.maximum(accum, fimg, accum)\n",
    "            return accum\n",
    "\n",
    "    features=process(gray,filters)\n",
    "    return features\n",
    "\n",
    "def exctract_glcm_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    graycom = feature.graycomatrix(gray, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256)\n",
    "    # Find the GLCM properties\n",
    "    contrast = feature.graycoprops(graycom, 'contrast')\n",
    "    dissimilarity = feature.graycoprops(graycom, 'dissimilarity')\n",
    "    homogeneity = feature.graycoprops(graycom, 'homogeneity')\n",
    "    energy = feature.graycoprops(graycom, 'energy')\n",
    "    correlation = feature.graycoprops(graycom, 'correlation')\n",
    "    ASM = feature.graycoprops(graycom, 'ASM')\n",
    "    contrast = contrast.flatten()\n",
    "    dissimilarity = dissimilarity.flatten()\n",
    "    homogeneity = homogeneity.flatten()\n",
    "    energy = energy.flatten()\n",
    "    correlation = correlation.flatten()\n",
    "    ASM = ASM.flatten()\n",
    "\n",
    "    features = np.concatenate((contrast, dissimilarity, homogeneity, energy, correlation, ASM), axis=0) \n",
    "    return features\n",
    "    \n",
    "\n",
    "def extract_zernike_moments(image, radius=21, degree=8):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return zernike_moments(gray, radius, degree)\n",
    "\n",
    "def extract_hu_moments(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    moments = cv2.moments(gray)\n",
    "    hu_moments = cv2.HuMoments(moments)\n",
    "    return np.concatenate(hu_moments)\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "def extract_orb_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def extract_eigenfaces_features(image, num_components=50):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    flattened_image = gray.flatten()\n",
    "    \n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(flattened_image.reshape(1, -1))\n",
    "    \n",
    "    return pca.components_.flatten()\n",
    "\n",
    "\n",
    "def extract_haralick_features(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    textures = haralick(gray)\n",
    "    return textures.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(image):\n",
    "    color_histogram = extract_color_histogram(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    lbp_hist, lbp_features = extract_lbp_features(image)\n",
    "    glcm_features = exctract_glcm_features(image)\n",
    "    zernike_moments = extract_zernike_moments(image)\n",
    "    hu_moments = extract_hu_moments(image)\n",
    "    haralick_features = extract_haralick_features(image)\n",
    "    return [color_histogram.flatten(), hog_features.flatten(), lbp_hist.flatten(), glcm_features, zernike_moments.flatten(), hu_moments.flatten(), haralick_features.flatten()]\n",
    "\n",
    "def read_images(dataset_path, start_id=1):\n",
    "    multiview_dataset = {\n",
    "        'Color Histogram': [],\n",
    "        'HOG': [],\n",
    "        'LBP': [],\n",
    "        'GLCM Features': [],\n",
    "        'Zernike Moments': [],\n",
    "        'Hu Moments': [],\n",
    "        'Haralick Texture': []\n",
    "    }\n",
    "    labels = {}\n",
    "    id = start_id\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            img = cv2.imread(image_path)\n",
    "            resized_image = cv2.resize(img, (224, 224))\n",
    "            features = compute_features(resized_image)\n",
    "            multiview_dataset['Color Histogram'].append(features[0])\n",
    "            multiview_dataset['HOG'].append(features[1])    \n",
    "            multiview_dataset['LBP'].append(features[2])\n",
    "            multiview_dataset['GLCM Features'].append(features[3])\n",
    "            multiview_dataset['Zernike Moments'].append(features[4])\n",
    "            multiview_dataset['Hu Moments'].append(features[5])\n",
    "            multiview_dataset['Haralick Texture'].append(features[6])\n",
    "            labels.update({id: class_name})\n",
    "            id += 1\n",
    "    return multiview_dataset, labels\n",
    "\n",
    "# Example usage:\n",
    "train_set_path = \"./corel/training_set\"\n",
    "multiview_train_set, train_labels = read_images(train_set_path)\n",
    "test_set_path = \"./corel/test_set\"\n",
    "multiview_test_set, test_labels = read_images(test_set_path, len(train_labels) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multiview_train_set[\"LBP\"] + multiview_test_set[\"LBP\"]), len(list(train_labels.keys()) + list(test_labels.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 512) (1000,)\n",
      "(1000, 1296) (1000,)\n",
      "(1000, 7) (1000,)\n",
      "(1000, 24) (1000,)\n",
      "(1000, 25) (1000,)\n",
      "(1000, 7) (1000,)\n",
      "(1000, 13) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "multiview_dataset = {}\n",
    "for k, v in multiview_train_set.items():\n",
    "    multiview_dataset[k] = multiview_train_set[k] + multiview_test_set[k]\n",
    "    multiview_dataset[k] = np.array(multiview_dataset[k])\n",
    "    labels_keys = list(train_labels.keys()) + list(test_labels.keys())\n",
    "    labels_keys = np.array(labels_keys)\n",
    "    print(multiview_dataset[k].shape, labels_keys.shape)\n",
    "    multiview_dataset[k] = np.insert(multiview_dataset[k], 0, labels_keys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color Histogram (1000, 513)\n",
      "HOG (1000, 1297)\n",
      "LBP (1000, 8)\n",
      "GLCM Features (1000, 25)\n",
      "Zernike Moments (1000, 26)\n",
      "Hu Moments (1000, 8)\n",
      "Haralick Texture (1000, 14)\n"
     ]
    }
   ],
   "source": [
    "for k, v in multiview_dataset.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color Histogram\n",
      "HOG\n",
      "LBP\n",
      "GLCM Features\n",
      "Zernike Moments\n",
      "Hu Moments\n",
      "Haralick Texture\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "os.makedirs('corel_features', exist_ok=True)\n",
    "# Save each array as a CSV file\n",
    "for k, v in multiview_dataset.items():\n",
    "    df = pd.DataFrame(v)\n",
    "    # df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    arr = df.to_numpy()\n",
    "    filename = f'corel_features/{k}.csv'\n",
    "    print(k)\n",
    "    np.savetxt(filename, arr, delimiter=',', fmt='%s')\n",
    "\n",
    "keys = np.array(list(train_labels.keys()))\n",
    "vals = np.array(list(train_labels.values()), dtype=object)\n",
    "array = np.column_stack((keys, vals))\n",
    "array.shape, array\n",
    "np.savetxt(\"corel_features/labels.txt\", array, delimiter=' ', fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
